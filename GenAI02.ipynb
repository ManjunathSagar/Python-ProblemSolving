{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7440442b-7a62-47ea-938c-708fb9943ada",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140aa96-222d-4e18-b92e-4ec7e10ce0b3",
   "metadata": {},
   "source": [
    "Prompting in GenAI means giving clear instructions or queries to a generative AI model so it can produce text, code, images, or answers.\n",
    "It is simply the technique of communicating with AI effectively using natural language or structured formats.\n",
    "\n",
    "Types of prompting (task, instruction, conversational)\n",
    "Task Prompting: A short prompt that simply describes the task, without extra instructions or style details.\n",
    "\n",
    "Instruction Prompting: Explain the steps of making tea in simple English, in exactly 5 bullet points.\n",
    "\n",
    "Conversational Prompting: Prompts look like normal human conversation instead of commands.\n",
    "\n",
    "Anatomy of a prompt (instruction + context + examples + constraints)\n",
    "\n",
    "Differences between models (GPT, Gemini, Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214f040a-5f19-47ae-8160-53f3b58bac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b2386e-4e13-47e0-b288-fa922df93ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05916fe9-2267-4dab-a88e-4730cb82f6d5",
   "metadata": {},
   "source": [
    "#### Task Prompting: A short prompt that simply describes the task, without extra instructions or style details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445454fc-b8b1-47ce-855e-c6a7a4f83413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarees are traditional Indian garments consisting of a long piece of cloth draped elegantly around the body, often accompanied by a blouse and petticoat, reflecting cultural heritage and personal style.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize the following text in one sentence:\\n\\n About Sarees\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3164c8-06a7-401d-9b2e-f1af9988ab21",
   "metadata": {},
   "source": [
    "#### Instruction Prompting: Draft message based on the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917ea5e2-4c50-4b99-be6e-780d697b1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I regret to inform you that I will be unable to attend the meeting today.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Rewrite the text below by following these instructions:\n",
    "1. Make it polite and professional.\n",
    "2. Keep it under 2 sentences.\n",
    "3. Do not change the meaning.\n",
    "\n",
    "Text: \"I cannot attend the meeting today.\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00e13a-5f6a-4045-911a-ef1da40fcc11",
   "metadata": {},
   "source": [
    "#### Conversational Prompting is a natural, dialogue-style way of asking the model something, just like talking to a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad28b83-45a7-4fa0-a529-25c88fb713e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Absolutely! In Python, a list is a built-in data type that allows you to store a collection of items. Here are some key points about Python lists:\n",
      "\n",
      "1. **Ordered**: The items in a list have a defined order, and that order will not change unless you explicitly modify the list. Each item can be accessed by its position (or index).\n",
      "\n",
      "2. **Mutable**: You can change (update, add, or remove) the items in a list after it has been created.\n",
      "\n",
      "3. **Heterogeneous**: A list can hold items of different data types. For example, you can mix integers, strings, and even other lists within the same list.\n",
      "\n",
      "4. **Dynamic**: Lists can grow and shrink in size as you add or remove items.\n",
      "\n",
      "### Creating a List\n",
      "To create a list, you can use square brackets `[]`, separating items with commas. Here’s how you can create a simple list:\n",
      "\n",
      "```python\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "```\n",
      "\n",
      "### Accessing Items\n",
      "You can access items in a list by their index. Note that Python uses zero-based indexing:\n",
      "\n",
      "```python\n",
      "print(my_list[0])  # Outputs: 1\n",
      "print(my_list[2])  # Outputs: 3\n",
      "```\n",
      "\n",
      "### Modifying Items\n",
      "You can change the value of an item in a list:\n",
      "\n",
      "```python\n",
      "my_list[1] = 10\n",
      "print(my_list)  # Outputs: [1, 10, 3, 4, 5]\n",
      "```\n",
      "\n",
      "### Adding and Removing Items\n",
      "You can add items to a list using the `append()` method or `insert()` method, and you can remove items using the `remove()` method or `pop()` method.\n",
      "\n",
      "```python\n",
      "my_list.append(6)       # Adds 6 to the end of the list\n",
      "print(my_list)          # Outputs: [1, 10, 3, 4, 5, 6]\n",
      "\n",
      "my_list.remove(10)      # Removes the first occurrence of 10 from the list\n",
      "print(my_list)          # Outputs: [1, 3, 4, 5, 6]\n",
      "\n",
      "popped_item = my_list.pop()  # Removes and returns the last item\n",
      "print(popped_item)       # Outputs: 6\n",
      "print(my_list)          # Outputs: [1, 3, 4, 5]\n",
      "```\n",
      "\n",
      "### Iterating Through a List\n",
      "You can loop through the items in a list using a `for` loop:\n",
      "\n",
      "```python\n",
      "for item in my_list:\n",
      "    print(item)\n",
      "```\n",
      "\n",
      "### Summary\n",
      "Lists are versatile and useful for storing collections of items in Python. They provide a variety of methods for manipulation, making them a fundamental part of the language. As you continue learning Python, you'll find that lists are frequently used in different applications and algorithms.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I’m trying to learn Python. Can you explain what a list is?\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff926a2-41e5-46ec-a14c-a18a6080a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Sure! Let's create a small example that demonstrates how to use lists in Python. In this example, we will create a list of fruits, add some fruits, remove one, and then print the list.\n",
      "\n",
      "### Example: Managing a Fruit List\n",
      "\n",
      "```python\n",
      "# Step 1: Create a list of fruits\n",
      "fruits = ['apple', 'banana', 'orange', 'strawberry']\n",
      "\n",
      "# Step 2: Print the original list\n",
      "print(\"Original list of fruits:\", fruits)\n",
      "\n",
      "# Step 3: Add a fruit to the list\n",
      "fruits.append('grape')\n",
      "print(\"After adding grape:\", fruits)\n",
      "\n",
      "# Step 4: Remove a fruit from the list\n",
      "fruits.remove('banana')\n",
      "print(\"After removing banana:\", fruits)\n",
      "\n",
      "# Step 5: Access and print the first fruit in the list\n",
      "first_fruit = fruits[0]\n",
      "print(\"The first fruit in the list is:\", first_fruit)\n",
      "\n",
      "# Step 6: Iterate through the list and print each fruit\n",
      "print(\"All fruits in the list:\")\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "### Output\n",
      "If you run the code above, you would see output like this:\n",
      "\n",
      "```\n",
      "Original list of fruits: ['apple', 'banana', 'orange', 'strawberry']\n",
      "After adding grape: ['apple', 'banana', 'orange', 'strawberry', 'grape']\n",
      "After removing banana: ['apple', 'orange', 'strawberry', 'grape']\n",
      "The first fruit in the list is: apple\n",
      "All fruits in the list:\n",
      "apple\n",
      "orange\n",
      "strawberry\n",
      "grape\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "1. **Creating the list**: We start by creating a list called `fruits` with four initial elements.\n",
      "2. **Printing the list**: We print the original list to see what we have.\n",
      "3. **Adding an item**: We use `append()` to add \"grape\" to the end of the list.\n",
      "4. **Removing an item**: We remove \"banana\" using `remove()`.\n",
      "5. **Accessing an item**: We access the first item in the list using the index `0`.\n",
      "6. **Iterating through the list**: Finally, we use a `for` loop to print each fruit in the list.\n",
      "\n",
      "This simple example covers various aspects of list manipulation in Python, helping illustrate how you can work with lists!\n"
     ]
    }
   ],
   "source": [
    "# Continuing the conversation like a human chat\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Great! Can you also give me a small example?\"})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\nAssistant:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dba6e7-9cbb-4de1-bb64-70c36cd54466",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cf131-f7d6-4f2a-a77a-3ea2a2b34b64",
   "metadata": {},
   "source": [
    "#### Zero-shot prompting means giving the model a task without providing any examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10dc345-056f-400a-a44d-92df729d9476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean energy is crucial for fostering a sustainable future, as it significantly reduces greenhouse gas emissions and mitigates climate change. By harnessing renewable resources such as solar, wind, and hydroelectric power, we can decrease our dependence on fossil fuels, which are major contributors to environmental degradation. Transitioning to clean energy not only protects ecosystems and public health but also promotes economic growth through the creation of green jobs and energy independence. Ultimately, embracing clean energy is essential for ensuring a healthier planet for future generations while addressing urgent global challenges.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a short paragraph describing the importance of clean energy.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe83d0f-8a51-4810-9ccc-67aadf385281",
   "metadata": {},
   "source": [
    "#### One-shot prompting is when you give the model one example of how you want it to respond before giving the real task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c2dd21c-c8db-45e1-97c7-a1d49499e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    " \"role\": \"user\",\n",
    "        \"content\":\n",
    "        \"Example of how to classify sentiment:\\n\\n\"\n",
    "        \"Text: 'I love this product!'\\n\"\n",
    "        \"Sentiment: Positive\\n\\n\"\n",
    "        \"Now classify this text:\\n\"\n",
    "        \"Text: 'The service was really slow and disappointing.'\\n\"\n",
    "        \"Sentiment:\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddb087-7fc3-4b08-9627-c07be844dbfa",
   "metadata": {},
   "source": [
    "#### Few-shot prompting means giving the model multiple examples (usually 2–5) so it learns the pattern or style before answering your actual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f85063ed-2057-4c6a-b3bf-09083c30f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\n",
    "        \"Here are examples of how to classify sentiment:\\n\\n\"\n",
    "        \n",
    "        \"Example 1:\\n\"\n",
    "        \"Text: 'The movie was amazing! Absolutely loved it.'\\n\"\n",
    "        \"Sentiment: Positive\\n\\n\"\n",
    "        \n",
    "        \"Example 2:\\n\"\n",
    "        \"Text: 'This product broke the same day I bought it.'\\n\"\n",
    "        \"Sentiment: Negative\\n\\n\"\n",
    "        \n",
    "        \"Now classify this text:\\n\"\n",
    "        \"Text: 'The food was okay, nothing special but not bad.'\\n\"\n",
    "        \"Sentiment:\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fe2e3-531b-4674-806f-2a8a68649e2b",
   "metadata": {},
   "source": [
    "#### Chain-of-thought prompting is when you explicitly ask the model to show its reasoning step by step before giving the final answer. It’s used for maths, logic, coding, and multi-step problems to improve accuracy and transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b787791-3149-4ccb-b698-83979a187c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the total profit of the shopkeeper, we will analyze each transaction step by step.\n",
      "\n",
      "1. **First Transaction:**\n",
      "   - **Cost Price (CP)** of the first pen: ₹10\n",
      "   - **Selling Price (SP)** of the first pen: ₹15\n",
      "\n",
      "   To find the profit for the first pen, we use the formula:\n",
      "   \\[\n",
      "   \\text{Profit} = \\text{Selling Price} - \\text{Cost Price}\n",
      "   \\]\n",
      "   Thus, for the first pen:\n",
      "   \\[\n",
      "   \\text{Profit} = 15 - 10 = 5\n",
      "   \\]\n",
      "\n",
      "2. **Second Transaction:**\n",
      "   - **Cost Price (CP)** of the second pen: ₹20\n",
      "   - **Selling Price (SP)** of the second pen: ₹25\n",
      "\n",
      "   Again, using the profit formula:\n",
      "   \\[\n",
      "   \\text{Profit} = \\text{Selling Price} - \\text{Cost Price}\n",
      "   \\]\n",
      "   Thus, for the second pen:\n",
      "   \\[\n",
      "   \\text{Profit} = 25 - 20 = 5\n",
      "   \\]\n",
      "\n",
      "3. **Total Profit:**\n",
      "   Now, we need to add the profits from both transactions to find the total profit:\n",
      "   \\[\n",
      "   \\text{Total Profit} = \\text{Profit from 1st pen} + \\text{Profit from 2nd pen}\n",
      "   \\]\n",
      "   Therefore:\n",
      "   \\[\n",
      "   \\text{Total Profit} = 5 + 5 = 10\n",
      "   \\]\n",
      "\n",
      "Thus, the total profit made by the shopkeeper is ₹10.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "A shopkeeper buys a pen for ₹10 and sells it for ₹15. \n",
    "Then he buys another pen for ₹20 and sells it for ₹25. What is his total profit?\n",
    "Think step by step and explain your reasoning before giving the final answer\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "                   \n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Here, “Think step by step and explain your reasoning” turns it into a chain-of-thought prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb11f08-033d-45f1-a07d-5b18021a1553",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d85dd9-6a6c-4a9a-904b-b0ec71595294",
   "metadata": {},
   "source": [
    "#### Self-critique / Self-reflection prompting is when you ask the model to review, evaluate, or correct its own answer before finalizing it. It helps improve accuracy, reduce errors, and produce higher-quality outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4234b9e1-9ad3-460b-a714-35ebdbea0d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subset of artificial intelligence that enables computer systems to learn and improve from experience without being explicitly programmed. It involves algorithms that analyze and learn patterns from data, allowing for predictions or decisions based on new data inputs.\n",
      "\n",
      "**Critique:** The initial answer provides a concise definition of machine learning but could be further clarified by emphasizing its application areas and the types of learning involved (supervised, unsupervised, and reinforcement learning). \n",
      "\n",
      "**Refined Final Version:** \n",
      "Machine learning is a branch of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions based on data. Through various techniques such as supervised, unsupervised, and reinforcement learning, machines can improve their performance over time without explicit programming, making it applicable in diverse fields like healthcare, finance, and robotics.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\n",
    "        \"Explain what machine learning is in 3–4 lines.\\n\"\n",
    "        \"Then critique your answer and provide a refined final version.\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "                   \n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Here, “Think step by step and explain your reasoning” turns it into a chain-of-thought prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bab79a-8d3a-49a2-9a0d-3f0578d4a398",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b59f9b-9907-494d-a5f0-3dfda32dea76",
   "metadata": {},
   "source": [
    "#### Meta-prompting means giving instructions to the model about how to create or improve prompts themselves. Instead of asking the model to answer a question, you ask it to generate better prompts, rewrite prompts, evaluate prompts, or guide prompt creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb1aee9b-4128-4c28-8fbe-069b707984b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's an improved version of your prompt that is more specific, structured, and detailed:\n",
      "\n",
      "---\n",
      "\n",
      "**Prompt: Explain Artificial Intelligence (AI)**\n",
      "\n",
      "1. **Definition**: Begin with a clear definition of Artificial Intelligence. What is AI, and how is it understood within the field of computer science?\n",
      "\n",
      "2. **Categories**: Describe the different categories of AI. For example, differentiate between Narrow AI, General AI, and Superintelligent AI. Provide examples of each type.\n",
      "\n",
      "3. **Technologies and Techniques**: Outline the key technologies and techniques used in AI, such as machine learning, neural networks, natural language processing, and computer vision. Give a brief explanation of each.\n",
      "\n",
      "4. **Applications**: Discuss various applications of AI in real-world scenarios, such as in healthcare, finance, automotive, entertainment, and customer service. Provide specific examples to illustrate its impact.\n",
      "\n",
      "5. **Ethical Considerations**: Analyze the ethical implications of AI. What are the potential risks and challenges associated with AI development and deployment? \n",
      "\n",
      "6. **Future Outlook**: Speculate on the future of AI. What advancements can we expect in the next 5-10 years, and how might they affect society?\n",
      "\n",
      "7. **Conclusion**: Summarize the key points made in the explanation and reflect on the importance of understanding AI in today's world.\n",
      "\n",
      "---\n",
      "\n",
      "This structured approach will help guide a comprehensive exploration of AI, covering its definition, technologies, applications, ethical considerations, and future prospects.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\n",
    "        \"Here is my prompt: 'Explain AI'.\\n\\n\"\n",
    "        \"Improve this prompt by making it more specific, structured, and detailed.\\n\"\n",
    "        \"This is a meta-prompt because I am asking you to improve the prompt itself.\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "                   \n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d229c9b-5e30-44eb-ac7a-65dbee3b654b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b12f4-4f32-4a62-b3ef-855856d4e486",
   "metadata": {},
   "source": [
    "### How to Structure a High-Quality Prompt\n",
    "\n",
    "Core idea: Don’t just “ask a question”. Give the model a role, a clear task, any input data, how you want the output formatted, and any constraints (limits, style, etc.).\n",
    "\n",
    "A simple mental template:\n",
    "\n",
    "Instruction → Context → Data → Format → Constraints\n",
    "\n",
    "Instruction: Tell the model what to do in one clear sentence.\n",
    "Context: Give background so the model knows the situation.\n",
    "Data: Provide the input the model should work on instead of letting it make things up.\n",
    "Format: Specify what the answer should look like.\n",
    "Constraints: Add limits and requirements that reduce ambiguity.\n",
    "\n",
    "### Avoiding Ambiguity to Reduce Hallucinations\n",
    "Ambiguous prompt:\n",
    "\n",
    "“Tell me about revenue for our product last month.”\n",
    "\n",
    "Problems:\n",
    "\n",
    "1) The model does not know your company or real data.\n",
    "2) It will either ask for more info (good) or hallucinate fake numbers (bad).\n",
    "\n",
    "Better structured prompt (and safer):\n",
    "\n",
    "    “You do not have access to real company data.\n",
    "    I will provide sample data. Never invent numbers. If the data is missing, say ‘data not provided’.\n",
    "    Here is the CSV data for our product revenue:\n",
    "    \n",
    "    month,product,revenue  \n",
    "    2025-10,Product A,12000  \n",
    "    2025-10,Product B,8000  \n",
    "    \n",
    "    \n",
    "    Task:\n",
    "    \n",
    "    Summarize the total revenue.\n",
    "    \n",
    "    List each product and its revenue.\n",
    "    Output format:\n",
    "    \n",
    "    First a 2-line summary\n",
    "    \n",
    "    Then a markdown table with columns: product, revenue.”\n",
    "\n",
    "Key anti-hallucination tricks:\n",
    "\n",
    "Clearly say: “Don’t make up facts. Say ‘I don’t know’ if you’re unsure.”\n",
    "\n",
    "Supply all needed data inside the prompt.\n",
    "\n",
    "Ask model to quote / reference the data it used.\n",
    "\n",
    "Use constraints like “Use only the provided text”, “If missing, answer: ‘not in text’”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f82e5-aba4-4c09-9247-7c7f5970e8fb",
   "metadata": {},
   "source": [
    "### Output Structuring: JSON-formatted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b51c5e9a-3e30-4ae4-ae33-da83425912a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"2025_AO_mens_singles_semifinals\": [\n",
      "    {\n",
      "      \"player1\": \"Novak Djokovic\",\n",
      "      \"player2\": \"Daniil Medvedev\",\n",
      "      \"score\": \"6-3, 6-4, 7-6\",\n",
      "      \"winner\": \"Novak Djokovic\"\n",
      "    },\n",
      "    {\n",
      "      \"player1\": \"Carlos Alcaraz\",\n",
      "      \"player2\": \"Stefanos Tsitsipas\",\n",
      "      \"score\": \"6-2, 4-6, 6-3, 6-1\",\n",
      "      \"winner\": \"Carlos Alcaraz\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{'2025_AO_mens_singles_semifinals': [{'player1': 'Novak Djokovic', 'player2': 'Daniil Medvedev', 'score': '6-3, 6-4, 7-6', 'winner': 'Novak Djokovic'}, {'player1': 'Carlos Alcaraz', 'player2': 'Stefanos Tsitsipas', 'score': '6-2, 4-6, 6-3, 6-1', 'winner': 'Carlos Alcaraz'}]}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a tennis-data assistant.  \n",
    "Provide the 2025 Australian Open men’s singles semifinal results in the following JSON format — and output ONLY valid JSON (no extra text, no markdown, no explanation).  \n",
    "\n",
    "Output format:\n",
    "{\n",
    "  \"2025_AO_mens_singles_semifinals\": [\n",
    "    {\n",
    "      \"player1\": string,\n",
    "      \"player2\": string,\n",
    "      \"score\": string,\n",
    "      \"winner\": string\n",
    "    },\n",
    "    {\n",
    "      \"player1\": string,\n",
    "      \"player2\": string,\n",
    "      \"score\": string,\n",
    "      \"winner\": string\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n",
    "# Optionally parse to verify JSON\n",
    "data = json.loads(resp.choices[0].message.content)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c383c-8d6f-4534-8b06-70b39da38d11",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f1662-0308-4966-a35b-993dea1d3374",
   "metadata": {},
   "source": [
    "### What is ReAct Prompting?\n",
    "\n",
    "ReAct stands for: Reason + Act\n",
    "\n",
    "It means an LLM should:\n",
    "1. Reason – explain or write intermediate thinking steps (chain-of-thought)\n",
    "2. Act – take an action (search / compute / extract / decide / output answer)\n",
    "\n",
    "Instead of just answering directly.\n",
    "\n",
    "ReAct was proposed by Google AI to improve:\n",
    "+ decision making\n",
    "+ planning\n",
    "+ tool use\n",
    "+ question answering\n",
    "+ avoiding hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b576d283-3bb9-4699-b875-5eecb6f6fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: To determine the arrival time of the train, I need to add the travel duration of 2 hours and 30 minutes to the departure time of 5 PM.\n",
      "\n",
      "Action: I will add 2 hours to 5 PM, which brings me to 7 PM. Then, I will add the additional 30 minutes to 7 PM.\n",
      "\n",
      "Observation: Adding 30 minutes to 7 PM results in 7:30 PM.\n",
      "\n",
      "Final Answer: The train will reach at 7:30 PM.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Use ReAct (Reason + Act) format.\n",
    "\n",
    "Question:\n",
    "If the train leaves at 5 PM and takes 2 hours 30 mins,\n",
    "what time will it reach?\n",
    "\n",
    "Format:\n",
    "Thought:\n",
    "Action:\n",
    "Observation:\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb5819-7145-48d1-85be-b65b504b1197",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ae2a4-b42b-4a8d-a8af-8a09589e1e0f",
   "metadata": {},
   "source": [
    "#### ReAct reduces hallucination because:\n",
    "\n",
    "+ LLM has to justify the next step\n",
    "+ Each action is checked\n",
    "+ It prevents guessing the final answer\n",
    "+ It forces logical flow\n",
    "\n",
    "This is why many LLM agents use ReAct style internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070fde6-0088-43b8-ae7f-10eac61c2fc9",
   "metadata": {},
   "source": [
    "#### Retrieval-Augmented Generation (RAG) Prompting\n",
    "#### Multi-Agent Prompting (AutoGen, CrewAI, LangChain Agents)\n",
    "#### Programmatic Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73af75d-201e-46ca-9e0d-ee0e8d6a1c4d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808cc47-3506-44f1-a3a6-52615599e234",
   "metadata": {},
   "source": [
    "#### What is Prompt Cache?\n",
    "\n",
    "Prompt Cache is a mechanism used in LLMs (like GPT-based models) to speed up responses and reduce cost by reusing already-computed results for repeated prompts.\n",
    "\n",
    "Think of it like a browser cache:\n",
    "1. The first time you ask a question → the model computes everything.\n",
    "2. The next time same prompt or very similar prompt appears, it returns the stored result instantly without recomputing the entire tokens.\n",
    "\n",
    "Why Prompt Cache Exists?\n",
    "\n",
    "LLMs are expensive to run because they compute embeddings + transformer layers every time.\n",
    "If many users send similar prompts like:\n",
    "\n",
    "“Explain Zero-Shot prompting with example”\n",
    "\n",
    "Instead of recomputing the whole output, the model checks the cache and returns the stored answer.\n",
    "\n",
    "Key Benefits\n",
    "1. Faster response: No need to calculate embeddings/attention again.\n",
    "\n",
    "2. Reduced compute and cost: Server does less work.\n",
    "\n",
    "3. Useful for repeated or template prompts\n",
    "For example:\n",
    "Summarize this text: <some paragraph>\n",
    "Translate this sentence…\n",
    "Explain a concept…\n",
    "\n",
    "\n",
    "#### Where is Prompt Cache stored?\n",
    "\n",
    "It depends on the LLM provider, but the cache is usually stored in:\n",
    "1.  The provider’s server-side memory (not local to you)\n",
    "2. Often in a vector store or KV (Key-Value) cache\n",
    "3.  Maintained by the model runtime layer (not by users)\n",
    "\n",
    "Architecture\n",
    "\n",
    "Internally, prompt cache is usually stored like this:\n",
    "\n",
    "Key\tValue : Prompt or embedding hash\t\n",
    "Value : Pre-computed tokens or embeddings\n",
    "\n",
    "Meaning:\n",
    "Server takes your prompt\n",
    "Hashes/embeds it\n",
    "Checks in distributed cache\n",
    "\n",
    "#### You can also build your own customer prompt cache store\n",
    "\n",
    "Please review below mentioned document to stay updated and relevant \n",
    "https://platform.openai.com/docs/guides/prompt-caching#page-top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c80f68e1-9ed8-4221-8e04-cef86b3d8428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FIRST CALL (expected: cached_tokens ≈ 0, slower) ===\n",
      "Time: 10.693 sec\n",
      "prompt_tokens: 3647\n",
      "cached_tokens: 0\n",
      "----------------------------------------\n",
      "Based on the repeated mention of privacy terms and legal statements in the document, while specific advantages cannot be derived without the actual content of those terms, common advantages that are often found in data protection agreements can include:\n",
      "\n",
      "1. **Transparency:** Clear terms help users u ...\n",
      "\n",
      "\n",
      "=== SECOND CALL (expected: cached_tokens > 0, faster) ===\n",
      "Time: 5.204 sec\n",
      "prompt_tokens: 3647\n",
      "cached_tokens: 3072\n",
      "----------------------------------------\n",
      "The text you provided appears to be a repetition, primarily stating that it includes privacy terms and legal statements without any specific details about the contents of the agreement. As a result, I cannot identify any advantages or specific terms within the agreement since the actual text or cont ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Cgip58EINgLWai34SZoEQtlEsLIiR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The text you provided appears to be a repetition, primarily stating that it includes privacy terms and legal statements without any specific details about the contents of the agreement. As a result, I cannot identify any advantages or specific terms within the agreement since the actual text or context is not included.\\n\\nHowever, in general, agreements that include privacy terms and legal statements typically offer several advantages, such as:\\n\\n1. **Transparency**: Clearly outlines how personal data will be collected, used, and protected, helping users understand their rights and the extent of data handling.\\n\\n2. **User Control**: Provides users with mechanisms to control their personal information, such as options for consent, access, correction, and deletion of their data.\\n\\n3. **Legal Protection**: Establishes a legal framework that protects both the organization and the users, potentially reducing the risk of legal disputes.\\n\\n4. **Trust Building**: By demonstrating a commitment to privacy protection, organizations can build trust with their users, which may improve customer loyalty and satisfaction.\\n\\n5. **Compliance with Regulations**: Ensures that the organization complies with relevant privacy laws (such as GDPR), helping to avoid legally costly violations.\\n\\n6. **Data Security Measures**: May outline the security measures in place to protect personal data, which is essential for safeguarding against data breaches.\\n\\nFor a more tailored answer, specific details from the actual agreement would be required.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1764298111, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_eca0ce8298', usage=CompletionUsage(completion_tokens=277, prompt_tokens=3647, total_tokens=3924, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=3072)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Make a long static prefix so it's cacheable by the model\n",
    "STATIC_TEXT = (\n",
    "    \"You are an AI that reviews data protection documentation. \"\n",
    "    \"Analyze the extended policy terms and highlight security, privacy, and GDPR risks.\\n\\n\"\n",
    "    + (\"This document contains privacy terms and legal statements. \" * 400)\n",
    ")\n",
    "\n",
    "def call_with_long_prefix(label: str):\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",   # or \"gpt-4o-2024-08-06\" if available to you\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": STATIC_TEXT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What are some of the advantages in this agreement.\"\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    usage = response.usage\n",
    "\n",
    "    # Try to read cached_tokens if your SDK/model exposes it\n",
    "    cached_tokens = None\n",
    "    details = getattr(usage, \"prompt_tokens_details\", None)\n",
    "    if details is not None:\n",
    "        # details is a Pydantic model, so use getattr, not .get()\n",
    "        cached_tokens = getattr(details, \"cached_tokens\", None)\n",
    "\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    print(f\"Time: {elapsed:.3f} sec\")\n",
    "    print(\"prompt_tokens:\", getattr(usage, \"prompt_tokens\", None))\n",
    "    print(\"cached_tokens:\", cached_tokens)  # may be None if not supported\n",
    "    print(\"-\" * 40)\n",
    "    print(response.choices[0].message.content[:300], \"...\\n\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# 1️⃣ First request — no cache yet\n",
    "call_with_long_prefix(\"FIRST CALL (expected: cached_tokens ≈ 0, slower)\")\n",
    "\n",
    "# 2️⃣ Second request — identical prompt, LLM-side cache should kick in\n",
    "call_with_long_prefix(\"SECOND CALL (expected: cached_tokens > 0, faster)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ed755-90f0-4fdd-ab85-83f2c883d1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
