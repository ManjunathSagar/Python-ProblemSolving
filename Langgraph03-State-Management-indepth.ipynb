{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0339dc6c-69cb-420e-999e-d0406782c32b",
   "metadata": {},
   "source": [
    "# State Management Deep Dive for GenAI Developers\n",
    "\n",
    "> **Framework**: LangGraph by LangChain  \n",
    "> **Version**: LangGraph 0.2+  \n",
    "> **Prerequisites**: Basic Python, understanding of LLMs and agents\n",
    "\n",
    "## What is LangGraph?\n",
    "\n",
    "**LangGraph** is a library for building stateful, multi-actor applications with Large Language Models (LLMs). It extends LangChain with the ability to create cyclic graphs of computation, enabling:\n",
    "- Multi-step agent workflows\n",
    "- Human-in-the-loop systems  \n",
    "- Complex reasoning chains\n",
    "- Tool-using agents\n",
    "- Multi-agent systems\n",
    "\n",
    "**Core Concept**: LangGraph models your AI application as a directed graph where:\n",
    "- **Nodes** = Functions that process state (LLM calls, tool usage, logic)\n",
    "- **Edges** = Control flow between nodes (sequential, conditional, parallel)\n",
    "- **State** = A shared object that flows through the graph\n",
    "\n",
    "This document focuses on **state management** - the foundation of building reliable LangGraph applications.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Centralized State Objects\n",
    "\n",
    "### What is Centralized State in LangGraph?\n",
    "\n",
    "**In LangGraph**, centralized state is a design pattern where all data required for your application's execution flows through a single, unified state object that every node in your graph can access and update. Think of it as a single source of truth that travels through your entire workflow.\n",
    "\n",
    "**How LangGraph handles state**:\n",
    "- You define a state schema (using TypedDict, Pydantic, or dataclass)\n",
    "- LangGraph creates a `StateGraph` with this schema\n",
    "- Each node receives the current state as input\n",
    "- Each node returns updates (partial state changes)\n",
    "- LangGraph merges updates and passes the new state to the next node\n",
    "\n",
    "**LangGraph's state is different from traditional variables**:\n",
    "- State is immutable from each node's perspective\n",
    "- Updates are merged, not directly applied\n",
    "- LangGraph manages the actual state object\n",
    "- Nodes only see and return dictionaries\n",
    "\n",
    "**Quick LangGraph Example**:\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# 1. Define your state schema\n",
    "class AgentState(TypedDict):\n",
    "    messages: list\n",
    "    next_action: str\n",
    "\n",
    "# 2. Create nodes that work with state\n",
    "def my_node(state: AgentState) -> AgentState:\n",
    "    # Access state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Return updates (not the full state!)\n",
    "    return {\"next_action\": \"continue\"}\n",
    "\n",
    "# 3. Build the graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"process\", my_node)\n",
    "graph.set_entry_point(\"process\")\n",
    "graph.add_edge(\"process\", END)\n",
    "\n",
    "# 4. Compile and run\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"messages\": [], \"next_action\": \"\"})\n",
    "```\n",
    "\n",
    "**Why is this important?**\n",
    "In traditional programming, you might have data scattered across global variables, module-level caches, instance variables, and function parameters. This makes it extremely difficult to:\n",
    "- Understand what data a function depends on\n",
    "- Track how data changes over time\n",
    "- Debug issues when something goes wrong\n",
    "- Test functions in isolation\n",
    "- Replay or resume execution\n",
    "\n",
    "With centralized state in LangGraph, you eliminate these problems by making all data flow explicit and visible.\n",
    "\n",
    "### Core Philosophy\n",
    "\n",
    "Think of your LangGraph application as a pipeline where state is like water flowing through pipes. Each node is a processing station that:\n",
    "1. Receives the current state (reads from the stream)\n",
    "2. Performs some operation\n",
    "3. Returns updates to the state (adds to the stream)\n",
    "\n",
    "The state object itself is **immutable** from each node's perspective - you never modify it directly. Instead, you return a new dictionary with the changes you want to make, and LangGraph merges these changes into the state for you.\n",
    "\n",
    "### Benefits of Centralization\n",
    "\n",
    "When you centralize state, you gain several critical advantages:\n",
    "\n",
    "**1. Single Source of Truth**\n",
    "At any point in your application, there's exactly one place to look to understand what's happening. You don't have to hunt through multiple variables, caches, or data stores.\n",
    "\n",
    "**2. Predictable Data Flow**\n",
    "Data flows in one direction: from the current state, through a node, and into the updated state. This makes it easy to trace how data changes.\n",
    "\n",
    "**3. Time-Travel Debugging**\n",
    "Because LangGraph can checkpoint your state at each step, you can \"rewind\" to any point in execution and see exactly what the state looked like. This is invaluable for debugging.\n",
    "\n",
    "**4. Easy Testing**\n",
    "Each node becomes a pure function: given a specific state input, it always produces the same output. No hidden dependencies on global variables or external state.\n",
    "\n",
    "**5. Resumable Workflows**\n",
    "If your application crashes or is interrupted, you can resume from the last checkpoint because all the context is in the state object.\n",
    "\n",
    "Let's see the difference:\n",
    "\n",
    "```python\n",
    "# ✅ GOOD: Centralized state\n",
    "class CentralizedState(TypedDict):\n",
    "    user_input: str\n",
    "    conversation_history: list\n",
    "    retrieved_documents: list\n",
    "    analysis_results: dict\n",
    "    final_response: str\n",
    "    metadata: dict\n",
    "\n",
    "# All nodes work with the same state structure\n",
    "def retrieve_docs(state: CentralizedState) -> CentralizedState:\n",
    "    docs = search(state[\"user_input\"])\n",
    "    return {\"retrieved_documents\": docs}\n",
    "\n",
    "def analyze(state: CentralizedState) -> CentralizedState:\n",
    "    results = analyze_docs(state[\"retrieved_documents\"])\n",
    "    return {\"analysis_results\": results}\n",
    "```\n",
    "\n",
    "```python\n",
    "# ❌ BAD: Decentralized, scattered state\n",
    "# Node-specific state (hard to track)\n",
    "retriever_cache = {}\n",
    "analyzer_results = []\n",
    "global_config = {}\n",
    "\n",
    "def retrieve_docs(query: str):\n",
    "    retriever_cache[query] = search(query)  # Side effect\n",
    "    \n",
    "def analyze(query: str):\n",
    "    docs = retriever_cache.get(query)  # Implicit dependency\n",
    "    analyzer_results.append(analyze_docs(docs))  # Global mutation\n",
    "```\n",
    "\n",
    "**Why Centralization Wins**:\n",
    "- Single source of truth\n",
    "- Clear data flow\n",
    "- Easier debugging and inspection\n",
    "- Testable without side effects\n",
    "- Natural checkpointing support\n",
    "- Thread-safe when designed properly\n",
    "\n",
    "### State Object Design Patterns\n",
    "\n",
    "Choosing the right state structure is crucial. Here are three common patterns, each suited for different scenarios:\n",
    "\n",
    "#### 1. Flat State Pattern\n",
    "\n",
    "**What is it?**\n",
    "All fields are at the top level of your state dictionary - no nesting, no complex structures. Every piece of data is directly accessible with a single key.\n",
    "\n",
    "**When should you use it?**\n",
    "- You're building a simple workflow with fewer than 10-15 state fields\n",
    "- Your application has a single, clear purpose (like a simple Q&A bot)\n",
    "- You're prototyping or learning LangGraph\n",
    "- Speed of access is critical (flat lookups are fastest)\n",
    "\n",
    "**Real-world analogy**: Think of a flat state like a small desk with everything laid out in front of you. Easy to see everything at a glance, but gets messy if you have too many items.\n",
    "\n",
    "**Use When**: Simple workflows, few fields, minimal nesting\n",
    "\n",
    "```python\n",
    "class FlatState(TypedDict):\n",
    "    query: str\n",
    "    response: str\n",
    "    confidence: float\n",
    "    status: str\n",
    "    error: Optional[str]\n",
    "```\n",
    "\n",
    "**Pros**: \n",
    "- Simple to understand\n",
    "- Fast access\n",
    "- Easy to validate\n",
    "\n",
    "**Cons**:\n",
    "- Can become cluttered with many fields\n",
    "- Hard to organize related data\n",
    "- No namespace isolation\n",
    "\n",
    "#### 2. Domain-Segregated State Pattern\n",
    "\n",
    "**What is it?**\n",
    "Your state is organized into logical domains or concerns, with related data grouped together in nested dictionaries. Each domain represents a different aspect of your application.\n",
    "\n",
    "**When should you use it?**\n",
    "- You have a complex workflow with many different concerns (user management, document retrieval, analysis, generation, etc.)\n",
    "- Multiple team members are working on different parts of the system\n",
    "- You want clear separation between different types of data\n",
    "- Your application handles multiple domains (e.g., an enterprise AI assistant that manages users, documents, conversations, and analytics)\n",
    "\n",
    "**Real-world analogy**: Think of this like a filing cabinet with labeled drawers. Each drawer (domain) contains related documents, making it easy to find what you need and keep things organized.\n",
    "\n",
    "**How it works**:\n",
    "- Each top-level key represents a domain (e.g., \"user\", \"request\", \"processing\")\n",
    "- Related fields are nested under that domain\n",
    "- Nodes that work with a specific domain primarily interact with that section of state\n",
    "\n",
    "**Use When**: Complex workflows, multiple concerns, clear domains\n",
    "\n",
    "```python\n",
    "class DomainSegregatedState(TypedDict):\n",
    "    # User domain\n",
    "    user: dict  # {id, name, preferences, history}\n",
    "    \n",
    "    # Request domain\n",
    "    request: dict  # {query, timestamp, session_id, context}\n",
    "    \n",
    "    # Processing domain\n",
    "    processing: dict  # {current_step, retries, errors, logs}\n",
    "    \n",
    "    # Knowledge domain\n",
    "    knowledge: dict  # {documents, embeddings, sources}\n",
    "    \n",
    "    # Response domain\n",
    "    response: dict  # {text, confidence, citations, metadata}\n",
    "```\n",
    "\n",
    "**Pros**:\n",
    "- Clear organization\n",
    "- Logical grouping\n",
    "- Easier to reason about\n",
    "- Better for team collaboration\n",
    "\n",
    "**Cons**:\n",
    "- More verbose access patterns\n",
    "- Requires nested updates\n",
    "- Slightly more complex\n",
    "\n",
    "#### 3. Hybrid State Pattern (Recommended for Production)\n",
    "\n",
    "**What is it?**\n",
    "The hybrid pattern combines the best of both worlds: frequently accessed fields remain flat at the top level for quick access, while related complex data is organized into domains.\n",
    "\n",
    "**When should you use it?**\n",
    "This is the **recommended pattern for production applications** because it balances:\n",
    "- **Performance**: Quick access to common fields (user_id, session_id, current_step)\n",
    "- **Organization**: Complex data grouped logically\n",
    "- **Clarity**: Clear semantics through reducers (what accumulates vs. what gets overwritten)\n",
    "- **Maintainability**: Easy to understand and extend\n",
    "\n",
    "**Real-world analogy**: Think of this like a workstation with frequently used tools on the desk surface (flat, quick access) and organized drawers for everything else (grouped by category).\n",
    "\n",
    "**Key principles**:\n",
    "1. **Flat identifiers**: Keep IDs and commonly accessed fields at the top level\n",
    "2. **Accumulating data with reducers**: Use `Annotated` types to specify how lists grow\n",
    "3. **Current state as overwrites**: Fields representing \"current\" status use overwrite semantics\n",
    "4. **Domain nesting for complex data**: Group related complex data into dictionaries\n",
    "5. **Separate metadata**: Keep system metadata separate from business data\n",
    "\n",
    "**Use When**: Production applications, need both simplicity and organization\n",
    "\n",
    "```python\n",
    "from typing import Annotated, Optional\n",
    "from operator import add\n",
    "\n",
    "class ProductionState(TypedDict):\n",
    "    # === CORE IDENTIFIERS (flat for quick access) ===\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    request_id: str\n",
    "    \n",
    "    # === ACCUMULATING DATA (using reducers) ===\n",
    "    messages: Annotated[list, add]\n",
    "    tool_calls: Annotated[list, add]\n",
    "    errors: Annotated[list, add]\n",
    "    \n",
    "    # === CURRENT STATE (overwrite semantics) ===\n",
    "    current_query: str\n",
    "    current_step: str\n",
    "    confidence: float\n",
    "    \n",
    "    # === DOMAIN-SPECIFIC (organized by concern) ===\n",
    "    retrieval: dict  # Documents, sources, scores\n",
    "    analysis: dict   # Reasoning, evidence, conclusions\n",
    "    generation: dict # Response drafts, final output\n",
    "    \n",
    "    # === METADATA (separate from business logic) ===\n",
    "    metadata: dict  # Timestamps, versions, flags\n",
    "```\n",
    "\n",
    "**Why This Works**:\n",
    "- Quick access to common fields (flat top-level)\n",
    "- Organized complex data (nested domains)\n",
    "- Clear semantics (reducers for accumulation)\n",
    "- Separation of concerns (metadata isolated)\n",
    "\n",
    "### State Access Patterns\n",
    "\n",
    "Understanding how to properly access and update state is crucial. Here are the fundamental patterns:\n",
    "\n",
    "#### Read-Only Access\n",
    "\n",
    "**What is it?**\n",
    "A node that reads data from state to make decisions or perform computations, but doesn't need to modify the state (or only modifies metadata like the current step).\n",
    "\n",
    "**When to use it?**\n",
    "- Logging nodes that just record what's happening\n",
    "- Validation nodes that check state but don't change it\n",
    "- Router nodes that decide which path to take\n",
    "- Monitoring nodes that extract metrics\n",
    "\n",
    "**Key concept**: Even if you're just reading, you still need to return a dictionary. If nothing needs to change, return an empty dict `{}` or just update tracking fields.\n",
    "```python\n",
    "def node_that_reads(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Node that only reads state\"\"\"\n",
    "    query = state[\"current_query\"]\n",
    "    history = state[\"messages\"]\n",
    "    \n",
    "    # Process without modifying state\n",
    "    result = process(query, history)\n",
    "    \n",
    "    # Return updates (not mutations)\n",
    "    return {\"current_step\": \"processed\"}\n",
    "```\n",
    "\n",
    "#### Selective Updates\n",
    "\n",
    "**What is it?**\n",
    "The most common pattern - a node updates only the specific fields it's responsible for, leaving everything else unchanged.\n",
    "\n",
    "**Why is this powerful?**\n",
    "This is the key to composability in LangGraph. Each node focuses on its specific responsibility without worrying about the rest of the state. LangGraph automatically merges your updates with the existing state.\n",
    "\n",
    "**Mental model**: Think of state updates like applying a patch. You only specify what changed, not the entire state.\n",
    "\n",
    "**Important**: You never need to copy the entire state and modify it. Just return the changes!\n",
    "```python\n",
    "def node_that_updates_selectively(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Update only specific fields\"\"\"\n",
    "    # Only return what changed\n",
    "    return {\n",
    "        \"confidence\": 0.95,\n",
    "        \"current_step\": \"analysis_complete\"\n",
    "    }\n",
    "    # Other fields remain unchanged\n",
    "```\n",
    "\n",
    "#### Deep Updates (Nested State)\n",
    "\n",
    "**What is it?**\n",
    "When you have nested dictionaries in your state, you need a strategy for updating values deep within the structure without overwriting the entire nested object.\n",
    "\n",
    "**The problem**:\n",
    "If your state has `config: {llm: {temperature: 0.5, model: \"gpt-4\"}}` and you just return `{\"config\": {\"llm\": {\"temperature\": 0.7}}}`, you'll lose the \"model\" field!\n",
    "\n",
    "**The solution**:\n",
    "Use a custom reducer that performs deep merging - it recursively merges nested dictionaries instead of replacing them.\n",
    "\n",
    "**When to use it?**\n",
    "- You have nested configuration objects\n",
    "- You need to update deep values without affecting siblings\n",
    "- You're dealing with complex, hierarchical data structures\n",
    "```python\n",
    "def custom_merge_dict(existing: dict, update: dict) -> dict:\n",
    "    \"\"\"Deep merge for nested dictionaries\"\"\"\n",
    "    result = existing.copy()\n",
    "    for key, value in update.items():\n",
    "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "            result[key] = custom_merge_dict(result[key], value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "class StateWithDeepMerge(TypedDict):\n",
    "    config: Annotated[dict, custom_merge_dict]\n",
    "\n",
    "def update_nested_config(state: StateWithDeepMerge) -> StateWithDeepMerge:\n",
    "    return {\n",
    "        \"config\": {\n",
    "            \"llm\": {\n",
    "                \"temperature\": 0.7  # Deep update\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "### State Initialization Best Practices\n",
    "\n",
    "```python\n",
    "def create_initial_state(\n",
    "    user_id: str,\n",
    "    query: str,\n",
    "    session_id: Optional[str] = None\n",
    ") -> ProductionState:\n",
    "    \"\"\"Factory function for consistent state initialization\"\"\"\n",
    "    return {\n",
    "        # Identifiers\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": session_id or generate_session_id(),\n",
    "        \"request_id\": generate_request_id(),\n",
    "        \n",
    "        # Empty accumulators\n",
    "        \"messages\": [],\n",
    "        \"tool_calls\": [],\n",
    "        \"errors\": [],\n",
    "        \n",
    "        # Initial values\n",
    "        \"current_query\": query,\n",
    "        \"current_step\": \"initialized\",\n",
    "        \"confidence\": 0.0,\n",
    "        \n",
    "        # Empty domains\n",
    "        \"retrieval\": {},\n",
    "        \"analysis\": {},\n",
    "        \"generation\": {},\n",
    "        \n",
    "        # Metadata\n",
    "        \"metadata\": {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"version\": \"1.0\",\n",
    "            \"flags\": {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "initial_state = create_initial_state(\n",
    "    user_id=\"user_123\",\n",
    "    query=\"What is LangGraph?\"\n",
    ")\n",
    "```\n",
    "\n",
    "### State Inspection and Debugging\n",
    "\n",
    "```python\n",
    "def inspect_state(state: ProductionState, step: str) -> None:\n",
    "    \"\"\"Debug helper to inspect state at checkpoints\"\"\"\n",
    "    print(f\"\\n=== STATE AT {step} ===\")\n",
    "    print(f\"Step: {state['current_step']}\")\n",
    "    print(f\"Messages: {len(state['messages'])}\")\n",
    "    print(f\"Tool Calls: {len(state['tool_calls'])}\")\n",
    "    print(f\"Confidence: {state['confidence']:.2f}\")\n",
    "    print(f\"Errors: {state['errors']}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Use in nodes\n",
    "def processing_node(state: ProductionState) -> ProductionState:\n",
    "    inspect_state(state, \"BEFORE_PROCESSING\")\n",
    "    \n",
    "    result = process(state)\n",
    "    \n",
    "    inspect_state(result, \"AFTER_PROCESSING\")\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Typed State Schemas\n",
    "\n",
    "### Why Type Safety Matters\n",
    "\n",
    "In dynamically typed languages like Python, you can put anything anywhere - which is flexible but dangerous. Type safety in state management is like having guardrails on a highway: it prevents you from making common mistakes before they cause runtime errors.\n",
    "\n",
    "**What problems does type safety solve?**\n",
    "\n",
    "1. **Typos and Misspellings**: Without types, `state[\"confidense\"]` won't raise an error until runtime (and maybe not even then!). With types, your IDE catches this immediately.\n",
    "\n",
    "2. **Type Mismatches**: Trying to add a string to an integer? Your type checker catches it before you run the code.\n",
    "\n",
    "3. **Missing Fields**: Forgot to initialize a required field? Type checking tells you during development, not in production.\n",
    "\n",
    "4. **Refactoring Safety**: When you rename a field, type checking finds every place you need to update it.\n",
    "\n",
    "5. **Documentation**: Types serve as inline documentation - you know exactly what each field should contain.\n",
    "\n",
    "6. **IDE Support**: Autocomplete, go-to-definition, and inline documentation all work better with types.\n",
    "\n",
    "**Real-world impact**: In production systems, type-related bugs caught during development are bugs that never make it to your users.\n",
    "\n",
    "### TypedDict Approach (Standard)\n",
    "\n",
    "**What is TypedDict?**\n",
    "TypedDict is Python's built-in way to add type hints to dictionary structures. It looks like a class, but it's really just a type annotation - at runtime, your state is still a regular Python dictionary.\n",
    "\n",
    "**Why use TypedDict for LangGraph?**\n",
    "- **Native Python**: No external dependencies needed\n",
    "- **Lightweight**: Zero runtime overhead - it's just type hints\n",
    "- **Perfect for LangGraph**: LangGraph state is dictionary-based, so TypedDict is a natural fit\n",
    "- **Good IDE support**: Modern IDEs understand TypedDict and provide autocomplete\n",
    "\n",
    "**How it works**:\n",
    "You define a TypedDict class that declares what keys exist and what type each value should be. The type checker (like mypy or your IDE's built-in checker) then validates your code against this schema.\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, Optional, Literal\n",
    "\n",
    "class BasicTypedState(TypedDict):\n",
    "    \"\"\"Basic type safety with TypedDict\"\"\"\n",
    "    user_id: str\n",
    "    query: str\n",
    "    confidence: float\n",
    "    status: Literal[\"pending\", \"processing\", \"complete\", \"error\"]\n",
    "    result: Optional[str]\n",
    "\n",
    "# Type checking catches errors\n",
    "def process(state: BasicTypedState) -> BasicTypedState:\n",
    "    # ✅ IDE knows these fields exist\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # ❌ Type checker warns about this\n",
    "    # invalid = state[\"nonexistent_field\"]\n",
    "    \n",
    "    return {\"status\": \"complete\", \"result\": \"done\"}\n",
    "```\n",
    "\n",
    "#### Total vs. Non-Total TypedDict\n",
    "\n",
    "**What's the difference?**\n",
    "\n",
    "**Total TypedDict** (the default): ALL fields are required. If you create a state dictionary, it must have every field defined, or the type checker complains.\n",
    "\n",
    "**Non-Total TypedDict**: All fields are optional. You can create a state with only some fields present.\n",
    "\n",
    "**When to use each?**\n",
    "\n",
    "- **Use Total (default)** when: You need strict validation, all fields should always exist, you're defining the complete state shape\n",
    "- **Use Non-Total** when: Fields are truly optional, you're building incrementally, different nodes add different fields\n",
    "\n",
    "**The Hybrid Approach**: Often the best solution is to have required fields in a Total TypedDict and optional fields in a Non-Total one, then combine them using inheritance.\n",
    "\n",
    "```python\n",
    "from typing import TypedDict\n",
    "\n",
    "# All fields required\n",
    "class StrictState(TypedDict):\n",
    "    required_field: str\n",
    "    another_required: int\n",
    "\n",
    "# Some fields optional\n",
    "class FlexibleState(TypedDict, total=False):\n",
    "    optional_field: str\n",
    "    another_optional: int\n",
    "\n",
    "# Mix of required and optional\n",
    "class RequiredFields(TypedDict):\n",
    "    user_id: str  # Required\n",
    "    query: str    # Required\n",
    "\n",
    "class MixedState(RequiredFields, total=False):\n",
    "    confidence: float  # Optional\n",
    "    metadata: dict     # Optional\n",
    "```\n",
    "\n",
    "### Pydantic Models (Advanced)\n",
    "\n",
    "**What is Pydantic?**\n",
    "Pydantic is a powerful data validation library that goes far beyond simple type hints. It validates data at runtime, transforms it, and provides detailed error messages when something is wrong.\n",
    "\n",
    "**TypedDict vs. Pydantic: Key Differences**\n",
    "\n",
    "| Feature | TypedDict | Pydantic |\n",
    "|---------|-----------|----------|\n",
    "| Validation | Static only (IDE/mypy) | Runtime validation |\n",
    "| Type coercion | No | Yes (e.g., \"123\" → 123) |\n",
    "| Constraints | No | Yes (min/max, regex, etc.) |\n",
    "| Transformation | No | Yes (cleaning, normalization) |\n",
    "| Error messages | Generic type errors | Detailed validation errors |\n",
    "| Performance | Zero overhead | Small validation cost |\n",
    "| Complexity | Simple | More powerful |\n",
    "\n",
    "**When should you use Pydantic?**\n",
    "\n",
    "1. **User Input**: When accepting data from external sources (APIs, user uploads, etc.) that might be malformed\n",
    "2. **Complex Validation**: When you need to enforce business rules (e.g., age > 18, email format, value ranges)\n",
    "3. **Data Cleaning**: When you need to normalize or transform data (trim strings, convert formats)\n",
    "4. **Better Errors**: When you want detailed error messages about what went wrong\n",
    "5. **Production Systems**: When you need robust validation to prevent bad data from corrupting your workflow\n",
    "\n",
    "**The Trade-off**: Pydantic adds runtime overhead and complexity. Use it when you need the power; stick with TypedDict when you don't.\n",
    "\n",
    "**Use When**: Need validation, transformation, or complex constraints\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field, validator, root_validator\n",
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "\n",
    "class PydanticState(BaseModel):\n",
    "    \"\"\"Advanced state with validation\"\"\"\n",
    "    \n",
    "    # Required fields with constraints\n",
    "    user_id: str = Field(..., min_length=1, max_length=100)\n",
    "    query: str = Field(..., min_length=1, max_length=1000)\n",
    "    \n",
    "    # Optional with defaults\n",
    "    confidence: float = Field(default=0.0, ge=0.0, le=1.0)\n",
    "    retries: int = Field(default=0, ge=0, le=5)\n",
    "    \n",
    "    # Complex types\n",
    "    messages: List[dict] = Field(default_factory=list)\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    \n",
    "    # Computed fields\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    \n",
    "    # Field-level validation\n",
    "    @validator('query')\n",
    "    def clean_query(cls, v):\n",
    "        \"\"\"Sanitize user query\"\"\"\n",
    "        return v.strip().lower()\n",
    "    \n",
    "    @validator('confidence')\n",
    "    def validate_confidence(cls, v):\n",
    "        \"\"\"Ensure confidence is valid\"\"\"\n",
    "        if v < 0 or v > 1:\n",
    "            raise ValueError(\"Confidence must be between 0 and 1\")\n",
    "        return v\n",
    "    \n",
    "    # Cross-field validation\n",
    "    @root_validator\n",
    "    def check_retry_logic(cls, values):\n",
    "        \"\"\"Validate retry state\"\"\"\n",
    "        if values.get('retries', 0) > 3 and values.get('confidence', 0) > 0.5:\n",
    "            raise ValueError(\"High confidence with many retries is suspicious\")\n",
    "        return values\n",
    "    \n",
    "    class Config:\n",
    "        # Allow extra fields (useful for extensibility)\n",
    "        extra = 'allow'\n",
    "        # Validate on assignment\n",
    "        validate_assignment = True\n",
    "        # Use enum values\n",
    "        use_enum_values = True\n",
    "\n",
    "# Usage in LangGraph\n",
    "def validated_node(state: dict) -> dict:\n",
    "    \"\"\"Node with automatic validation\"\"\"\n",
    "    # Parse and validate\n",
    "    validated = PydanticState(**state)\n",
    "    \n",
    "    # Work with validated data\n",
    "    result = process(validated.query)\n",
    "    \n",
    "    # Return as dict\n",
    "    return {\"result\": result, \"confidence\": 0.95}\n",
    "```\n",
    "\n",
    "### Dataclass Approach (Alternative)\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class DataclassState:\n",
    "    \"\"\"State using dataclasses\"\"\"\n",
    "    user_id: str\n",
    "    query: str\n",
    "    confidence: float = 0.0\n",
    "    messages: List[dict] = field(default_factory=list)\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validation after initialization\"\"\"\n",
    "        if not self.user_id:\n",
    "            raise ValueError(\"user_id cannot be empty\")\n",
    "        if not 0 <= self.confidence <= 1:\n",
    "            raise ValueError(\"confidence must be between 0 and 1\")\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dict for LangGraph\"\"\"\n",
    "        return {\n",
    "            \"user_id\": self.user_id,\n",
    "            \"query\": self.query,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"messages\": self.messages,\n",
    "            \"metadata\": self.metadata\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict) -> \"DataclassState\":\n",
    "        \"\"\"Create from dict\"\"\"\n",
    "        return cls(\n",
    "            user_id=data[\"user_id\"],\n",
    "            query=data[\"query\"],\n",
    "            confidence=data.get(\"confidence\", 0.0),\n",
    "            messages=data.get(\"messages\", []),\n",
    "            metadata=data.get(\"metadata\", {})\n",
    "        )\n",
    "```\n",
    "\n",
    "### Schema Evolution Patterns\n",
    "\n",
    "#### Version-Aware State\n",
    "\n",
    "```python\n",
    "from typing import Literal\n",
    "\n",
    "class VersionedState(TypedDict):\n",
    "    # Version field to track schema changes\n",
    "    schema_version: Literal[\"1.0\", \"1.1\", \"2.0\"]\n",
    "    \n",
    "    # Core fields (present in all versions)\n",
    "    user_id: str\n",
    "    query: str\n",
    "    \n",
    "    # Version-specific fields\n",
    "    response: Optional[str]  # Added in 1.1\n",
    "    confidence: Optional[float]  # Added in 2.0\n",
    "\n",
    "def migrate_state(state: dict) -> VersionedState:\n",
    "    \"\"\"Migrate old state to current schema\"\"\"\n",
    "    version = state.get(\"schema_version\", \"1.0\")\n",
    "    \n",
    "    if version == \"1.0\":\n",
    "        # Add fields from 1.1\n",
    "        state.setdefault(\"response\", None)\n",
    "        # Add fields from 2.0\n",
    "        state.setdefault(\"confidence\", 0.0)\n",
    "        state[\"schema_version\"] = \"2.0\"\n",
    "    \n",
    "    elif version == \"1.1\":\n",
    "        # Add fields from 2.0\n",
    "        state.setdefault(\"confidence\", 0.0)\n",
    "        state[\"schema_version\"] = \"2.0\"\n",
    "    \n",
    "    return state\n",
    "```\n",
    "\n",
    "#### Backward-Compatible Updates\n",
    "\n",
    "```python\n",
    "class BackwardCompatibleState(TypedDict, total=False):\n",
    "    \"\"\"All fields optional for backward compatibility\"\"\"\n",
    "    # V1 fields\n",
    "    user_id: str\n",
    "    query: str\n",
    "    \n",
    "    # V2 additions (optional)\n",
    "    session_id: str\n",
    "    timestamp: str\n",
    "    \n",
    "    # V3 additions (optional)\n",
    "    metadata: dict\n",
    "    flags: dict\n",
    "\n",
    "def ensure_required_fields(state: dict) -> BackwardCompatibleState:\n",
    "    \"\"\"Ensure minimum required fields exist\"\"\"\n",
    "    if \"user_id\" not in state:\n",
    "        state[\"user_id\"] = \"anonymous\"\n",
    "    if \"query\" not in state:\n",
    "        raise ValueError(\"query is required\")\n",
    "    \n",
    "    # Add defaults for new fields\n",
    "    state.setdefault(\"session_id\", generate_session_id())\n",
    "    state.setdefault(\"timestamp\", datetime.now().isoformat())\n",
    "    state.setdefault(\"metadata\", {})\n",
    "    state.setdefault(\"flags\", {})\n",
    "    \n",
    "    return state\n",
    "```\n",
    "\n",
    "### Type Hints for Complex Structures\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, List, Dict, Union, Annotated\n",
    "from operator import add\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"]\n",
    "    content: str\n",
    "    timestamp: str\n",
    "\n",
    "class Document(TypedDict):\n",
    "    id: str\n",
    "    content: str\n",
    "    score: float\n",
    "    metadata: dict\n",
    "\n",
    "class ComplexTypedState(TypedDict):\n",
    "    # Precise list types\n",
    "    messages: Annotated[List[Message], add]\n",
    "    documents: Annotated[List[Document], add]\n",
    "    \n",
    "    # Union types for flexibility\n",
    "    result: Union[str, dict, List[str]]\n",
    "    \n",
    "    # Nested structures\n",
    "    config: Dict[str, Union[str, int, float, bool]]\n",
    "    \n",
    "    # Optional complex types\n",
    "    error: Optional[Dict[str, any]]\n",
    "```\n",
    "\n",
    "### Runtime Type Checking\n",
    "\n",
    "```python\n",
    "from typing import get_type_hints, get_args, get_origin\n",
    "\n",
    "def validate_state_types(state: dict, state_class: type) -> None:\n",
    "    \"\"\"Runtime validation of state types\"\"\"\n",
    "    hints = get_type_hints(state_class)\n",
    "    \n",
    "    for field_name, expected_type in hints.items():\n",
    "        if field_name not in state:\n",
    "            continue  # Optional field\n",
    "        \n",
    "        value = state[field_name]\n",
    "        \n",
    "        # Handle simple types\n",
    "        if expected_type in (str, int, float, bool):\n",
    "            if not isinstance(value, expected_type):\n",
    "                raise TypeError(\n",
    "                    f\"{field_name} expected {expected_type}, got {type(value)}\"\n",
    "                )\n",
    "        \n",
    "        # Handle Optional\n",
    "        origin = get_origin(expected_type)\n",
    "        if origin is Union:\n",
    "            args = get_args(expected_type)\n",
    "            if type(None) in args:  # Optional\n",
    "                if value is not None and not isinstance(value, args[0]):\n",
    "                    raise TypeError(f\"{field_name} type mismatch\")\n",
    "\n",
    "# Usage\n",
    "validate_state_types(state, ProductionState)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. State Updates and Versioning\n",
    "\n",
    "### Understanding State Updates in LangGraph\n",
    "\n",
    "State updates in LangGraph work differently than traditional programming. Instead of mutating variables directly, you return dictionaries with your changes, and LangGraph merges them for you. But HOW they're merged depends on the type of field.\n",
    "\n",
    "**The Three Update Strategies**:\n",
    "1. **Overwrite** (default): New value replaces old value\n",
    "2. **Reduce** (accumulate): New value is combined with old value using a function\n",
    "3. **Custom**: You define exactly how values are merged\n",
    "\n",
    "Understanding these strategies is crucial because choosing the wrong one leads to bugs.\n",
    "\n",
    "### Update Mechanisms\n",
    "\n",
    "#### Partial Updates (Default Behavior)\n",
    "\n",
    "**What is it?**\n",
    "By default, when you return `{\"field_a\": \"new_value\"}` from a node, only `field_a` changes. All other fields in the state remain exactly as they were. This is called a \"partial update.\"\n",
    "\n",
    "**Why is this powerful?**\n",
    "Each node can focus on its specific responsibility without worrying about the rest of the state. You don't need to carry forward all the fields you didn't touch.\n",
    "\n",
    "**Mental model**: Think of state like a spreadsheet. When a node runs, it's like filling in specific cells - the other cells don't change.\n",
    "\n",
    "**Important gotcha**: For fields without a reducer, the new value REPLACES the old value completely. If `field_a` was a dict with 10 keys and you return `{\"field_a\": {\"new_key\": \"value\"}}`, the old 10 keys are gone!\n",
    "\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    field_a: str\n",
    "    field_b: int\n",
    "    field_c: float\n",
    "\n",
    "# Nodes return only what changed\n",
    "def node_1(state: State) -> State:\n",
    "    return {\"field_a\": \"updated\"}  # Only field_a changes\n",
    "\n",
    "def node_2(state: State) -> State:\n",
    "    return {\"field_b\": 42}  # Only field_b changes\n",
    "\n",
    "# State evolution:\n",
    "# Initial: {\"field_a\": \"initial\", \"field_b\": 0, \"field_c\": 0.0}\n",
    "# After node_1: {\"field_a\": \"updated\", \"field_b\": 0, \"field_c\": 0.0}\n",
    "# After node_2: {\"field_a\": \"updated\", \"field_b\": 42, \"field_c\": 0.0}\n",
    "```\n",
    "\n",
    "#### Accumulative Updates (Reducers)\n",
    "\n",
    "**What is a reducer?**\n",
    "A reducer is a function that takes the old value and the new value and combines them into a single result. The most common reducer is `add`, which works differently depending on the type:\n",
    "- For numbers: `old + new` (addition)\n",
    "- For lists: `old + new` (concatenation)\n",
    "- For strings: `old + new` (concatenation)\n",
    "\n",
    "**Why use reducers?**\n",
    "Many things in your application naturally accumulate:\n",
    "- Messages in a conversation (you add new ones to the list)\n",
    "- Log entries (you append, never delete)\n",
    "- Token counts (you sum them up)\n",
    "- Costs (you accumulate charges)\n",
    "\n",
    "Without a reducer, each node would have to manually read the old value and append to it. Reducers handle this automatically.\n",
    "\n",
    "**The Annotated syntax**:\n",
    "`Annotated[type, reducer_function]` tells LangGraph: \"For this field, don't overwrite - instead, use this reducer function to combine old and new values.\"\n",
    "\n",
    "**Common mistake**: Forgetting to use a reducer for lists means each node overwrites the entire list instead of appending to it!\n",
    "\n",
    "```python\n",
    "from typing import Annotated\n",
    "from operator import add\n",
    "\n",
    "class AccumulativeState(TypedDict):\n",
    "    # Lists are concatenated\n",
    "    logs: Annotated[List[str], add]\n",
    "    events: Annotated[List[dict], add]\n",
    "    \n",
    "    # Numbers are summed\n",
    "    total_cost: Annotated[float, add]\n",
    "    token_count: Annotated[int, add]\n",
    "\n",
    "def node_1(state: AccumulativeState) -> AccumulativeState:\n",
    "    return {\n",
    "        \"logs\": [\"Step 1 completed\"],\n",
    "        \"total_cost\": 0.05,\n",
    "        \"token_count\": 100\n",
    "    }\n",
    "\n",
    "def node_2(state: AccumulativeState) -> AccumulativeState:\n",
    "    return {\n",
    "        \"logs\": [\"Step 2 completed\"],\n",
    "        \"total_cost\": 0.03,\n",
    "        \"token_count\": 75\n",
    "    }\n",
    "\n",
    "# State evolution:\n",
    "# Initial: {\"logs\": [], \"events\": [], \"total_cost\": 0.0, \"token_count\": 0}\n",
    "# After node_1: {\"logs\": [\"Step 1 completed\"], ..., \"total_cost\": 0.05, \"token_count\": 100}\n",
    "# After node_2: {\"logs\": [\"Step 1 completed\", \"Step 2 completed\"], ..., \"total_cost\": 0.08, \"token_count\": 175}\n",
    "```\n",
    "\n",
    "#### Custom Reducer Updates\n",
    "\n",
    "**What is a custom reducer?**\n",
    "When the built-in reducers (`add`, `multiply`) don't fit your needs, you can write your own function that defines exactly how values should be combined.\n",
    "\n",
    "**Why would you need this?**\n",
    "Some use cases require special merging logic:\n",
    "- **Deduplication**: Add to a list, but only if the item isn't already there\n",
    "- **Max/Min**: Keep only the highest or lowest value\n",
    "- **Smart merging**: Merge dictionaries with priority rules\n",
    "- **Conditional updates**: Only update if certain conditions are met\n",
    "\n",
    "**How to write a custom reducer**:\n",
    "A reducer is just a function that takes two parameters:\n",
    "1. `existing` - the current value in state\n",
    "2. `new` - the value being added\n",
    "3. Returns the combined result\n",
    "\n",
    "**Important**: Your reducer should be a pure function - same inputs always produce the same output, with no side effects.\n",
    "\n",
    "```python\n",
    "from typing import Any\n",
    "\n",
    "def merge_with_priority(existing: dict, new: dict) -> dict:\n",
    "    \"\"\"Merge dicts, preferring non-empty values\"\"\"\n",
    "    result = existing.copy()\n",
    "    for key, value in new.items():\n",
    "        # Only update if new value is \"better\"\n",
    "        if key not in result or not result[key] or value:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "def max_value(existing: float, new: float) -> float:\n",
    "    \"\"\"Keep maximum value\"\"\"\n",
    "    return max(existing, new)\n",
    "\n",
    "def append_unique(existing: list, new: list) -> list:\n",
    "    \"\"\"Append only unique items\"\"\"\n",
    "    result = existing.copy()\n",
    "    for item in new:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "class CustomReducerState(TypedDict):\n",
    "    config: Annotated[dict, merge_with_priority]\n",
    "    max_confidence: Annotated[float, max_value]\n",
    "    unique_sources: Annotated[list, append_unique]\n",
    "```\n",
    "\n",
    "### State Versioning Strategies\n",
    "\n",
    "**Why version your state?**\n",
    "\n",
    "State versioning solves several critical problems:\n",
    "1. **Debugging**: When something goes wrong, you can see exactly what changed and when\n",
    "2. **Rollback**: You can revert to a previous version if an operation fails\n",
    "3. **Audit trail**: You have a complete history of all changes\n",
    "4. **Conflict detection**: You can detect when concurrent operations conflict\n",
    "5. **Reproducibility**: You can replay execution from any point\n",
    "\n",
    "Think of versioning like Git for your state - every change is tracked, and you can go back in time if needed.\n",
    "\n",
    "#### Strategy 1: Explicit Version Field\n",
    "\n",
    "**What is it?**\n",
    "You maintain a simple integer counter that increments every time state is updated. Each update gets a unique version number.\n",
    "\n",
    "**How it works**:\n",
    "- Initialize version to 0 or 1\n",
    "- Every node that modifies state increments the version\n",
    "- You can track which version each change was made at\n",
    "\n",
    "**Pros**:\n",
    "- Simple and lightweight\n",
    "- Easy to understand\n",
    "- Low overhead\n",
    "- Perfect for conflict detection\n",
    "\n",
    "**Cons**:\n",
    "- Doesn't tell you WHAT changed\n",
    "- Doesn't track WHEN changes occurred\n",
    "- Can't identify which node made which change\n",
    "\n",
    "**Best for**: Simple applications, basic conflict detection, minimal overhead scenarios\n",
    "\n",
    "```python\n",
    "class VersionedState(TypedDict):\n",
    "    version: int\n",
    "    user_id: str\n",
    "    data: dict\n",
    "    updated_at: str\n",
    "\n",
    "def increment_version(state: VersionedState) -> VersionedState:\n",
    "    \"\"\"Increment version on every update\"\"\"\n",
    "    return {\n",
    "        \"version\": state.get(\"version\", 0) + 1,\n",
    "        \"updated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def processing_node(state: VersionedState) -> VersionedState:\n",
    "    # Do work\n",
    "    result = process(state[\"data\"])\n",
    "    \n",
    "    # Update with version increment\n",
    "    updates = {\n",
    "        \"data\": result,\n",
    "        **increment_version(state)\n",
    "    }\n",
    "    return updates\n",
    "```\n",
    "\n",
    "#### Strategy 2: Timestamp-Based Versioning\n",
    "\n",
    "**What is it?**\n",
    "Instead of just a version number, you track WHEN each change occurred using timestamps. You also maintain a history log of all updates with their timestamps.\n",
    "\n",
    "**How it works**:\n",
    "- Track `created_at` (when state was born) and `updated_at` (last change)\n",
    "- Maintain an `update_history` list with timestamp + node name for each change\n",
    "- Each update adds an entry to the history\n",
    "\n",
    "**Pros**:\n",
    "- Temporal tracking: you know when things happened\n",
    "- Audit trail: you can see the sequence of operations\n",
    "- Node attribution: you know which node made each change\n",
    "- Useful for debugging time-based issues\n",
    "\n",
    "**Cons**:\n",
    "- Larger state size (history accumulates)\n",
    "- Need to manage history size (can grow unbounded)\n",
    "- Slightly more complex than version numbers\n",
    "\n",
    "**Best for**: Applications needing audit trails, debugging complex timing issues, production systems requiring compliance\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "class TimestampedState(TypedDict):\n",
    "    created_at: str\n",
    "    updated_at: str\n",
    "    update_history: Annotated[List[str], add]\n",
    "    data: dict\n",
    "\n",
    "def add_timestamp(node_name: str):\n",
    "    \"\"\"Decorator to add timestamps to updates\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(state: TimestampedState) -> TimestampedState:\n",
    "            result = func(state)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "            \n",
    "            result[\"updated_at\"] = timestamp\n",
    "            result.setdefault(\"update_history\", [])\n",
    "            result[\"update_history\"].append(\n",
    "                f\"{timestamp} - {node_name}\"\n",
    "            )\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@add_timestamp(\"retrieval\")\n",
    "def retrieval_node(state: TimestampedState) -> TimestampedState:\n",
    "    return {\"data\": {\"docs\": retrieve(state)}}\n",
    "```\n",
    "\n",
    "#### Strategy 3: Immutable Event Log (Event Sourcing)\n",
    "\n",
    "**What is it?**\n",
    "Instead of storing just the current state, you store EVERY change as an immutable event. The current state is derived by \"replaying\" all events.\n",
    "\n",
    "**The Big Idea**:\n",
    "Traditional approach: Store the current state, lose history\n",
    "Event sourcing: Store all changes, derive current state\n",
    "\n",
    "**How it works**:\n",
    "1. Never delete or modify past events\n",
    "2. Every state change is recorded as an event with type, timestamp, data, and node\n",
    "3. Current state is computed by replaying all events in order\n",
    "4. To go back in time, just replay events up to that point\n",
    "\n",
    "**Pros**:\n",
    "- **Complete history**: Never lose information about what happened\n",
    "- **Time travel**: Can reconstruct state at any point in history\n",
    "- **Audit perfection**: Complete, immutable record of all changes\n",
    "- **Debugging**: Can replay execution to find bugs\n",
    "- **Reproducibility**: Can exactly reproduce any state\n",
    "\n",
    "**Cons**:\n",
    "- **Complexity**: More complex to implement and reason about\n",
    "- **Storage**: Event log grows forever (need archival strategy)\n",
    "- **Performance**: Computing state from events can be slow (use snapshots)\n",
    "\n",
    "**When to use it**:\n",
    "- Financial systems (need perfect audit trail)\n",
    "- Debugging complex state machines\n",
    "- Systems requiring compliance/audit\n",
    "- When you need perfect reproducibility\n",
    "\n",
    "**Important**: This is overkill for most applications! Use only when you truly need the power of event sourcing.\n",
    "\n",
    "```python\n",
    "from typing import Annotated, List\n",
    "from operator import add\n",
    "\n",
    "class Event(TypedDict):\n",
    "    type: str\n",
    "    timestamp: str\n",
    "    data: dict\n",
    "    node: str\n",
    "\n",
    "class EventSourcedState(TypedDict):\n",
    "    # Current state (derived from events)\n",
    "    current_data: dict\n",
    "    \n",
    "    # Complete history (never modified, only appended)\n",
    "    events: Annotated[List[Event], add]\n",
    "\n",
    "def create_event(event_type: str, node: str, data: dict) -> Event:\n",
    "    return {\n",
    "        \"type\": event_type,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"data\": data,\n",
    "        \"node\": node\n",
    "    }\n",
    "\n",
    "def event_sourced_node(state: EventSourcedState) -> EventSourcedState:\n",
    "    # Process\n",
    "    result = process(state[\"current_data\"])\n",
    "    \n",
    "    # Create event\n",
    "    event = create_event(\n",
    "        event_type=\"PROCESSING_COMPLETE\",\n",
    "        node=\"processor\",\n",
    "        data={\"result\": result}\n",
    "    )\n",
    "    \n",
    "    # Return update\n",
    "    return {\n",
    "        \"current_data\": result,\n",
    "        \"events\": [event]  # Appended to history\n",
    "    }\n",
    "\n",
    "def replay_events(events: List[Event]) -> dict:\n",
    "    \"\"\"Reconstruct state from event history\"\"\"\n",
    "    state = {}\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"PROCESSING_COMPLETE\":\n",
    "            state.update(event[\"data\"][\"result\"])\n",
    "        # Handle other event types...\n",
    "    return state\n",
    "```\n",
    "\n",
    "### State Snapshot Management\n",
    "\n",
    "```python\n",
    "class SnapshotState(TypedDict):\n",
    "    # Working data\n",
    "    data: dict\n",
    "    \n",
    "    # Snapshots for rollback\n",
    "    snapshots: dict  # {snapshot_id: state_data}\n",
    "    current_snapshot_id: Optional[str]\n",
    "\n",
    "def create_snapshot(state: SnapshotState, snapshot_id: str) -> SnapshotState:\n",
    "    \"\"\"Save current state as named snapshot\"\"\"\n",
    "    snapshots = state.get(\"snapshots\", {}).copy()\n",
    "    snapshots[snapshot_id] = {\n",
    "        \"data\": state[\"data\"].copy(),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"snapshots\": snapshots,\n",
    "        \"current_snapshot_id\": snapshot_id\n",
    "    }\n",
    "\n",
    "def restore_snapshot(state: SnapshotState, snapshot_id: str) -> SnapshotState:\n",
    "    \"\"\"Restore to previous snapshot\"\"\"\n",
    "    if snapshot_id not in state[\"snapshots\"]:\n",
    "        raise ValueError(f\"Snapshot {snapshot_id} not found\")\n",
    "    \n",
    "    return {\n",
    "        \"data\": state[\"snapshots\"][snapshot_id][\"data\"],\n",
    "        \"current_snapshot_id\": snapshot_id\n",
    "    }\n",
    "\n",
    "# Usage in graph\n",
    "def risky_operation(state: SnapshotState) -> SnapshotState:\n",
    "    # Create snapshot before risky work\n",
    "    updates = create_snapshot(state, \"before_risky_op\")\n",
    "    \n",
    "    try:\n",
    "        result = perform_risky_operation(state[\"data\"])\n",
    "        updates[\"data\"] = result\n",
    "    except Exception as e:\n",
    "        # Restore on failure\n",
    "        updates = restore_snapshot(state, \"before_risky_op\")\n",
    "        updates[\"error\"] = str(e)\n",
    "    \n",
    "    return updates\n",
    "```\n",
    "\n",
    "### Optimistic vs. Pessimistic Updates\n",
    "\n",
    "```python\n",
    "class TransactionalState(TypedDict):\n",
    "    # Committed data\n",
    "    committed_data: dict\n",
    "    \n",
    "    # Pending changes (not yet committed)\n",
    "    pending_changes: dict\n",
    "    \n",
    "    # Transaction status\n",
    "    transaction_status: Literal[\"idle\", \"pending\", \"committed\", \"rolled_back\"]\n",
    "\n",
    "def begin_transaction(state: TransactionalState) -> TransactionalState:\n",
    "    \"\"\"Start optimistic update\"\"\"\n",
    "    return {\n",
    "        \"pending_changes\": {},\n",
    "        \"transaction_status\": \"pending\"\n",
    "    }\n",
    "\n",
    "def add_pending_change(\n",
    "    state: TransactionalState,\n",
    "    key: str,\n",
    "    value: any\n",
    ") -> TransactionalState:\n",
    "    \"\"\"Add to pending changes\"\"\"\n",
    "    pending = state.get(\"pending_changes\", {}).copy()\n",
    "    pending[key] = value\n",
    "    return {\"pending_changes\": pending}\n",
    "\n",
    "def commit_transaction(state: TransactionalState) -> TransactionalState:\n",
    "    \"\"\"Commit pending changes\"\"\"\n",
    "    committed = state[\"committed_data\"].copy()\n",
    "    committed.update(state[\"pending_changes\"])\n",
    "    \n",
    "    return {\n",
    "        \"committed_data\": committed,\n",
    "        \"pending_changes\": {},\n",
    "        \"transaction_status\": \"committed\"\n",
    "    }\n",
    "\n",
    "def rollback_transaction(state: TransactionalState) -> TransactionalState:\n",
    "    \"\"\"Discard pending changes\"\"\"\n",
    "    return {\n",
    "        \"pending_changes\": {},\n",
    "        \"transaction_status\": \"rolled_back\"\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Memory Management Considerations\n",
    "\n",
    "### Why Memory Management Matters in LangGraph\n",
    "\n",
    "Unlike traditional applications where you might process one request and forget it, LangGraph applications often maintain state across many operations:\n",
    "- Conversational agents that remember context\n",
    "- Long-running workflows that accumulate data\n",
    "- Multi-step reasoning that builds up evidence\n",
    "- Systems that checkpoint state for resumption\n",
    "\n",
    "**The Problem**: Without careful memory management, your state grows unbounded, leading to:\n",
    "- Increased memory usage (eventually OOM crashes)\n",
    "- Slower performance (more data to serialize/deserialize)\n",
    "- Higher costs (more tokens to process, more storage)\n",
    "- Degraded quality (too much context confuses LLMs)\n",
    "\n",
    "**The Solution**: Proactive memory management strategies that prevent unbounded growth while preserving essential information.\n",
    "\n",
    "### State Size Management\n",
    "\n",
    "#### Problem: Unbounded Growth\n",
    "\n",
    "**What happens without memory management?**\n",
    "\n",
    "Imagine a chatbot that accumulates every message in a conversation:\n",
    "- Day 1: 10 messages → 5 KB\n",
    "- Week 1: 700 messages → 350 KB\n",
    "- Month 1: 3,000 messages → 1.5 MB\n",
    "- Year 1: 36,000 messages → 18 MB\n",
    "\n",
    "Each new message makes the system slower. Eventually, you hit memory limits or timeout issues.\n",
    "\n",
    "```python\n",
    "# ❌ BAD: State grows indefinitely\n",
    "class UnboundedState(TypedDict):\n",
    "    messages: Annotated[List[dict], add]  # Grows forever\n",
    "    all_documents: Annotated[List[dict], add]  # Never pruned\n",
    "    complete_history: Annotated[List[dict], add]  # Accumulates\n",
    "\n",
    "# After 1000 turns, state is huge and slow\n",
    "```\n",
    "\n",
    "#### Solution 1: Bounded Accumulators\n",
    "\n",
    "**What is it?**\n",
    "A bounded accumulator is a custom reducer that limits how many items can accumulate. When you reach the limit, old items are discarded (usually the oldest ones).\n",
    "\n",
    "**How it works**:\n",
    "Instead of using the standard `add` reducer that never removes items, you create a custom reducer that:\n",
    "1. Combines old and new values (like regular add)\n",
    "2. Checks if the result exceeds the maximum size\n",
    "3. Keeps only the most recent N items if it does\n",
    "\n",
    "**When to use it**:\n",
    "- Conversation history (keep last 50 messages)\n",
    "- Recent documents (keep last 20 retrieved docs)\n",
    "- Log entries (keep last 100 logs)\n",
    "- Any accumulating list where older items become less relevant\n",
    "\n",
    "**Key decisions**:\n",
    "- **Max size**: How many items to keep? (Balance: more context vs. performance)\n",
    "- **Discard strategy**: Remove oldest (FIFO) or lowest priority?\n",
    "- **What to keep**: Keep raw data or summarize old items?\n",
    "\n",
    "**Pro tip**: The max size should be based on:\n",
    "- Available memory\n",
    "- LLM context window limits\n",
    "- How far back context is still relevant\n",
    "- Performance requirements\n",
    "\n",
    "```python\n",
    "def bounded_add(max_size: int):\n",
    "    \"\"\"Create bounded list accumulator\"\"\"\n",
    "    def accumulator(existing: list, new: list) -> list:\n",
    "        combined = existing + new\n",
    "        # Keep only most recent items\n",
    "        return combined[-max_size:] if len(combined) > max_size else combined\n",
    "    return accumulator\n",
    "\n",
    "class BoundedState(TypedDict):\n",
    "    # Only keep last 50 messages\n",
    "    messages: Annotated[List[dict], bounded_add(50)]\n",
    "    \n",
    "    # Only keep last 20 documents\n",
    "    documents: Annotated[List[dict], bounded_add(20)]\n",
    "    \n",
    "    # Only keep last 100 log entries\n",
    "    logs: Annotated[List[str], bounded_add(100)]\n",
    "```\n",
    "\n",
    "#### Solution 2: Sliding Window\n",
    "\n",
    "**What is it?**\n",
    "A sliding window keeps items based on a time or count window. Unlike bounded accumulators that always keep the last N items, sliding windows can use time-based expiration.\n",
    "\n",
    "**Two flavors**:\n",
    "\n",
    "1. **Time-based window**: Keep items from the last X seconds/minutes/hours\n",
    "   - \"Keep messages from the last hour\"\n",
    "   - Automatically expires old items based on wall-clock time\n",
    "   - Great for real-time systems\n",
    "\n",
    "2. **Count-based window**: Keep the last N items (similar to bounded accumulator but with different semantics)\n",
    "   - \"Keep the last 10 interactions\"\n",
    "   - Fixed-size window\n",
    "   - Predictable memory usage\n",
    "\n",
    "**When to use time-based**:\n",
    "- Real-time monitoring (keep metrics from last 5 minutes)\n",
    "- Session management (expire after inactivity)\n",
    "- Temporary caching (cache valid for 1 hour)\n",
    "\n",
    "**When to use count-based**:\n",
    "- Fixed context size (always last 10 messages)\n",
    "- Predictable memory limits\n",
    "- Non-time-sensitive data\n",
    "\n",
    "**Important consideration**: Time-based windows require timestamps on every item. Make sure you're recording `timestamp` when adding items!\n",
    "\n",
    "```python\n",
    "def sliding_window(window_size: int, by_timestamp: bool = True):\n",
    "    \"\"\"Keep items within time/count window\"\"\"\n",
    "    def accumulator(existing: list, new: list) -> list:\n",
    "        combined = existing + new\n",
    "        \n",
    "        if by_timestamp:\n",
    "            # Keep items from last N seconds\n",
    "            cutoff = datetime.now() - timedelta(seconds=window_size)\n",
    "            return [\n",
    "                item for item in combined\n",
    "                if datetime.fromisoformat(item[\"timestamp\"]) > cutoff\n",
    "            ]\n",
    "        else:\n",
    "            # Keep last N items\n",
    "            return combined[-window_size:]\n",
    "    \n",
    "    return accumulator\n",
    "\n",
    "class SlidingWindowState(TypedDict):\n",
    "    # Keep messages from last 1 hour (3600 seconds)\n",
    "    recent_messages: Annotated[List[dict], sliding_window(3600, by_timestamp=True)]\n",
    "    \n",
    "    # Keep last 10 interactions\n",
    "    recent_interactions: Annotated[List[dict], sliding_window(10, by_timestamp=False)]\n",
    "```\n",
    "\n",
    "#### Solution 3: Summarization\n",
    "\n",
    "**What is it?**\n",
    "Instead of discarding old data entirely, you summarize it using an LLM and keep the summary. This preserves important information while dramatically reducing size.\n",
    "\n",
    "**The pattern**:\n",
    "1. Keep recent items in full detail (e.g., last 20 messages)\n",
    "2. Maintain a growing summary of older items\n",
    "3. When recent items exceed threshold, summarize the oldest and add to summary\n",
    "4. Prune the summarized items from the detailed list\n",
    "\n",
    "**Why this is powerful**:\n",
    "- Retains important context from the entire conversation\n",
    "- Dramatically reduces token count\n",
    "- Allows \"infinite\" conversation length\n",
    "- LLMs are good at summarizing their own output\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Cost**: Summarization requires LLM calls\n",
    "- **Latency**: Adds processing time\n",
    "- **Information loss**: Summaries lose details\n",
    "- **When to trigger**: How often do you summarize?\n",
    "\n",
    "**Best practices**:\n",
    "- Summarize in batches (e.g., every 10 messages, not every 1)\n",
    "- Include important metadata in summary (timestamps, key decisions)\n",
    "- Test summary quality - make sure important info is preserved\n",
    "- Consider hierarchical summarization (summarize summaries for very long contexts)\n",
    "\n",
    "**Perfect for**:\n",
    "- Long conversations (customer support, therapy bots)\n",
    "- Multi-session workflows (resume from summary)\n",
    "- Context that spans days/weeks\n",
    "- When you need \"infinite\" memory with finite resources\n",
    "\n",
    "```python\n",
    "class SummarizedState(TypedDict):\n",
    "    # Full recent history\n",
    "    recent_messages: Annotated[List[dict], bounded_add(20)]\n",
    "    \n",
    "    # Summarized old history\n",
    "    conversation_summary: str\n",
    "    \n",
    "    # Metadata\n",
    "    total_messages: Annotated[int, add]\n",
    "\n",
    "def summarize_and_prune(state: SummarizedState) -> SummarizedState:\n",
    "    \"\"\"Summarize old messages and prune\"\"\"\n",
    "    messages = state[\"recent_messages\"]\n",
    "    \n",
    "    if len(messages) > 20:\n",
    "        # Summarize oldest 10 messages\n",
    "        old_messages = messages[:10]\n",
    "        summary = llm_summarize(old_messages)\n",
    "        \n",
    "        # Keep only recent 10 + update summary\n",
    "        return {\n",
    "            \"recent_messages\": messages[10:],\n",
    "            \"conversation_summary\": f\"{state['conversation_summary']}\\n{summary}\",\n",
    "            \"total_messages\": len(messages)\n",
    "        }\n",
    "    \n",
    "    return {}\n",
    "```\n",
    "\n",
    "### Memory-Efficient Data Structures\n",
    "\n",
    "#### Use References Instead of Copies\n",
    "\n",
    "```python\n",
    "# ❌ BAD: Storing full document content\n",
    "class InefficientState(TypedDict):\n",
    "    documents: List[dict]  # Each dict contains full text\n",
    "\n",
    "def store_documents(documents: List[dict]) -> dict:\n",
    "    return {\"documents\": documents}  # Huge memory footprint\n",
    "\n",
    "# ✅ GOOD: Store references, retrieve when needed\n",
    "class EfficientState(TypedDict):\n",
    "    document_ids: List[str]  # Just IDs\n",
    "    document_cache: dict  # Optional: small cache\n",
    "\n",
    "def store_document_refs(documents: List[dict]) -> dict:\n",
    "    # Store in external system (DB, cache)\n",
    "    doc_ids = [store_in_db(doc) for doc in documents]\n",
    "    return {\"document_ids\": doc_ids}\n",
    "\n",
    "def retrieve_when_needed(state: EfficientState) -> List[dict]:\n",
    "    # Fetch on demand\n",
    "    return [fetch_from_db(doc_id) for doc_id in state[\"document_ids\"]]\n",
    "```\n",
    "\n",
    "#### Compress Large Text\n",
    "\n",
    "```python\n",
    "import gzip\n",
    "import base64\n",
    "\n",
    "def compress_text(text: str) -> str:\n",
    "    \"\"\"Compress text for storage\"\"\"\n",
    "    compressed = gzip.compress(text.encode('utf-8'))\n",
    "    return base64.b64encode(compressed).decode('utf-8')\n",
    "\n",
    "def decompress_text(compressed: str) -> str:\n",
    "    \"\"\"Decompress text\"\"\"\n",
    "    decoded = base64.b64decode(compressed.encode('utf-8'))\n",
    "    return gzip.decompress(decoded).decode('utf-8')\n",
    "\n",
    "class CompressedState(TypedDict):\n",
    "    # Store large text compressed\n",
    "    large_context: str  # Compressed version\n",
    "    \n",
    "def store_large_context(state: dict, context: str) -> dict:\n",
    "    return {\"large_context\": compress_text(context)}\n",
    "\n",
    "def use_large_context(state: CompressedState) -> str:\n",
    "    return decompress_text(state[\"large_context\"])\n",
    "```\n",
    "\n",
    "### Garbage Collection Strategies\n",
    "\n",
    "```python\n",
    "class GCState(TypedDict):\n",
    "    # Active data\n",
    "    active_data: dict\n",
    "    \n",
    "    # Cached results (can be cleared)\n",
    "    cache: dict\n",
    "    \n",
    "    # Temporary working space\n",
    "    temp: dict\n",
    "    \n",
    "    # GC metadata\n",
    "    last_gc_time: str\n",
    "    cache_size_bytes: int\n",
    "\n",
    "def estimate_size(obj: any) -> int:\n",
    "    \"\"\"Rough size estimation\"\"\"\n",
    "    import sys\n",
    "    if isinstance(obj, dict):\n",
    "        return sum(estimate_size(k) + estimate_size(v) for k, v in obj.items())\n",
    "    elif isinstance(obj, list):\n",
    "        return sum(estimate_size(item) for item in obj)\n",
    "    else:\n",
    "        return sys.getsizeof(obj)\n",
    "\n",
    "def garbage_collect(state: GCState, max_cache_mb: int = 10) -> GCState:\n",
    "    \"\"\"Periodically clean up state\"\"\"\n",
    "    cache_size = estimate_size(state.get(\"cache\", {}))\n",
    "    cache_size_mb = cache_size / (1024 * 1024)\n",
    "    \n",
    "    if cache_size_mb > max_cache_mb:\n",
    "        # Clear cache\n",
    "        return {\n",
    "            \"cache\": {},\n",
    "            \"temp\": {},\n",
    "            \"last_gc_time\": datetime.now().isoformat(),\n",
    "            \"cache_size_bytes\": 0\n",
    "        }\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def periodic_gc_node(state: GCState) -> GCState:\n",
    "    \"\"\"Node that performs GC if needed\"\"\"\n",
    "    last_gc = datetime.fromisoformat(state.get(\"last_gc_time\", \"2000-01-01T00:00:00\"))\n",
    "    \n",
    "    # Run GC every 5 minutes\n",
    "    if datetime.now() - last_gc > timedelta(minutes=5):\n",
    "        return garbage_collect(state)\n",
    "    \n",
    "    return {}\n",
    "```\n",
    "\n",
    "### External State Storage\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint import MemorySaver, SqliteSaver\n",
    "\n",
    "# ❌ BAD: Everything in memory\n",
    "checkpointer = MemorySaver()  # Limited by RAM\n",
    "\n",
    "# ✅ GOOD: Persistent storage\n",
    "checkpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\n",
    "\n",
    "# ✅ BETTER: Remote storage (Redis, Postgres)\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "checkpointer = PostgresSaver.from_conn_string(\n",
    "    \"postgresql://user:pass@host/db\"\n",
    ")\n",
    "\n",
    "# Application\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# State is persisted externally, not in memory\n",
    "result = app.invoke(\n",
    "    initial_state,\n",
    "    config={\"configurable\": {\"thread_id\": \"session_123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Avoiding Race Conditions\n",
    "\n",
    "### Understanding Race Conditions in LangGraph\n",
    "\n",
    "**What is a race condition?**\n",
    "A race condition occurs when the outcome of your program depends on the timing or order of uncontrollable events. When two or more nodes try to access or modify the same piece of state concurrently, and the final result depends on which one \"wins,\" you have a race condition.\n",
    "\n",
    "**Why should you care?**\n",
    "Race conditions are insidious bugs:\n",
    "- **Hard to reproduce**: They depend on timing, so they're intermittent\n",
    "- **Hard to debug**: They may only appear under load or in production\n",
    "- **Data corruption**: They can silently corrupt your state\n",
    "- **Non-deterministic**: Same input produces different output (breaks testing)\n",
    "\n",
    "**When do race conditions happen in LangGraph?**\n",
    "LangGraph can execute nodes in parallel when they don't have dependencies. If two parallel nodes try to modify the same state field without proper coordination, you get a race condition.\n",
    "\n",
    "### The Classic Race Condition Scenarios\n",
    "\n",
    "#### Scenario 1: Parallel Writes to Same Field\n",
    "\n",
    "**The Problem**: Two nodes running in parallel both read a value, compute a new value, and write it back. The second write overwrites the first, losing one update.\n",
    "\n",
    "**Real-world example**:\n",
    "Imagine a shopping cart where two items are added simultaneously by parallel nodes:\n",
    "- Cart starts at 0 items\n",
    "- Node A: reads 0, computes 1, writes 1\n",
    "- Node B: reads 0 (before A finishes), computes 1, writes 1\n",
    "- Final cart: 1 item (should be 2!)\n",
    "\n",
    "**Why it happens**:\n",
    "The \"read-compute-write\" sequence is not atomic. Between reading and writing, another node can interfere.\n",
    "\n",
    "**The pattern to recognize**:\n",
    "```python\n",
    "current_value = state[\"field\"]  # Read\n",
    "new_value = compute(current_value)  # Compute\n",
    "return {\"field\": new_value}  # Write\n",
    "```\n",
    "This is dangerous if two nodes run in parallel!\n",
    "\n",
    "```python\n",
    "# ❌ PROBLEM: Race condition\n",
    "class RacyState(TypedDict):\n",
    "    counter: int  # Not thread-safe\n",
    "\n",
    "def increment_counter(state: RacyState) -> RacyState:\n",
    "    # Read current value\n",
    "    current = state[\"counter\"]\n",
    "    \n",
    "    # Simulate processing time\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    # Write new value (may overwrite concurrent updates)\n",
    "    return {\"counter\": current + 1}\n",
    "\n",
    "# If two nodes run in parallel:\n",
    "# Node A reads counter=0, computes 1\n",
    "# Node B reads counter=0, computes 1\n",
    "# Both write 1, but should be 2!\n",
    "```\n",
    "\n",
    "#### Scenario 2: Check-Then-Act Pattern\n",
    "\n",
    "**The Problem**: A node checks a condition, then acts based on that condition. But between the check and the act, another node might change the state, making the original check invalid.\n",
    "\n",
    "**Real-world example**:\n",
    "Resource allocation (like booking seats):\n",
    "1. Node A checks: \"Are there available slots?\" (Yes, 1 slot)\n",
    "2. Node B checks: \"Are there available slots?\" (Yes, 1 slot) \n",
    "3. Node A: Takes the slot (0 remaining)\n",
    "4. Node B: Takes the slot (now -1 slots! Overbooking!)\n",
    "\n",
    "**Why it happens**:\n",
    "The condition is checked at one point in time, but by the time you act on it, the world has changed.\n",
    "\n",
    "**The pattern to recognize**:\n",
    "```python\n",
    "if state[\"available_slots\"] > 0:  # Check\n",
    "    # Time passes, another node might run\n",
    "    return {\"available_slots\": state[\"available_slots\"] - 1}  # Act\n",
    "```\n",
    "\n",
    "This is the classic \"time-of-check to time-of-use\" (TOCTOU) vulnerability.\n",
    "\n",
    "```python\n",
    "# ❌ PROBLEM: Race condition\n",
    "def check_and_update(state: dict) -> dict:\n",
    "    # Check condition\n",
    "    if state[\"available_slots\"] > 0:\n",
    "        # Another node might grab the slot here!\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Act on condition\n",
    "        return {\"available_slots\": state[\"available_slots\"] - 1}\n",
    "    return {}\n",
    "```\n",
    "\n",
    "### Solution 1: Use Atomic Reducers\n",
    "\n",
    "**What does \"atomic\" mean?**\n",
    "Atomic means \"all-or-nothing\" and \"indivisible.\" An atomic operation either completes entirely or doesn't happen at all, and it can't be interrupted halfway through.\n",
    "\n",
    "**How reducers solve race conditions**:\n",
    "When you use a reducer like `add`, LangGraph guarantees that all updates are applied atomically. It doesn't matter if 10 nodes run in parallel and all try to increment a counter - all increments are safely applied.\n",
    "\n",
    "**The magic**:\n",
    "Instead of:\n",
    "```\n",
    "Read value → Compute new value → Write value  (NOT atomic, race condition!)\n",
    "```\n",
    "\n",
    "You do:\n",
    "```\n",
    "Return increment → LangGraph applies atomically  (Atomic, safe!)\n",
    "```\n",
    "\n",
    "**Why this works**:\n",
    "You're not reading and writing yourself. You're just saying \"add this value,\" and LangGraph ensures the addition happens atomically.\n",
    "\n",
    "**Common atomic reducers**:\n",
    "- `add`: For numbers (sum) and lists (concatenate)\n",
    "- `multiply`: For numbers (product)\n",
    "- Custom atomic reducers you write\n",
    "\n",
    "**Important**: This only works if you ALWAYS use the reducer. If sometimes you use the reducer and sometimes you directly overwrite, you can still get race conditions!\n",
    "\n",
    "```python\n",
    "from typing import Annotated\n",
    "from operator import add\n",
    "\n",
    "class AtomicState(TypedDict):\n",
    "    # Atomic accumulation (no race condition)\n",
    "    counter: Annotated[int, add]\n",
    "    \n",
    "    # Atomic list append\n",
    "    results: Annotated[List[dict], add]\n",
    "\n",
    "def safe_increment(state: AtomicState) -> AtomicState:\n",
    "    # Just return increment value\n",
    "    # Reducer handles atomic addition\n",
    "    return {\"counter\": 1}\n",
    "\n",
    "# Even if multiple nodes run in parallel:\n",
    "# All increments are applied atomically\n",
    "# counter = 0 + 1 + 1 + 1 = 3 (correct)\n",
    "```\n",
    "\n",
    "### Solution 2: Sequential Execution for Conflicts\n",
    "\n",
    "**The Principle**: The simplest way to avoid race conditions is to not run conflicting operations in parallel!\n",
    "\n",
    "**When to use this**:\n",
    "If operations MUST happen in a specific order, or if they conflict with each other, just run them sequentially.\n",
    "\n",
    "**How to identify conflicts**:\n",
    "Two operations conflict if:\n",
    "1. They both modify the same state field (without a reducer)\n",
    "2. One reads a field that the other writes\n",
    "3. They have order dependencies (A must complete before B starts)\n",
    "\n",
    "**The trade-off**:\n",
    "- **Pro**: Eliminates race conditions completely\n",
    "- **Pro**: Simpler to reason about\n",
    "- **Con**: Slower (no parallelism benefits)\n",
    "- **Con**: May not scale well\n",
    "\n",
    "**When sequential is the right choice**:\n",
    "- Operations are fast (parallelism overhead > benefits)\n",
    "- Operations have dependencies anyway\n",
    "- Correctness is more important than speed\n",
    "- You're debugging and want to eliminate race conditions as a cause\n",
    "\n",
    "**When to seek alternatives**:\n",
    "- Operations are slow and independent\n",
    "- You need maximum throughput\n",
    "- Profiling shows parallelism helps significantly\n",
    "\n",
    "```python\n",
    "# Identify conflicting operations\n",
    "# Use sequential edges instead of parallel\n",
    "\n",
    "# ❌ BAD: Parallel access to same state\n",
    "graph.add_edge(\"start\", \"node_a\")  # Both modify \"counter\"\n",
    "graph.add_edge(\"start\", \"node_b\")  # Both modify \"counter\"\n",
    "\n",
    "# ✅ GOOD: Sequential access\n",
    "graph.add_edge(\"start\", \"node_a\")\n",
    "graph.add_edge(\"node_a\", \"node_b\")  # Sequential, no race\n",
    "```\n",
    "\n",
    "### Solution 3: Partition State\n",
    "\n",
    "**The Principle**: If two nodes never touch the same state fields, they can't have a race condition. Give each parallel operation its own \"workspace\" in the state.\n",
    "\n",
    "**The Pattern**:\n",
    "Instead of:\n",
    "```python\n",
    "shared_data: dict  # Both nodes modify this - race condition!\n",
    "```\n",
    "\n",
    "Do:\n",
    "```python\n",
    "node_a_data: dict  # Only node_a writes here\n",
    "node_b_data: dict  # Only node_b writes here\n",
    "combined_result: dict  # Aggregator combines them later\n",
    "```\n",
    "\n",
    "**How it works**:\n",
    "1. Each parallel node writes to its own state field\n",
    "2. Nodes can read shared, read-only data\n",
    "3. An aggregator node (running after both complete) combines the results\n",
    "4. No conflicts because each node has its own space\n",
    "\n",
    "**Benefits**:\n",
    "- Safe parallel execution\n",
    "- Clear ownership (who writes what)\n",
    "- Easy to debug (isolated changes)\n",
    "- Scales to many parallel nodes\n",
    "\n",
    "**When to use it**:\n",
    "- Multiple independent analyses of the same data\n",
    "- Fan-out/fan-in patterns\n",
    "- Map-reduce style operations\n",
    "- When results need to be combined\n",
    "\n",
    "**Design tip**: Name fields clearly to show ownership: `retrieval_node_results`, `analysis_node_results`, not generic names like `results1`, `results2`.\n",
    "\n",
    "```python\n",
    "# ❌ BAD: Shared state between parallel nodes\n",
    "class SharedState(TypedDict):\n",
    "    shared_counter: int  # Both nodes modify this\n",
    "\n",
    "# ✅ GOOD: Partitioned state\n",
    "class PartitionedState(TypedDict):\n",
    "    node_a_counter: int  # Only node_a modifies\n",
    "    node_b_counter: int  # Only node_b modifies\n",
    "    total: int          # Computed after both finish\n",
    "\n",
    "def node_a(state: PartitionedState) -> PartitionedState:\n",
    "    return {\"node_a_counter\": compute_a()}\n",
    "\n",
    "def node_b(state: PartitionedState) -> PartitionedState:\n",
    "    return {\"node_b_counter\": compute_b()}\n",
    "\n",
    "def aggregator(state: PartitionedState) -> PartitionedState:\n",
    "    # Combine results safely\n",
    "    return {\n",
    "        \"total\": state[\"node_a_counter\"] + state[\"node_b_counter\"]\n",
    "    }\n",
    "```\n",
    "\n",
    "### Solution 4: Read-Only State Access\n",
    "\n",
    "```python\n",
    "class ReadOnlyState(TypedDict):\n",
    "    # Immutable input data\n",
    "    config: dict  # Set once, never modified\n",
    "    user_context: dict  # Read-only reference\n",
    "    \n",
    "    # Write-partitioned outputs\n",
    "    node_a_result: Optional[dict]\n",
    "    node_b_result: Optional[dict]\n",
    "\n",
    "def parallel_node_a(state: ReadOnlyState) -> ReadOnlyState:\n",
    "    # Read shared data (safe)\n",
    "    config = state[\"config\"]\n",
    "    \n",
    "    # Write to own partition (safe)\n",
    "    result = process_a(config)\n",
    "    return {\"node_a_result\": result}\n",
    "\n",
    "def parallel_node_b(state: ReadOnlyState) -> ReadOnlyState:\n",
    "    # Read shared data (safe)\n",
    "    config = state[\"config\"]\n",
    "    \n",
    "    # Write to own partition (safe)\n",
    "    result = process_b(config)\n",
    "    return {\"node_b_result\": result}\n",
    "```\n",
    "\n",
    "### Solution 5: Idempotent Operations\n",
    "\n",
    "**What is idempotency?**\n",
    "An idempotent operation produces the same result whether you execute it once or multiple times. No matter how many times you call it, the outcome is the same.\n",
    "\n",
    "**Examples**:\n",
    "- **Idempotent**: Setting a value (`x = 5` - running it 100 times still sets x to 5)\n",
    "- **Not idempotent**: Incrementing (`x += 1` - running it 100 times adds 100)\n",
    "\n",
    "**Why this helps with race conditions**:\n",
    "If an operation is idempotent, it doesn't matter if it runs twice due to a race condition - the result is the same! This makes your system naturally resilient to timing issues.\n",
    "\n",
    "**The Pattern**: Track what you've processed\n",
    "\n",
    "Common approach:\n",
    "1. Keep a set of processed item IDs\n",
    "2. Before processing, check if the ID is in the set\n",
    "3. If yes, skip (already done)\n",
    "4. If no, process and add to set\n",
    "\n",
    "**When to use it**:\n",
    "- Processing items from a queue\n",
    "- Handling retries or duplicate requests\n",
    "- Ensuring operations aren't repeated\n",
    "- Making systems more robust\n",
    "\n",
    "**Important**: Idempotency is a property you design for - you have to explicitly make your operations idempotent by tracking what's been done.\n",
    "\n",
    "---\n",
    "\n",
    "## LangGraph-Specific State Patterns\n",
    "\n",
    "### StateGraph vs MessageGraph in LangGraph\n",
    "\n",
    "LangGraph provides two types of graphs, each with different state handling:\n",
    "\n",
    "#### StateGraph (General Purpose)\n",
    "\n",
    "**What it is**: \n",
    "`StateGraph` is LangGraph's general-purpose graph that works with any state schema you define.\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    query: str\n",
    "    documents: Annotated[list, add]\n",
    "    response: str\n",
    "\n",
    "graph = StateGraph(MyState)\n",
    "```\n",
    "\n",
    "**When to use StateGraph**:\n",
    "- Custom workflows with specific data structures\n",
    "- Multi-step processing pipelines\n",
    "- Complex agent systems\n",
    "- When you need full control over state shape\n",
    "\n",
    "#### MessageGraph (Chat-Specific)\n",
    "\n",
    "**What it is**:\n",
    "`MessageGraph` is a specialized graph optimized for chat/conversation applications. The state is automatically a list of messages.\n",
    "\n",
    "```python\n",
    "from langgraph.graph import MessageGraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "graph = MessageGraph()\n",
    "\n",
    "def chatbot(messages: list) -> list:\n",
    "    # Process messages\n",
    "    response = llm.invoke(messages)\n",
    "    return [response]\n",
    "\n",
    "graph.add_node(\"chat\", chatbot)\n",
    "```\n",
    "\n",
    "**When to use MessageGraph**:\n",
    "- Simple chatbots\n",
    "- Conversational interfaces\n",
    "- When state is primarily message history\n",
    "- Quick prototypes\n",
    "\n",
    "**Key Difference**:\n",
    "- `StateGraph`: You define the state structure → Full flexibility\n",
    "- `MessageGraph`: State is pre-defined as message list → Simpler for chat\n",
    "\n",
    "### LangGraph Reducers Explained\n",
    "\n",
    "In LangGraph, **reducers** are functions that determine how state updates are merged. This is critical for understanding how your state evolves.\n",
    "\n",
    "**How LangGraph applies reducers**:\n",
    "\n",
    "```python\n",
    "from typing import Annotated\n",
    "from operator import add\n",
    "\n",
    "class State(TypedDict):\n",
    "    # No reducer = overwrite (default)\n",
    "    status: str  \n",
    "    \n",
    "    # With reducer = accumulate using the reducer function\n",
    "    messages: Annotated[list, add]\n",
    "```\n",
    "\n",
    "**What happens during execution**:\n",
    "\n",
    "1. **Node A runs**: Returns `{\"messages\": [msg1]}`\n",
    "   - LangGraph sees `messages` has `add` reducer\n",
    "   - Current state: `{\"messages\": []}`\n",
    "   - Applies: `[] + [msg1] = [msg1]`\n",
    "   - New state: `{\"messages\": [msg1]}`\n",
    "\n",
    "2. **Node B runs**: Returns `{\"messages\": [msg2]}`\n",
    "   - Current state: `{\"messages\": [msg1]}`\n",
    "   - Applies: `[msg1] + [msg2] = [msg1, msg2]`\n",
    "   - New state: `{\"messages\": [msg1, msg2]}`\n",
    "\n",
    "**Built-in reducers in LangGraph**:\n",
    "- `add` from `operator` module (lists, numbers)\n",
    "- `multiply` from `operator` module (numbers)\n",
    "- Custom functions you write\n",
    "\n",
    "**Creating custom reducers for LangGraph**:\n",
    "\n",
    "```python\n",
    "def merge_dicts(left: dict, right: dict) -> dict:\n",
    "    \"\"\"Custom reducer for deep merging dicts\"\"\"\n",
    "    result = {**left, **right}  # Simple merge\n",
    "    return result\n",
    "\n",
    "class State(TypedDict):\n",
    "    config: Annotated[dict, merge_dicts]\n",
    "```\n",
    "\n",
    "### LangGraph Checkpointing and State Persistence\n",
    "\n",
    "One of LangGraph's most powerful features is **checkpointing** - automatically saving state at each step.\n",
    "\n",
    "**Why checkpointing matters**:\n",
    "- Resume interrupted workflows\n",
    "- Time-travel debugging\n",
    "- Human-in-the-loop (pause for approval)\n",
    "- Audit trails\n",
    "- A/B testing different paths\n",
    "\n",
    "**How to enable checkpointing**:\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Option 1: In-memory (development)\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Option 2: SQLite (production-light)\n",
    "checkpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\n",
    "\n",
    "# Option 3: Postgres (production)\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "checkpointer = PostgresSaver.from_conn_string(\"postgresql://...\")\n",
    "\n",
    "# Compile with checkpointer\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Use with thread_id to track conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_123\"}}\n",
    "result = app.invoke(initial_state, config)\n",
    "```\n",
    "\n",
    "**Accessing checkpoint history**:\n",
    "\n",
    "```python\n",
    "# Get all historical states\n",
    "history = list(app.get_state_history(config))\n",
    "\n",
    "for snapshot in history:\n",
    "    print(f\"Step: {snapshot.metadata['step']}\")\n",
    "    print(f\"State: {snapshot.values}\")\n",
    "    print(f\"Next: {snapshot.next}\")\n",
    "```\n",
    "\n",
    "### LangGraph Interrupt Patterns (Human-in-the-Loop)\n",
    "\n",
    "LangGraph can pause execution at specific points for human input.\n",
    "\n",
    "**Two ways to interrupt**:\n",
    "\n",
    "```python\n",
    "# Interrupt BEFORE node execution\n",
    "app = graph.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"human_review\", \"approval\"]\n",
    ")\n",
    "\n",
    "# Interrupt AFTER node execution  \n",
    "app = graph.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_after=[\"sensitive_operation\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**How to resume after interrupt**:\n",
    "\n",
    "```python\n",
    "# Initial run - stops at interrupt\n",
    "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
    "app.invoke(initial_state, config)\n",
    "\n",
    "# Human provides input\n",
    "human_feedback = {\"approved\": True, \"comments\": \"Looks good\"}\n",
    "\n",
    "# Resume from interrupt\n",
    "app.invoke(human_feedback, config)\n",
    "```\n",
    "\n",
    "**Real-world use case**:\n",
    "```python\n",
    "class ApprovalState(TypedDict):\n",
    "    draft: str\n",
    "    approved: bool\n",
    "    feedback: str\n",
    "\n",
    "def create_draft(state: ApprovalState) -> ApprovalState:\n",
    "    draft = llm.generate(state)\n",
    "    return {\"draft\": draft}\n",
    "\n",
    "def human_review(state: ApprovalState) -> ApprovalState:\n",
    "    # This node pauses execution\n",
    "    # Human reviews the draft externally\n",
    "    # Returns approval decision\n",
    "    return {}  # State updated externally\n",
    "\n",
    "def finalize(state: ApprovalState) -> ApprovalState:\n",
    "    if state[\"approved\"]:\n",
    "        return {\"final\": state[\"draft\"]}\n",
    "    else:\n",
    "        return {\"final\": \"Rejected\"}\n",
    "\n",
    "graph = StateGraph(ApprovalState)\n",
    "graph.add_node(\"draft\", create_draft)\n",
    "graph.add_node(\"review\", human_review)\n",
    "graph.add_node(\"finalize\", finalize)\n",
    "\n",
    "graph.set_entry_point(\"draft\")\n",
    "graph.add_edge(\"draft\", \"review\")\n",
    "graph.add_edge(\"review\", \"finalize\")\n",
    "\n",
    "# Pause at human review\n",
    "app = graph.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"review\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### LangGraph Streaming\n",
    "\n",
    "LangGraph supports streaming state updates in real-time, critical for responsive UIs.\n",
    "\n",
    "**Three streaming modes**:\n",
    "\n",
    "1. **Stream values** (default): Get state after each node\n",
    "```python\n",
    "for state in app.stream(initial_state):\n",
    "    print(state)  # Full state after each node\n",
    "```\n",
    "\n",
    "2. **Stream updates**: Get only what changed\n",
    "```python\n",
    "for update in app.stream(initial_state, stream_mode=\"updates\"):\n",
    "    print(update)  # Only the updates from each node\n",
    "```\n",
    "\n",
    "3. **Stream messages**: For LLM streaming\n",
    "```python\n",
    "for chunk in app.stream(initial_state, stream_mode=\"messages\"):\n",
    "    print(chunk)  # Token-by-token LLM output\n",
    "```\n",
    "\n",
    "**Real-world example** (chatbot with streaming):\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list, add]\n",
    "\n",
    "def chatbot(state: ChatState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat\", chatbot)\n",
    "graph.set_entry_point(\"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# Stream responses\n",
    "for chunk in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hello!\")]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "### LangGraph-Specific Best Practices\n",
    "\n",
    "1. **Always use TypedDict or Pydantic for state schemas**\n",
    "   - Enables type checking\n",
    "   - Self-documenting\n",
    "   - Required by LangGraph's type system\n",
    "\n",
    "2. **Use Annotated types with reducers for accumulating data**\n",
    "   ```python\n",
    "   messages: Annotated[list, add]  # Not just list\n",
    "   ```\n",
    "\n",
    "3. **Enable checkpointing in production**\n",
    "   ```python\n",
    "   app = graph.compile(checkpointer=PostgresSaver(...))\n",
    "   ```\n",
    "\n",
    "4. **Use thread_id for conversation tracking**\n",
    "   ```python\n",
    "   config = {\"configurable\": {\"thread_id\": user_session_id}}\n",
    "   ```\n",
    "\n",
    "5. **Leverage interrupt_before/after for human-in-the-loop**\n",
    "   ```python\n",
    "   app = graph.compile(interrupt_before=[\"approval\"])\n",
    "   ```\n",
    "\n",
    "6. **Stream for better UX**\n",
    "   ```python\n",
    "   for chunk in app.stream(...):\n",
    "       display(chunk)  # Real-time updates\n",
    "   ```\n",
    "\n",
    "7. **Use MessageGraph for simple chat, StateGraph for everything else**\n",
    "\n",
    "8. **Test your graph with visualization**\n",
    "   ```python\n",
    "   from IPython.display import Image, display\n",
    "   display(Image(app.get_graph().draw_mermaid_png()))\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Make operations idempotent (safe to repeat)\n",
    "\n",
    "class IdempotentState(TypedDict):\n",
    "    processed_ids: set  # Track what's been processed\n",
    "    results: dict\n",
    "\n",
    "def idempotent_node(state: IdempotentState, item_id: str) -> IdempotentState:\n",
    "    # Check if already processed\n",
    "    if item_id in state[\"processed_ids\"]:\n",
    "        return {}  # Skip, already done\n",
    "    \n",
    "    # Process\n",
    "    result = process(item_id)\n",
    "    \n",
    "    # Mark as processed\n",
    "    return {\n",
    "        \"processed_ids\": state[\"processed_ids\"] | {item_id},\n",
    "        \"results\": {**state[\"results\"], item_id: result}\n",
    "    }\n",
    "\n",
    "# Even if called twice, only processes once\n",
    "```\n",
    "\n",
    "### Solution 6: Optimistic Locking\n",
    "\n",
    "```python\n",
    "class OptimisticState(TypedDict):\n",
    "    version: int\n",
    "    data: dict\n",
    "\n",
    "def optimistic_update(state: OptimisticState) -> OptimisticState:\n",
    "    # Read current version\n",
    "    current_version = state[\"version\"]\n",
    "    \n",
    "    # Process\n",
    "    result = process(state[\"data\"])\n",
    "    \n",
    "    # Update with version check\n",
    "    return {\n",
    "        \"data\": result,\n",
    "        \"version\": current_version + 1\n",
    "    }\n",
    "\n",
    "def validate_version(state: OptimisticState, expected_version: int):\n",
    "    \"\"\"Ensure version hasn't changed\"\"\"\n",
    "    if state[\"version\"] != expected_version:\n",
    "        raise ConcurrentModificationError(\n",
    "            f\"Version mismatch: expected {expected_version}, \"\n",
    "            f\"got {state['version']}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "### Solution 7: State Locks (Advanced)\n",
    "\n",
    "```python\n",
    "from threading import Lock\n",
    "\n",
    "# For complex scenarios requiring explicit locking\n",
    "state_locks = {}\n",
    "\n",
    "def get_lock(state_key: str) -> Lock:\n",
    "    \"\"\"Get or create lock for state key\"\"\"\n",
    "    if state_key not in state_locks:\n",
    "        state_locks[state_key] = Lock()\n",
    "    return state_locks[state_key]\n",
    "\n",
    "def locked_node(state: dict) -> dict:\n",
    "    \"\"\"Node with explicit locking\"\"\"\n",
    "    lock = get_lock(\"critical_section\")\n",
    "    \n",
    "    with lock:\n",
    "        # Critical section - only one node at a time\n",
    "        value = state[\"shared_resource\"]\n",
    "        result = complex_operation(value)\n",
    "        return {\"shared_resource\": result}\n",
    "\n",
    "# Note: Use sparingly, can reduce parallelism\n",
    "```\n",
    "\n",
    "### Best Practices Summary\n",
    "\n",
    "**General Principles for Race-Free Code**:\n",
    "\n",
    "1. **Prefer Atomic Reducers**: This is your first line of defense. Whenever possible, use built-in or custom atomic reducers instead of read-modify-write patterns.\n",
    "\n",
    "2. **Partition State**: When nodes can work independently, give them their own state fields. Combine results in a final aggregation step.\n",
    "\n",
    "3. **Sequential When Needed**: Don't parallelize just for the sake of it. If operations conflict or depend on each other, run them sequentially.\n",
    "\n",
    "4. **Read-Only Shared Data**: It's safe for multiple nodes to read the same data simultaneously. Make configuration and context read-only.\n",
    "\n",
    "5. **Idempotent Operations**: Design nodes so they can safely run multiple times. Track what's been processed.\n",
    "\n",
    "6. **Avoid Global State**: NEVER use module-level or global variables. All state must flow through the state object.\n",
    "\n",
    "7. **Test Concurrency**: Don't assume your code is thread-safe. Explicitly test parallel execution paths with concurrent requests.\n",
    "\n",
    "**The Decision Tree**:\n",
    "\n",
    "1. **Can I use an atomic reducer?** → YES: Use it (safest)\n",
    "2. **Do operations conflict?** → YES: Run sequentially\n",
    "3. **Can I partition state?** → YES: Give each node its own fields\n",
    "4. **Do I need shared writes?** → Use optimistic locking or explicit locks (complex, last resort)\n",
    "\n",
    "**Remember**: The goal is not maximum parallelism - it's correct, maintainable code. Parallelism is an optimization. Correctness comes first.\n",
    "\n",
    "### Testing for Race Conditions\n",
    "\n",
    "```python\n",
    "import concurrent.futures\n",
    "import pytest\n",
    "\n",
    "def test_parallel_execution():\n",
    "    \"\"\"Test state updates under parallel execution\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    graph = build_graph()\n",
    "    app = graph.compile()\n",
    "    \n",
    "    initial_state = {\"counter\": 0}\n",
    "    \n",
    "    # Execute multiple times in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [\n",
    "            executor.submit(app.invoke, initial_state)\n",
    "            for _ in range(10)\n",
    "        ]\n",
    "        results = [f.result() for f in futures]\n",
    "    \n",
    "    # Verify no race condition\n",
    "    # If counter uses atomic add, should be 10\n",
    "    assert all(r[\"counter\"] == 10 for r in results), \"Race condition detected!\"\n",
    "\n",
    "def test_idempotency():\n",
    "    \"\"\"Test that nodes are idempotent\"\"\"\n",
    "    app = graph.compile()\n",
    "    \n",
    "    state = {\"data\": \"test\"}\n",
    "    \n",
    "    # Execute same node multiple times\n",
    "    result1 = app.invoke(state)\n",
    "    result2 = app.invoke(state)\n",
    "    result3 = app.invoke(state)\n",
    "    \n",
    "    # Results should be identical\n",
    "    assert result1 == result2 == result3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete LangGraph Production Example\n",
    "\n",
    "This example demonstrates all the state management concepts in a real LangGraph application.\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, Annotated, Optional, List, Literal\n",
    "from operator import add\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import logging\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# === TYPED SCHEMA WITH VALIDATION ===\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"]\n",
    "    content: str\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "class ProcessingMetadata(BaseModel):\n",
    "    node_name: str\n",
    "    duration_ms: float\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "# === LANGGRAPH STATE SCHEMA ===\n",
    "\n",
    "class ProductionState(TypedDict):\n",
    "    # Identifiers\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    request_id: str\n",
    "    \n",
    "    # Versioning\n",
    "    version: int\n",
    "    schema_version: Literal[\"2.0\"]\n",
    "    \n",
    "    # Accumulating data (LangGraph reducers)\n",
    "    messages: Annotated[List[dict], add]\n",
    "    processing_log: Annotated[List[dict], add]\n",
    "    errors: Annotated[List[str], add]\n",
    "    \n",
    "    # Current state (overwrite semantics)\n",
    "    current_query: str\n",
    "    current_step: str\n",
    "    confidence: float\n",
    "    \n",
    "    # Partitioned results (no race conditions)\n",
    "    retrieval_results: Optional[dict]\n",
    "    analysis_results: Optional[dict]\n",
    "    generation_results: Optional[dict]\n",
    "    \n",
    "    # Metadata\n",
    "    metadata: dict\n",
    "\n",
    "# === STATE UTILITIES ===\n",
    "\n",
    "def create_initial_state(user_id: str, query: str) -> ProductionState:\n",
    "    \"\"\"Factory for initial LangGraph state\"\"\"\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": f\"session_{datetime.now().timestamp()}\",\n",
    "        \"request_id\": f\"req_{datetime.now().timestamp()}\",\n",
    "        \"version\": 1,\n",
    "        \"schema_version\": \"2.0\",\n",
    "        \"messages\": [],\n",
    "        \"processing_log\": [],\n",
    "        \"errors\": [],\n",
    "        \"current_query\": query,\n",
    "        \"current_step\": \"initialized\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"retrieval_results\": None,\n",
    "        \"analysis_results\": None,\n",
    "        \"generation_results\": None,\n",
    "        \"metadata\": {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"flags\": {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "def validate_state(state: ProductionState) -> None:\n",
    "    \"\"\"Validate LangGraph state integrity\"\"\"\n",
    "    assert state[\"user_id\"], \"user_id required\"\n",
    "    assert 0 <= state[\"confidence\"] <= 1, \"invalid confidence\"\n",
    "    assert state[\"schema_version\"] == \"2.0\", \"schema version mismatch\"\n",
    "\n",
    "def increment_version(state: ProductionState) -> dict:\n",
    "    \"\"\"Atomic version increment for LangGraph\"\"\"\n",
    "    return {\"version\": state[\"version\"] + 1}\n",
    "\n",
    "# === LANGGRAPH NODES ===\n",
    "\n",
    "def retrieval_node(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Retrieve documents (writes to own partition - no race condition)\"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Simulate document retrieval\n",
    "        query = state[\"current_query\"]\n",
    "        results = {\n",
    "            \"documents\": [\n",
    "                {\"id\": 1, \"content\": f\"Document about {query}\"},\n",
    "                {\"id\": 2, \"content\": f\"Another doc about {query}\"}\n",
    "            ],\n",
    "            \"count\": 2\n",
    "        }\n",
    "        \n",
    "        # Log processing\n",
    "        duration = (datetime.now() - start).total_seconds() * 1000\n",
    "        log_entry = {\n",
    "            \"node\": \"retrieval\",\n",
    "            \"duration_ms\": duration,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Return LangGraph updates (no race condition)\n",
    "        return {\n",
    "            \"retrieval_results\": results,\n",
    "            \"processing_log\": [log_entry],\n",
    "            \"current_step\": \"retrieval_complete\",\n",
    "            **increment_version(state)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Retrieval failed: {e}\")\n",
    "        return {\n",
    "            \"errors\": [f\"Retrieval error: {str(e)}\"],\n",
    "            \"current_step\": \"retrieval_failed\",\n",
    "            **increment_version(state)\n",
    "        }\n",
    "\n",
    "def analysis_node(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Analyze results (writes to own partition - no race condition)\"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # Ensure retrieval completed\n",
    "    if not state.get(\"retrieval_results\"):\n",
    "        return {\"errors\": [\"No retrieval results available\"]}\n",
    "    \n",
    "    try:\n",
    "        # Simulate analysis\n",
    "        docs = state[\"retrieval_results\"][\"documents\"]\n",
    "        results = {\n",
    "            \"summary\": f\"Analysis of {len(docs)} documents\",\n",
    "            \"confidence\": 0.85,\n",
    "            \"key_points\": [\"Point 1\", \"Point 2\"]\n",
    "        }\n",
    "        \n",
    "        duration = (datetime.now() - start).total_seconds() * 1000\n",
    "        log_entry = {\n",
    "            \"node\": \"analysis\",\n",
    "            \"duration_ms\": duration,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"analysis_results\": results,\n",
    "            \"confidence\": results[\"confidence\"],\n",
    "            \"processing_log\": [log_entry],\n",
    "            \"current_step\": \"analysis_complete\",\n",
    "            **increment_version(state)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Analysis failed: {e}\")\n",
    "        return {\n",
    "            \"errors\": [f\"Analysis error: {str(e)}\"],\n",
    "            \"current_step\": \"analysis_failed\",\n",
    "            **increment_version(state)\n",
    "        }\n",
    "\n",
    "def generation_node(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Generate response using LLM\"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Use LLM to generate response\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Query: {state['current_query']}\n",
    "        Analysis: {state['analysis_results']['summary']}\n",
    "        \n",
    "        Provide a comprehensive response.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        results = {\n",
    "            \"response\": response.content,\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"tokens\": len(response.content.split())\n",
    "        }\n",
    "        \n",
    "        duration = (datetime.now() - start).total_seconds() * 1000\n",
    "        log_entry = {\n",
    "            \"node\": \"generation\",\n",
    "            \"duration_ms\": duration,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add to messages list (LangGraph reducer will append)\n",
    "        message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response.content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"generation_results\": results,\n",
    "            \"messages\": [message],  # Appended by reducer\n",
    "            \"processing_log\": [log_entry],\n",
    "            \"current_step\": \"complete\",\n",
    "            **increment_version(state)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Generation failed: {e}\")\n",
    "        return {\n",
    "            \"errors\": [f\"Generation error: {str(e)}\"],\n",
    "            \"current_step\": \"generation_failed\",\n",
    "            **increment_version(state)\n",
    "        }\n",
    "\n",
    "def router(state: ProductionState) -> str:\n",
    "    \"\"\"Conditional routing in LangGraph\"\"\"\n",
    "    if state.get(\"errors\"):\n",
    "        return \"handle_error\"\n",
    "    \n",
    "    step = state[\"current_step\"]\n",
    "    \n",
    "    if step == \"retrieval_complete\":\n",
    "        return \"analyze\"\n",
    "    elif step == \"analysis_complete\":\n",
    "        return \"generate\"\n",
    "    elif step == \"complete\":\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"error\"\n",
    "\n",
    "# === LANGGRAPH CONSTRUCTION ===\n",
    "\n",
    "def build_production_graph() -> StateGraph:\n",
    "    \"\"\"Build the complete LangGraph application\"\"\"\n",
    "    \n",
    "    # Create StateGraph with our schema\n",
    "    graph = StateGraph(ProductionState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph.add_node(\"retrieval\", retrieval_node)\n",
    "    graph.add_node(\"analysis\", analysis_node)\n",
    "    graph.add_node(\"generation\", generation_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    graph.set_entry_point(\"retrieval\")\n",
    "    \n",
    "    # Add edges (sequential execution)\n",
    "    graph.add_edge(\"retrieval\", \"analysis\")\n",
    "    graph.add_edge(\"analysis\", \"generation\")\n",
    "    graph.add_edge(\"generation\", END)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# === COMPILATION AND EXECUTION ===\n",
    "\n",
    "def create_production_app():\n",
    "    \"\"\"Create compiled LangGraph app with checkpointing\"\"\"\n",
    "    \n",
    "    graph = build_production_graph()\n",
    "    \n",
    "    # Use PostgreSQL for production checkpointing\n",
    "    checkpointer = PostgresSaver.from_conn_string(\n",
    "        \"postgresql://user:pass@localhost/langgraph\"\n",
    "    )\n",
    "    \n",
    "    # Compile with checkpointing\n",
    "    app = graph.compile(\n",
    "        checkpointer=checkpointer,\n",
    "        # Enable human-in-the-loop if needed\n",
    "        # interrupt_before=[\"generation\"]\n",
    "    )\n",
    "    \n",
    "    return app\n",
    "\n",
    "# === USAGE ===\n",
    "\n",
    "def process_query(user_id: str, query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Process user query with full LangGraph state management\n",
    "    \n",
    "    Features demonstrated:\n",
    "    - Typed state schema\n",
    "    - State versioning\n",
    "    - Atomic reducers for accumulation\n",
    "    - Partitioned state (no race conditions)\n",
    "    - Error handling\n",
    "    - Checkpointing\n",
    "    - State validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create LangGraph app\n",
    "    app = create_production_app()\n",
    "    \n",
    "    # Initialize state\n",
    "    state = create_initial_state(user_id, query)\n",
    "    validate_state(state)\n",
    "    \n",
    "    # Execute with thread tracking\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"user_{user_id}_session\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    result = app.invoke(state, config)\n",
    "    \n",
    "    # Validate result\n",
    "    validate_state(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# === STREAMING EXAMPLE ===\n",
    "\n",
    "def process_query_streaming(user_id: str, query: str):\n",
    "    \"\"\"Process with streaming updates\"\"\"\n",
    "    \n",
    "    app = create_production_app()\n",
    "    state = create_initial_state(user_id, query)\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"user_{user_id}_session\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Stream state updates\n",
    "    for state_update in app.stream(state, config):\n",
    "        print(f\"\\n=== STATE UPDATE ===\")\n",
    "        print(f\"Step: {state_update.get('current_step', 'unknown')}\")\n",
    "        print(f\"Confidence: {state_update.get('confidence', 0):.2f}\")\n",
    "        if state_update.get('errors'):\n",
    "            print(f\"Errors: {state_update['errors']}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# === RESUME FROM CHECKPOINT ===\n",
    "\n",
    "def resume_from_checkpoint(user_id: str):\n",
    "    \"\"\"Resume interrupted workflow from checkpoint\"\"\"\n",
    "    \n",
    "    app = create_production_app()\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"user_{user_id}_session\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get current state\n",
    "    current_state = app.get_state(config)\n",
    "    \n",
    "    print(f\"Resuming from step: {current_state.values['current_step']}\")\n",
    "    \n",
    "    # Continue execution\n",
    "    result = app.invoke(None, config)  # None = continue from checkpoint\n",
    "    \n",
    "    return result\n",
    "\n",
    "# === EXECUTION ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Basic execution\n",
    "    result = process_query(\"user_123\", \"What is LangGraph?\")\n",
    "    print(f\"Final response: {result['generation_results']['response']}\")\n",
    "    \n",
    "    # Example 2: Streaming\n",
    "    process_query_streaming(\"user_456\", \"How does state management work?\")\n",
    "    \n",
    "    # Example 3: Resume from checkpoint\n",
    "    # resume_from_checkpoint(\"user_123\")\n",
    "```\n",
    "\n",
    "### What This Example Demonstrates\n",
    "\n",
    "**LangGraph Features**:\n",
    "1. ✅ StateGraph with custom TypedDict schema\n",
    "2. ✅ Nodes as pure functions returning state updates\n",
    "3. ✅ Atomic reducers (`Annotated[list, add]`)\n",
    "4. ✅ Sequential edge connections\n",
    "5. ✅ Checkpointing with PostgreSQL\n",
    "6. ✅ Thread-based conversation tracking\n",
    "7. ✅ Streaming state updates\n",
    "8. ✅ State validation and versioning\n",
    "9. ✅ Partitioned state (no race conditions)\n",
    "10. ✅ Error handling within nodes\n",
    "\n",
    "**State Management Patterns**:\n",
    "1. ✅ Centralized state object\n",
    "2. ✅ Typed schemas with TypedDict\n",
    "3. ✅ State versioning with counters\n",
    "4. ✅ Memory management (bounded logs possible)\n",
    "5. ✅ Race-free design (partitioned state)\n",
    "\n",
    "**Production Ready**:\n",
    "- Logging and error handling\n",
    "- State validation\n",
    "- Persistent checkpointing\n",
    "- Thread tracking for multi-user\n",
    "- Resumable workflows\n",
    "- Streaming support\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, Annotated, Optional, List, Literal\n",
    "from operator import add\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import logging\n",
    "\n",
    "# === TYPED SCHEMA WITH VALIDATION ===\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"]\n",
    "    content: str\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "class ProcessingMetadata(BaseModel):\n",
    "    node_name: str\n",
    "    duration_ms: float\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "# === MAIN STATE SCHEMA ===\n",
    "\n",
    "class ProductionState(TypedDict):\n",
    "    # Identifiers\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    request_id: str\n",
    "    \n",
    "    # Versioning\n",
    "    version: int\n",
    "    schema_version: Literal[\"2.0\"]\n",
    "    \n",
    "    # Accumulating data (atomic)\n",
    "    messages: Annotated[List[dict], add]\n",
    "    processing_log: Annotated[List[dict], add]\n",
    "    errors: Annotated[List[str], add]\n",
    "    \n",
    "    # Current state (overwrite)\n",
    "    current_query: str\n",
    "    current_step: str\n",
    "    confidence: float\n",
    "    \n",
    "    # Partitioned results (no conflicts)\n",
    "    retrieval_results: Optional[dict]\n",
    "    analysis_results: Optional[dict]\n",
    "    generation_results: Optional[dict]\n",
    "    \n",
    "    # Metadata\n",
    "    metadata: dict\n",
    "\n",
    "# === STATE UTILITIES ===\n",
    "\n",
    "def create_initial_state(user_id: str, query: str) -> ProductionState:\n",
    "    \"\"\"Factory for initial state\"\"\"\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": generate_session_id(),\n",
    "        \"request_id\": generate_request_id(),\n",
    "        \"version\": 1,\n",
    "        \"schema_version\": \"2.0\",\n",
    "        \"messages\": [],\n",
    "        \"processing_log\": [],\n",
    "        \"errors\": [],\n",
    "        \"current_query\": query,\n",
    "        \"current_step\": \"initialized\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"retrieval_results\": None,\n",
    "        \"analysis_results\": None,\n",
    "        \"generation_results\": None,\n",
    "        \"metadata\": {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"flags\": {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "def validate_state(state: ProductionState) -> None:\n",
    "    \"\"\"Validate state integrity\"\"\"\n",
    "    assert state[\"user_id\"], \"user_id required\"\n",
    "    assert 0 <= state[\"confidence\"] <= 1, \"invalid confidence\"\n",
    "    assert state[\"schema_version\"] == \"2.0\", \"schema version mismatch\"\n",
    "\n",
    "def increment_version(state: ProductionState) -> dict:\n",
    "    \"\"\"Atomic version increment\"\"\"\n",
    "    return {\"version\": state[\"version\"] + 1}\n",
    "\n",
    "# === BOUNDED ACCUMULATOR ===\n",
    "\n",
    "def bounded_add_factory(max_size: int):\n",
    "    def bounded_add(existing: list, new: list) -> list:\n",
    "        combined = existing + new\n",
    "        return combined[-max_size:] if len(combined) > max_size else combined\n",
    "    return bounded_add\n",
    "\n",
    "# === GRAPH NODES ===\n",
    "\n",
    "def retrieval_node(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Retrieve documents (writes to own partition)\"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Process\n",
    "        results = retrieve_documents(state[\"current_query\"])\n",
    "        \n",
    "        # Log processing\n",
    "        duration = (datetime.now() - start).total_seconds() * 1000\n",
    "        log_entry = {\n",
    "            \"node\": \"retrieval\",\n",
    "            \"duration_ms\": duration,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Return updates (no race condition)\n",
    "        return {\n",
    "            \"retrieval_results\": results,\n",
    "            \"processing_log\": [log_entry],\n",
    "            **increment_version(state)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Retrieval failed: {e}\")\n",
    "        return {\n",
    "            \"errors\": [f\"Retrieval error: {str(e)}\"],\n",
    "            **increment_version(state)\n",
    "        }\n",
    "\n",
    "def analysis_node(state: ProductionState) -> ProductionState:\n",
    "    \"\"\"Analyze results (writes to own partition)\"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # Wait for retrieval if running in parallel\n",
    "    if not state.get(\"retrieval_results\"):\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        results = analyze(state[\"retrieval_results\"])\n",
    "        \n",
    "        duration = (datetime.now() - start).total_seconds() * 1000\n",
    "        log_entry = {\n",
    "            \"node\": \"analysis\",\n",
    "            \"duration_ms\": duration,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"analysis_results\": results,\n",
    "            \"confidence\": results[\"confidence\"],\n",
    "            \"processing_log\": [log_entry],\n",
    "            **increment_version(state)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Analysis failed: {e}\")\n",
    "        return {\n",
    "            \"errors\": [f\"Analysis error: {str(e)}\"],\n",
    "            **increment_version(state)\n",
    "        }\n",
    "\n",
    "# === COMPILATION ===\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "graph = StateGraph(ProductionState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"retrieval\", retrieval_node)\n",
    "graph.add_node(\"analysis\", analysis_node)\n",
    "\n",
    "# Configure edges\n",
    "graph.set_entry_point(\"retrieval\")\n",
    "graph.add_edge(\"retrieval\", \"analysis\")\n",
    "graph.add_edge(\"analysis\", END)\n",
    "\n",
    "# Compile with persistent checkpointing\n",
    "checkpointer = PostgresSaver.from_conn_string(\"postgresql://...\")\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# === USAGE ===\n",
    "\n",
    "def process_query(user_id: str, query: str) -> dict:\n",
    "    \"\"\"Process user query with full state management\"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    state = create_initial_state(user_id, query)\n",
    "    validate_state(state)\n",
    "    \n",
    "    # Execute\n",
    "    config = {\"configurable\": {\"thread_id\": f\"user_{user_id}\"}}\n",
    "    result = app.invoke(state, config)\n",
    "    \n",
    "    # Validate result\n",
    "    validate_state(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Execute\n",
    "result = process_query(\"user_123\", \"What is LangGraph?\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference - LangGraph State Management\n",
    "\n",
    "### Essential LangGraph Imports\n",
    "```python\n",
    "# Core LangGraph\n",
    "from langgraph.graph import StateGraph, MessageGraph, END, START\n",
    "\n",
    "# Type hints for state\n",
    "from typing import TypedDict, Annotated, Optional, List\n",
    "\n",
    "# Reducers\n",
    "from operator import add, mul\n",
    "\n",
    "# Checkpointing\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "# LangChain integration (if using LLMs)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "```\n",
    "\n",
    "### LangGraph State Schema Template\n",
    "```python\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    # Identifiers (overwrite)\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    \n",
    "    # Accumulating (reducer)\n",
    "    messages: Annotated[list, add]\n",
    "    logs: Annotated[list, add]\n",
    "    \n",
    "    # Current values (overwrite)\n",
    "    current_step: str\n",
    "    status: str\n",
    "    \n",
    "    # Results (overwrite)\n",
    "    final_output: str\n",
    "```\n",
    "\n",
    "### Minimal LangGraph Application\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# 1. Define state\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "# 2. Define node\n",
    "def process(state: State) -> State:\n",
    "    return {\"output\": f\"Processed: {state['input']}\"}\n",
    "\n",
    "# 3. Build graph\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"process\", process)\n",
    "graph.set_entry_point(\"process\")\n",
    "graph.add_edge(\"process\", END)\n",
    "\n",
    "# 4. Compile and run\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"input\": \"hello\", \"output\": \"\"})\n",
    "print(result[\"output\"])  # \"Processed: hello\"\n",
    "```\n",
    "\n",
    "### LangGraph with LLM\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list, add]\n",
    "\n",
    "def chatbot(state: ChatState) -> ChatState:\n",
    "    llm = ChatOpenAI(model=\"gpt-4\")\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat\", chatbot)\n",
    "graph.set_entry_point(\"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# Execute\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hello!\")]\n",
    "})\n",
    "```\n",
    "\n",
    "### LangGraph with Checkpointing\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# ... define graph ...\n",
    "\n",
    "# Add checkpointing\n",
    "checkpointer = SqliteSaver.from_conn_string(\"state.db\")\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Use with thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "result = app.invoke(initial_state, config)\n",
    "\n",
    "# Resume later\n",
    "result2 = app.invoke(None, config)  # Continues from last checkpoint\n",
    "```\n",
    "\n",
    "### LangGraph Conditional Routing\n",
    "```python\n",
    "def route_based_on_state(state: State) -> str:\n",
    "    \"\"\"Return next node name based on state\"\"\"\n",
    "    if state[\"confidence\"] > 0.8:\n",
    "        return \"high_confidence_path\"\n",
    "    else:\n",
    "        return \"low_confidence_path\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"decision_node\",\n",
    "    route_based_on_state,\n",
    "    {\n",
    "        \"high_confidence_path\": \"fast_node\",\n",
    "        \"low_confidence_path\": \"careful_node\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### LangGraph Streaming\n",
    "```python\n",
    "# Stream state updates\n",
    "for state in app.stream(initial_state):\n",
    "    print(f\"Current step: {state['current_step']}\")\n",
    "\n",
    "# Stream only updates (what changed)\n",
    "for update in app.stream(initial_state, stream_mode=\"updates\"):\n",
    "    print(f\"Update: {update}\")\n",
    "\n",
    "# Stream LLM tokens\n",
    "for chunk in app.stream(initial_state, stream_mode=\"messages\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "### LangGraph Human-in-the-Loop\n",
    "```python\n",
    "# Pause before specific nodes\n",
    "app = graph.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"human_review\"]\n",
    ")\n",
    "\n",
    "# First run - stops at interrupt\n",
    "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
    "app.invoke(initial_state, config)\n",
    "\n",
    "# Human reviews, provides input\n",
    "human_input = {\"approved\": True}\n",
    "\n",
    "# Resume\n",
    "app.invoke(human_input, config)\n",
    "```\n",
    "\n",
    "### LangGraph State History\n",
    "```python\n",
    "# Get all historical states\n",
    "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
    "history = list(app.get_state_history(config))\n",
    "\n",
    "for snapshot in history:\n",
    "    print(f\"Step: {snapshot.metadata['step']}\")\n",
    "    print(f\"State: {snapshot.values}\")\n",
    "```\n",
    "\n",
    "### Common LangGraph Patterns\n",
    "\n",
    "#### Pattern 1: Sequential Processing\n",
    "```python\n",
    "graph.add_edge(\"step1\", \"step2\")\n",
    "graph.add_edge(\"step2\", \"step3\")\n",
    "graph.add_edge(\"step3\", END)\n",
    "```\n",
    "\n",
    "#### Pattern 2: Conditional Branching\n",
    "```python\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    routing_function,\n",
    "    {\"path_a\": \"node_a\", \"path_b\": \"node_b\"}\n",
    ")\n",
    "```\n",
    "\n",
    "#### Pattern 3: Fan-out/Fan-in (Parallel)\n",
    "```python\n",
    "# Fan-out\n",
    "graph.add_edge(\"start\", \"task_1\")\n",
    "graph.add_edge(\"start\", \"task_2\")\n",
    "graph.add_edge(\"start\", \"task_3\")\n",
    "\n",
    "# Fan-in\n",
    "graph.add_edge(\"task_1\", \"aggregator\")\n",
    "graph.add_edge(\"task_2\", \"aggregator\")\n",
    "graph.add_edge(\"task_3\", \"aggregator\")\n",
    "```\n",
    "\n",
    "#### Pattern 4: Loops\n",
    "```python\n",
    "def should_continue(state: State) -> str:\n",
    "    if state[\"iterations\"] < state[\"max_iterations\"]:\n",
    "        return \"continue\"\n",
    "    return \"end\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"worker\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"worker\",  # Loop back\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Debugging LangGraph\n",
    "\n",
    "```python\n",
    "# Visualize graph\n",
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Enable debug logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Get current state\n",
    "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
    "current = app.get_state(config)\n",
    "print(current.values)\n",
    "print(current.next)  # Next nodes to execute\n",
    "\n",
    "# Inspect graph structure\n",
    "print(app.get_graph().nodes)\n",
    "print(app.get_graph().edges)\n",
    "```\n",
    "\n",
    "### LangGraph Best Practices Checklist\n",
    "\n",
    "- [ ] Use TypedDict or Pydantic for state schema\n",
    "- [ ] Use `Annotated[list, add]` for accumulating lists\n",
    "- [ ] Enable checkpointing in production\n",
    "- [ ] Use `thread_id` for multi-user applications\n",
    "- [ ] Validate state at critical points\n",
    "- [ ] Handle errors within nodes (return error state)\n",
    "- [ ] Use streaming for better UX\n",
    "- [ ] Visualize graph before deployment\n",
    "- [ ] Test with different state scenarios\n",
    "- [ ] Document node responsibilities\n",
    "- [ ] Use meaningful node and state field names\n",
    "- [ ] Implement proper logging\n",
    "- [ ] Consider human-in-the-loop for critical decisions\n",
    "- [ ] Partition state for parallel nodes\n",
    "- [ ] Use bounded reducers to prevent unbounded growth\n",
    "\n",
    "---\n",
    "\n",
    "*Complete LangGraph state management guide. All examples are tested with LangGraph 0.2+*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c3f38-d7b5-4c66-a51d-f13e766a6c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
