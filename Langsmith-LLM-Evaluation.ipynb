{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac51abab-28e5-452c-99b7-b05f5d71820f",
   "metadata": {},
   "source": [
    "What is LangSmith?\n",
    "LangSmith is a developer platform created by LangChain for building, debugging, testing, and monitoring LLM (Large Language Model) applications. It's essentially an observability and evaluation tool designed specifically for applications built with language models.\n",
    "Why LangSmith is Used\n",
    "1. Debugging and Tracing\n",
    "LangSmith provides detailed traces of your LLM application's execution, allowing you to see exactly what's happening at each step. This is crucial because LLM applications often involve multiple calls, complex chains, and unpredictable outputs. You can trace the full execution path, including prompts sent, responses received, and any intermediate steps.\n",
    "2. Performance Monitoring\n",
    "It helps track key metrics like latency, token usage, and cost across your application. This visibility is essential for optimizing performance and managing expenses, especially when dealing with API-based LLM services that charge per token.\n",
    "3. Prompt Engineering and Iteration\n",
    "LangSmith allows you to experiment with different prompts and compare results side-by-side. You can see which prompts produce better outputs and iterate quickly without rebuilding your entire application.\n",
    "4. Testing and Evaluation\n",
    "The platform enables you to create test datasets and run evaluations against them. You can set up automated tests to ensure your LLM application maintains quality as you make changes, helping prevent regressions.\n",
    "5. Collaboration\n",
    "Teams can share traces, datasets, and evaluation results, making it easier to collaborate on LLM projects. This is particularly valuable since LLM behavior can be difficult to reproduce or explain without concrete examples.\n",
    "6. Production Monitoring\n",
    "Once deployed, LangSmith continues to monitor your application in production, helping you identify issues, track user interactions, and understand real-world performance.\n",
    "\n",
    "Key Features\n",
    "\n",
    "+ Visual trace inspection - See the entire flow of your application\n",
    "+ Dataset management - Create and maintain test datasets\n",
    "+ Automated evaluations - Run systematic tests on your prompts and chains\n",
    "+ Analytics dashboard - Track usage patterns and costs\n",
    "+ Feedback collection - Gather and analyze user feedback on outputs\n",
    "\n",
    "Common Use Cases\n",
    "\n",
    "+ Debugging complex LangChain applications with multiple agents or tools\n",
    "+ A/B testing different prompts or model configurations\n",
    "+ Monitoring production LLM applications for quality and cost\n",
    "+ Building regression test suites for LLM applications\n",
    "+ Understanding why a particular LLM output was generated\n",
    "\n",
    "LangSmith addresses one of the biggest challenges in LLM development: the difficulty of understanding, testing, and maintaining applications that rely on probabilistic, non-deterministic AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aac36d6-acc2-4e48-a464-6bbb9da3c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b3603b-bd32-4188-9522-511ccdde6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# 1) Load and index some docs (demo corpus)\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "docs = [d for url in urls for d in WebBaseLoader(url).load()]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c0174b-e758-4a14-a23a-58e3d732d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_bot(question: str) -> dict:\n",
    "    \"\"\"Return BOTH answer + retrieved docs for evaluation.\"\"\"\n",
    "    documents = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in documents)\n",
    "    prompt = f\"\"\"Use the context to answer the question.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "    QUESTION:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer, \"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d50087-d04b-407f-a614-4fd9ef5ec616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"my-rag-eval-dataset\"\n",
    "\n",
    "# Create dataset if not exists\n",
    "existing = [d for d in client.list_datasets() if d.name == dataset_name]\n",
    "if not existing:\n",
    "    dataset = client.create_dataset(dataset_name, description=\"RAG eval dataset (Q/A)\")\n",
    "else:\n",
    "    dataset = existing[0]\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is an LLM agent (high level)?\" },\n",
    "        \"outputs\": {\"answer\": \"An LLM agent is a system that uses an LLM to decide actions (e.g., tool use) to accomplish tasks, often iterating based on observations.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is prompt injection and why is it risky?\" },\n",
    "        \"outputs\": {\"answer\": \"Prompt injection is an attack where instructions are inserted into inputs/context to override system behavior, potentially causing data leakage or unsafe actions.\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "# Add examples\n",
    "for ex in examples:\n",
    "    client.create_example(\n",
    "        inputs=ex[\"inputs\"],\n",
    "        outputs=ex[\"outputs\"],\n",
    "        dataset_id=dataset.id,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "615a3421-b57e-4e5d-bf5f-bb740884d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "judge_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "class BoolGrade(BaseModel):\n",
    "    ok: bool = Field(description=\"true if criterion is satisfied, else false\")\n",
    "\n",
    "def _judge(system_prompt: str, user_prompt: str) -> bool:\n",
    "    grader = judge_llm.with_structured_output(BoolGrade, method=\"json_schema\", strict=True)\n",
    "    result = grader.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ])\n",
    "    return result.ok\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return _judge(\n",
    "        \"You are grading correctness vs a reference answer. Reply ok=true only if meaning matches.\",\n",
    "        f\"QUESTION: {inputs['question']}\\nMODEL: {outputs['answer']}\\nREFERENCE: {reference_outputs['answer']}\"\n",
    "    )\n",
    "\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    return _judge(\n",
    "        \"You are grading whether the answer is relevant to the question. Reply ok=true if it addresses the question.\",\n",
    "        f\"QUESTION: {inputs['question']}\\nANSWER: {outputs['answer']}\"\n",
    "    )\n",
    "\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    docs = \"\\n\\n\".join(d.page_content for d in outputs[\"documents\"])\n",
    "    return _judge(\n",
    "        \"You are grading groundedness. Reply ok=true only if the answer is supported by the provided facts, with no major unsupported claims.\",\n",
    "        f\"FACTS:\\n{docs}\\n\\nANSWER:\\n{outputs['answer']}\"\n",
    "    )\n",
    "\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    docs = \"\\n\\n\".join(d.page_content for d in outputs[\"documents\"])\n",
    "    return _judge(\n",
    "        \"You are grading if retrieved docs are relevant to the question. Reply ok=true if docs contain info that would help answer.\",\n",
    "        f\"QUESTION: {inputs['question']}\\nDOCS:\\n{docs}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce38c419-19d6-4d5d-8a62-fae3bf835bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-eval-dbabfddc' at:\n",
      "https://smith.langchain.com/o/819e6a1f-0e45-4de8-b5a2-4bb1d2ff5c50/datasets/e2f2c479-4edc-4c5a-9e75-b0d9fec4f514/compare?selectedSessions=faf2cc73-321b-43ab-9125-22656385c5d2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c690c75913a543e4a1353ba9a31bad69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-eval\",\n",
    "    metadata={\"version\": \"demo-rag + gpt-4o-mini\"},\n",
    ")\n",
    "\n",
    "# If you have pandas installed:\n",
    "#df = experiment_results.to_pandas()\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a343bd-5635-4322-a54f-e8262f08fe09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
