{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ede326-ba45-4fb4-a563-851d0a0d2ff5",
   "metadata": {},
   "source": [
    "# What is a Chain? (Very important foundation)\n",
    "Chain = Connecting steps together\n",
    "\n",
    "Think of a chain like a factory assembly line:\n",
    "Input >> Step 1 >> Step 2 >> Step 3 >> Output\n",
    "\n",
    "In LangChain:\n",
    "Each step can be:\n",
    "+ Prompt\n",
    "+ LLM call\n",
    "+ Tool\n",
    "+ Retrieval\n",
    "\n",
    "Chain helps you control the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ee786-70fa-4a8c-9ef0-9b675628d564",
   "metadata": {},
   "source": [
    "### LLMChain (Basic & Most Important)\n",
    "What it is:\n",
    "Single prompt + Single LLM call.\n",
    "\n",
    "The building block of all other chains\n",
    "\n",
    "Real-life analogy:\n",
    "Asking one question to a teacher and getting one answer.\n",
    "\n",
    "When to use:\n",
    "+ Simple Q&A\n",
    "+ Summarization\n",
    "+ Translation\n",
    "+ Text generation\n",
    "\n",
    "Flow:\n",
    "User Input >> Prompt Template >> LLM >> Output\n",
    "\n",
    "Example:\n",
    "prompt = \"Explain {topic} in simple words\"\n",
    "\n",
    "LLMChain(prompt + LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd7707e-d485-4f55-8d67-1d9aa47fae22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d655522-2f12-4704-ab05-33f15d751d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cae00c0-bd9d-4c24-b9b1-2c32ce818843",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple words.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef2f3ef-13b0-4217-8adc-a9b589c20ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "# Prompt + LLM = Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83125b5-6436-4e5f-a5c8-d3c992099d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI. It makes it easier to create programs that can understand and generate human-like text. \n",
      "\n",
      "Think of LangChain as a toolkit that provides various tools and components to connect language models with other data sources, manage conversations, and handle tasks like searching for information or generating responses. This allows developers to create more complex and interactive applications that can chat, answer questions, or perform tasks based on natural language input.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cea07e-f1d3-4985-b9d8-09861c48707e",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain (Linear & Easy)\n",
    "\n",
    "Here we pass Output of Step 1 as Input of Step 2\n",
    "  \n",
    "Real-life analogy:\n",
    "Washing clothes >> Drying >> Folding\n",
    "\n",
    "Each step depends on previous step.\n",
    "\n",
    "When to use:\n",
    "Straight forward workflows\n",
    "\n",
    "Text >> refined text\n",
    "\n",
    "Flow:\n",
    "Input >> Chain A >> Chain B >> Final Output\n",
    "\n",
    "Example:\n",
    "Input text >> summarize >> translate\n",
    "(Long Articles)  >>> input(Long article - 10 page) - Summarize - Output(Summarized Article - 1 page) >>>> input(Summarized Article - 1 page) - translate - Output(Translated Article - 1 page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fb86490-f604-4323-8608-bedcc5765348",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8cfae3f-bb90-4556-931e-cd6594bacb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Summarize\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the following text in one sentence:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Step 2: Translate\n",
    "translate_prompt = PromptTemplate.from_template(\n",
    "    \"Translate the following text into simple Kannada:\\n{text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21e38a97-28d8-41c5-a203-a2c15873db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Sequential Chain (Runnable)\n",
    "chain = summary_prompt | llm | translate_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69d0911c-9437-4418-a7f0-e361557faab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain ಒಂದು ಫ್ರೇಮ್‌ವರ್ಕ್ ಆಗಿದ್ದು, ಅಭಿವೃದ್ಧಿಪಡಕರಿಗೆ ಭಾಷಾ ಮಾದರಿಗಳನ್ನು ಬಳಸಿಕೊಂಡು ಅಪ್ಲಿಕೇಶನ್‌ಗಳನ್ನು ನಿರ್ಮಿಸಲು ಸಹಾಯ ಮಾಡುತ್ತದೆ. ಇದು ಡೇಟಾ ಮೂಲಗಳಿಗೆ ಸಂಪರ್ಕಿಸಲು, ಸಂಭಾಷಣೆಗಳನ್ನು ನಿರ್ವಹಿಸಲು ಮತ್ತು ನೈಸರ್ಗಿಕ ಭಾಷಾ ಇನ್ಪುಟ್ ಆಧಾರಿತ ಕಾರ್ಯಗಳನ್ನು ನಿರ್ವಹಿಸಲು ಉಪಕರಣಗಳನ್ನು ಒದಗಿಸುತ್ತದೆ.\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "input_text = \"\"\"\n",
    "LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI. \n",
    "It makes it easier to create programs that can understand and generate human-like text. \n",
    "\n",
    "Think of LangChain as a toolkit that provides various tools and components to connect language models with other data sources, \n",
    "manage conversations, and handle tasks like searching for information or generating responses. This allows developers to create \n",
    "more complex and interactive applications that can chat, answer questions, or perform tasks based on natural language input.\n",
    "\"\"\"\n",
    "\n",
    "# Run\n",
    "response = chain.invoke({\"text\": input_text})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f40eb1-62af-48b3-8606-0c354896dee3",
   "metadata": {},
   "source": [
    "### SequentialChain (with named intermediate outputs) \n",
    "\n",
    "In LangChain, SequentialChain is used for running components one after the other, where the output of one step is the input for the next, while RunnablePassthrough.assign is a method for adding new data fields to an input dictionary without altering the original input. \n",
    "\n",
    "Purpose: The SequentialChain class (or RunnableSequence in LangChain Expression Language, LCEL) allows you to combine multiple individual chains or runnables into a single, ordered workflow.\n",
    "\n",
    "Functionality: The output generated by the first chain serves as the direct input to the second chain, and so on, until all chains in the sequence have executed.\n",
    "\n",
    "Use Case: This is useful for multi-step tasks such as taking a topic from a user, generating a title from that topic with one chain, and then using that title to write a full story with a second chain.\n",
    "\n",
    "RunnablePassthrough.assign\n",
    "\n",
    "Purpose: RunnablePassthrough is a runnable that effectively returns its input as its output. The .assign() method extends this by allowing you to add new keys (and their corresponding values) to the input dictionary while still passing the original input data through the chain.\n",
    "\n",
    "Functionality: It's a way to enrich the context of a pipeline. The values for the new keys can be static values or generated dynamically by other runnables or functions.\n",
    "\n",
    "Use Case: A common use case is when you want to retrieve a context (e.g., from a vector store) and pass both the original user question and the retrieved context to an LLM chain.\n",
    "\n",
    "RunnablePassthrough (CORE CONCEPT)\n",
    "What is RunnablePassthrough?\n",
    "\n",
    "RunnablePassthrough passes the input forward unchanged, while allowing you to add or enrich new data.\n",
    "\n",
    "Simple definition:\n",
    "“It keeps the original input safe while we add more information step by step.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f49ab41-2400-4dbf-9221-68db734a2562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2055e407-96eb-41fe-abc5-754159548c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd7489b9-6c65-41a1-8a02-d731f08a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the following text in 3 bullet points:\\n\\n{text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24a9b0d-723d-4378-8a72-c026d86eaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract 5 important keywords from the text:\\n\\n{text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb15412-9de9-4e66-9c8e-278ddd169701",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Use the following information to give a clear explanation.\n",
    "\n",
    "Original Text:\n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Keywords:\n",
    "{keywords}\n",
    "\n",
    "Final Explanation:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a142177-c7a8-4644-b828-1256cd2a2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize = summary_prompt | llm\n",
    "keywords = keywords_prompt | llm\n",
    "final = final_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf3147e-12a5-4ae3-af00-07829fba65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_summary(x):\n",
    "    response = summarize.invoke({\"text\": x[\"text\"]})\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def add_keywords(x):\n",
    "    response = keywords.invoke({\"text\": x[\"text\"]})\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9002023-0e77-4f98-9c13-0c129f06c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    .assign(summary=add_summary)\n",
    "    .assign(keywords=add_keywords)\n",
    "    | final\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f2d2e8-ed15-47ec-b801-ea14c7ac4433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a specialized framework that empowers developers to create applications that leverage large language models. This framework facilitates the integration of various components—such as prompts, models, tools, and retrievers—allowing them to be connected in a sequential manner, or \"chained.\" As a result, LangChain is particularly popular for building applications like chatbots, retrieval-augmented generation (RAG) systems, and AI agents, making it a versatile tool in the realm of artificial intelligence development.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "LangChain is a framework that helps developers build applications using large language models.\n",
    "It allows chaining prompts, models, tools, and retrievers together.\n",
    "LangChain is widely used for chatbots, RAG systems, and AI agents.\n",
    "\"\"\"\n",
    "\n",
    "result = chain.invoke({\"text\": input_text})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed05c991-c903-4c1d-992b-46c57c40e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Understanding LangChain for Beginners**\n",
      "\n",
      "LangChain is a powerful framework designed to help developers create applications that utilize Large Language Models (LLMs). At its core, LangChain simplifies the process of building these applications by integrating several key components: prompts, tools, and retrieval processes.\n",
      "\n",
      "1. **LangChain**: This is the framework that brings everything together, making it easier to work with LLMs.\n",
      "  \n",
      "2. **LLM (Large Language Model)**: These are advanced AI models capable of understanding and generating human-like text. They can be used for various tasks, such as answering questions, writing content, or even having conversations.\n",
      "\n",
      "3. **Apps**: With LangChain, developers can create applications that leverage the capabilities of LLMs. These apps can serve different purposes, from chatbots to content generators.\n",
      "\n",
      "4. **Prompts**: In the context of LLMs, prompts are the instructions or questions given to the model to generate a response. LangChain helps in crafting effective prompts to get the best results from the LLM.\n",
      "\n",
      "5. **Retrieval**: This refers to the process of fetching relevant information from a database or other sources to enhance the responses generated by the LLM. LangChain integrates retrieval mechanisms to ensure that the applications can provide accurate and contextually relevant information.\n",
      "\n",
      "In summary, LangChain is a valuable tool for anyone looking to develop applications that harness the power of LLMs by effectively combining prompts, tools, and retrieval processes. Whether you're a beginner or an experienced developer, LangChain makes it easier to create intelligent and responsive applications.\n"
     ]
    }
   ],
   "source": [
    "#Advanced example for the same type of chain\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "summarize = PromptTemplate.from_template(\n",
    "    \"Summarize in 1 line:\\n{text}\"\n",
    ") | llm\n",
    "\n",
    "keywords = PromptTemplate.from_template(\n",
    "    \"Extract 5 keywords from this:\\n{text}\"\n",
    ") | llm\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"Create a final beginner explanation using:\\nSummary: {summary}\\nKeywords: {keywords}\"\n",
    ") | llm\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    .assign(summary=lambda x: summarize.invoke({\"text\": x[\"text\"]}).content)\n",
    "    .assign(keywords=lambda x: keywords.invoke({\"text\": x[\"text\"]}).content)\n",
    "    | final\n",
    ")\n",
    "\n",
    "resp = chain.invoke({\"text\": \"LangChain helps build LLM apps by chaining prompts, tools, and retrieval.\"})\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f51edf-0117-459a-9f89-612bf60164df",
   "metadata": {},
   "source": [
    "### RouterChain equivalent (choose Chain A or B)\n",
    "\n",
    "Use RunnableBranch (like if/else routing).\n",
    "\n",
    "RouterChain Equivalent using RunnableBranch\n",
    "\n",
    "##### What is Routing in LangChain?\n",
    "Routing means choosing one chain out of many based on the user input.\n",
    "Just like if / else in programming. \n",
    "\n",
    "##### What is RunnableBranch?\n",
    "RunnableBranch works like if / elif / else for chains.\n",
    "\n",
    "It checks conditions one by one and runs the first matching chain."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dd3b1d7-12f0-4046-aee1-42cd41e3bb71",
   "metadata": {},
   "source": [
    "User Input\n",
    "   ↓\n",
    "Condition 1? → Yes → Chain A\n",
    "   ↓ No\n",
    "Condition 2? → Yes → Chain B\n",
    "   ↓ No\n",
    "Default Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3c1cd-8565-423b-a84f-fc19d04843d0",
   "metadata": {},
   "source": [
    "Example: Customer Care System:\n",
    "\n",
    "If question is billing → Billing team\n",
    "\n",
    "If question is technical → Support team\n",
    "\n",
    "Only one path is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a23d48d3-90b2-464b-9a8b-f6be5f75c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the math question \\( 12 \\times 8 \\) is \\( 96 \\).\n",
      "LangChain is a tool that helps developers build applications using language models, like the ones that power chatbots or text generators. It makes it easier to connect these models with other data sources, manage conversations, and create more complex features. Essentially, LangChain helps you use language technology in your apps more effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "math_chain = PromptTemplate.from_template(\"Solve this math question: {q}\") | llm\n",
    "english_chain = PromptTemplate.from_template(\"Explain this in simple English: {q}\") | llm\n",
    "\n",
    "router = RunnableBranch(\n",
    "    (RunnableLambda(lambda x: \"math\" in x[\"q\"].lower()), math_chain),\n",
    "    english_chain  # default goes here (NOT a tuple)\n",
    ")\n",
    "\n",
    "print(router.invoke({\"q\": \"math: 12*8\"}).content)\n",
    "print(router.invoke({\"q\": \"What is LangChain?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7547a-fc4a-4682-be25-554250e5f323",
   "metadata": {},
   "source": [
    "### RetrievalChain (RAG)\n",
    "\n",
    "RetrievalChain is a modern RAG pattern where an LLM searches documents first and then answers using those documents, making responses accurate and reliable.\n",
    "\n",
    "Large Language Models (LLMs) are powerful, but they have an important limitation: they do not automatically know your private or latest data. They are trained on public data up to a certain point in time and cannot directly read your PDFs, company documents, or databases. If you ask them questions outside their training knowledge, they may guess and produce incorrect answers, which is called hallucination.\n",
    "\n",
    "To solve this problem, the industry uses a pattern called Retrieval-Augmented Generation (RAG). RetrievalChain is LangChain’s modern implementation of this pattern.\n",
    "\n",
    "What RetrievalChain Means\n",
    "\n",
    "RetrievalChain follows a very simple idea:\n",
    "\n",
    "“First search for relevant documents, then use those documents to answer the question.”\n",
    "\n",
    "Instead of expecting the LLM to know everything, we allow it to look up information at runtime and then generate an answer using that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f94d67-37e6-45d9-af1d-2387fbaa4aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is known as the Red Planet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in your environment/.env\"\n",
    "\n",
    "# 0) LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 1) Knowledge base\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Venus is often called Earth's twin because of its similar size and proximity.\"),\n",
    "    Document(page_content=\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\"),\n",
    "    Document(page_content=\"Jupiter, the largest planet in our solar system, has a prominent red spot.\"),\n",
    "    Document(page_content=\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"),\n",
    "]\n",
    "\n",
    "# 2) Vector store (Chroma in-memory)\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = Chroma.from_documents(documents=docs, embedding=emb, collection_name=\"demo_rag\")\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 3) Prompt\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"Answer using ONLY this context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# 4) Retrieval + answer chain\n",
    "def get_context(x):\n",
    "    retrieved_docs = retriever.invoke(x[\"question\"])\n",
    "    return format_docs(retrieved_docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": get_context,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "resp = rag_chain.invoke({\"question\": \"Which plannet is know as Red planet?\"})\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "892725fb-30c5-4e57-8456-022df2b8fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is used for connecting LLMs with prompts, tools, and retrieval (RAG).\n"
     ]
    }
   ],
   "source": [
    "# If needed (run once):\n",
    "# !pip install -U langchain langchain-openai langchain-community chromadb python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in your environment/.env\"\n",
    "\n",
    "# 0) LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 1) Knowledge base\n",
    "docs = [\n",
    "    Document(page_content=\"MySuits-shop sells suits and ethnic wear online and in-store.\"),\n",
    "    Document(page_content=\"LangChain connects LLMs with prompts, tools, and retrieval (RAG).\"),\n",
    "]\n",
    "\n",
    "# 2) Vector store (Chroma in-memory)\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = Chroma.from_documents(documents=docs, embedding=emb, collection_name=\"demo_rag\")\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 3) Prompt\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"Answer using ONLY this context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# 4) Retrieval + answer chain\n",
    "def get_context(x):\n",
    "    retrieved_docs = retriever.invoke(x[\"question\"])\n",
    "    return format_docs(retrieved_docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": get_context,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "resp = rag_chain.invoke({\"question\": \"What is LangChain used for?\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfdb2e-9b97-4a20-85f1-bc168af71c41",
   "metadata": {},
   "source": [
    "### Tool-using chain (LLM decides to call a tool)\n",
    "\n",
    "LangChain supports tool calling via binding tools to the model (bind_tools).\n",
    "\n",
    "Tool-Using Chain (LLM Decides to Call a Tool)\n",
    "\n",
    "A tool-using chain is a pattern where the LLM is not limited to just text generation.\n",
    "Instead, the LLM can decide to call an external tool (such as a calculator, API, database, or search function) when needed and then use the tool’s result to produce the final answer.\n",
    "\n",
    "In simple words, the LLM becomes a decision maker:\n",
    "\n",
    "If text knowledge is enough >> answer directly\n",
    "\n",
    "If an action or real data is required >> call a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01cc5d6e-2336-47d7-8736-7ce19221faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model message: content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 56, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-Cn84rgqeLrhjHolnv5VGAsj6oN2Pt', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b2361-99c2-7880-9e0d-06d8e76abb1d-0' tool_calls=[{'name': 'multiply', 'args': {'a': 12, 'b': 8}, 'id': 'call_dGqq156BLLKVgKOQYqQlisBQ', 'type': 'tool_call'}] usage_metadata={'input_tokens': 56, 'output_tokens': 17, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Tool result: 96\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two integers.\"\n",
    "    return a * b\n",
    "\n",
    "tool_llm = llm.bind_tools([multiply])\n",
    "\n",
    "msg = tool_llm.invoke(\"What is 12 times 8? Use the tool.\")\n",
    "print(\"Model message:\", msg)\n",
    "\n",
    "# If the model called a tool, run it:\n",
    "if getattr(msg, \"tool_calls\", None):\n",
    "    call = msg.tool_calls[0]\n",
    "    name = call[\"name\"]\n",
    "    args = call[\"args\"]\n",
    "\n",
    "    if name == \"multiply\":\n",
    "        tool_result = multiply.invoke(args)\n",
    "        print(\"Tool result:\", tool_result)\n",
    "\n",
    "        # Optional: ask LLM to form final answer using tool output\n",
    "        final = llm.invoke(f\"The tool returned {tool_result}. Reply with the final answer in one line.\")\n",
    "        print(final.content)\n",
    "else:\n",
    "    print(\"No tool call happened. Model answered directly:\", msg.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a170b42-8d57-4656-a465-29f8d1cab9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
