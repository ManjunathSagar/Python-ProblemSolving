{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cf9d25-e803-4219-a1a7-4646e85fb01b",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL) — Quick Notes\n",
    "\n",
    "• LCEL is the modern way to build LLM pipelines in LangChain  \n",
    "• It replaces old Chain classes with a simpler, composable style  \n",
    "• Everything in LCEL is a **Runnable** (can be invoked)\n",
    "\n",
    "### `|` (Pipe Operator)\n",
    "• Passes output of one step to the next  \n",
    "• Example: Prompt → LLM → Parser\n",
    "\n",
    "### RunnableSequence\n",
    "• Executes steps one after another  \n",
    "• Created using the `|` operator  \n",
    "• Most common LCEL pattern\n",
    "\n",
    "### RunnableParallel\n",
    "• Runs multiple chains at the same time  \n",
    "• Useful for summary + sentiment + keywords together\n",
    "\n",
    "### RunnableMap\n",
    "• Applies multiple transformations to the same input  \n",
    "• Lightweight operations (not heavy LLM calls)\n",
    "\n",
    "### Streaming\n",
    "• Streams tokens as the LLM generates output  \n",
    "• Enables real-time chat-like responses\n",
    "\n",
    "### Retry\n",
    "• Automatically retries failed LLM calls  \n",
    "• Improves reliability in production\n",
    "\n",
    "### Fallbacks\n",
    "• Uses a backup chain if the main chain fails  \n",
    "• Ensures system never breaks completely\n",
    "\n",
    "### RunnableLambda \n",
    "It is a way to turn a normal Python function into a LangChain runnable component, so it can be used inside chains just like an LLM, prompt, or retriever.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "“Wrap my Python logic so LangChain can treat it like a step in a chain.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71ac3397-e081-4ba5-8802-75f38f394763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports + a simple LLM that needs NO API key\n",
    "from langchain_core.language_models.fake import FakeListLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    "    RunnableGenerator,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "437b4337-cf6f-49b4-9e21-551c66678a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_llm = FakeListLLM(responses=[\n",
    "    \"FAKE_LLM_RESPONSE: LangChain LCEL makes pipelines composable.\"\n",
    "])\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Answer in 1 line: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd511ca-6e41-4063-827e-e971d558a71a",
   "metadata": {},
   "source": [
    "#### | operator + RunnableSequence (Prompt → LLM → Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a91c615-008a-49c8-bb84-7ba8d5879e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE_LLM_RESPONSE: LangChain LCEL makes pipelines composable.\n"
     ]
    }
   ],
   "source": [
    "# RunnableSequence using pipe operator |\n",
    "sequence_chain = prompt | fake_llm | parser\n",
    "\n",
    "out = sequence_chain.invoke({\"question\": \"What is LCEL?\"})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce808234-f071-431c-a3ae-43d1d0cd14fe",
   "metadata": {},
   "source": [
    "#### RunnableMap (multiple transformations on same input)\n",
    "In LCEL, “map-style” is often done by returning a dict from a Runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "924dd00d-6400-48c1-8833-20053cc092e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': 'langchain', 'upper': 'LANGCHAIN', 'length': 9}\n"
     ]
    }
   ],
   "source": [
    "#RunnableMap-like behavior using RunnableLambda returning a dict\n",
    "map_chain = RunnableLambda(lambda x: {\n",
    "    \"original\": x,\n",
    "    \"upper\": x.upper(),\n",
    "    \"length\": len(x),\n",
    "})\n",
    "\n",
    "print(map_chain.invoke(\"langchain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75707f1-a132-4b38-a5c0-ac35b98a6227",
   "metadata": {},
   "source": [
    "#### RunnableParallel (run multiple branches in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "006e41b0-bec5-4297-9ace-9d32c419b303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'FAKE_LLM_RESPONSE: LangChain LCEL makes pipelines composable.', 'question_length': 19, 'echo': 'ECHO: Explain LCEL simply'}\n"
     ]
    }
   ],
   "source": [
    "# Parallel branches\n",
    "summary_branch = prompt | fake_llm | parser\n",
    "len_branch = RunnableLambda(lambda d: len(d[\"question\"]))  # expects dict input\n",
    "echo_branch = RunnableLambda(lambda d: f\"ECHO: {d['question']}\")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_branch,\n",
    "    \"question_length\": len_branch,\n",
    "    \"echo\": echo_branch\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({\"question\": \"Explain LCEL simply\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab937590-9194-40b2-9e3a-e4ec377762a2",
   "metadata": {},
   "source": [
    "#### Streaming responses\n",
    "\n",
    "Easiest working solution (no API key): use a generator runnable that accepts an iterator and yields chunks\n",
    "\n",
    "This pattern works with LCEL streaming correctly:\n",
    "\n",
    "A function that uses yield:\n",
    "\n",
    "1. Does NOT return everything at once\n",
    "2. Returns values one by one\n",
    "3. Pauses after each yield\n",
    "4. Resumes from where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99e7ffe5-2765-4938-8ae7-62e815eaea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a streaming demo without any API key \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "\n",
    "def stream_transform(input_iter):\n",
    "    # input_iter is an iterator of incoming chunks (not a plain string)\n",
    "    for item in input_iter:\n",
    "        # item will be the original input string here\n",
    "        for w in str(item).split():\n",
    "            yield w + \" \"\n",
    "\n",
    "stream_chain = RunnableGenerator(stream_transform)\n",
    "\n",
    "for chunk in stream_chain.stream(\"This is a streaming demo without any API key\"):\n",
    "    print(chunk, end=\"\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631963da-2567-45f5-a9df-2c36fc3a3c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f37d5a48-4cdc-4ccc-9c8a-6fac8060c531",
   "metadata": {},
   "source": [
    "#### Retry (automatic retry if something fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e31e0e4-5eb8-4592-8189-dd5812a23f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success on try #2: Hello Retry\n"
     ]
    }
   ],
   "source": [
    "state = {\"tries\": 0}\n",
    "\n",
    "def flaky_fn(x):\n",
    "    state[\"tries\"] += 1 \n",
    "    if state[\"tries\"] < 2:\n",
    "        raise RuntimeError(\"Temporary failure, please retry!\")\n",
    "    return f\"Success on try #{state['tries']}: {x}\"\n",
    "\n",
    "flaky_runnable = RunnableLambda(flaky_fn).with_retry(stop_after_attempt=3)\n",
    "\n",
    "print(flaky_runnable.invoke(\"Hello Retry\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e032e-ab9f-46c3-b6c5-0a326d5a7f36",
   "metadata": {},
   "source": [
    "#### Fallbacks (backup runnable if primary fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a2cb7e2-a6ae-4dea-a122-b6f06380c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup worked for input: Hello Fallback\n"
     ]
    }
   ],
   "source": [
    "primary = RunnableLambda(lambda x: (_ for _ in ()).throw(RuntimeError(\"Primary failed!\")))\n",
    "backup = RunnableLambda(lambda x: f\"Backup worked for input: {x}\")\n",
    "\n",
    "fallback_chain = primary.with_fallbacks([backup])\n",
    "\n",
    "print(fallback_chain.invoke(\"Hello Fallback\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c42802-fd1a-4588-99a5-95d4428be35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
