{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2468bd2c-07f9-4444-bacf-7600342f33e6",
   "metadata": {},
   "source": [
    "#### LLM Wrappers in LangChain\n",
    "\n",
    "An LLM Wrapper is a Python class that lets LangChain talk to an AI model in a standard way.\n",
    "\n",
    "##### Understaning only - Story/Example \n",
    "+ Think of it like a universal remote \n",
    "+ Different TVs >>>>> one remote\n",
    "+ Different LLMs >>>>> one wrapper interface\n",
    " \n",
    "‚ùå Without wrappers ‚Üí you must learn each API\n",
    "‚úÖ With wrappers ‚Üí one common interface\n",
    "\n",
    "##### Key Benefit\n",
    "\n",
    "‚ÄúWrite once, switch models anytime‚Äù\n",
    "\n",
    "##### You can switch:\n",
    "1. GPT ‚Üí Claude\n",
    "2. Claude ‚Üí Llama\n",
    "3. Llama ‚Üí Mistral\n",
    "\n",
    "##### Without changing:\n",
    "1. prompts\n",
    "2. chains\n",
    "3. memory\n",
    "4. agents\n",
    "\n",
    "LLM Wrappers in LangChain provide a common interface to interact with different AI models without worrying about their individual APIs.\n",
    "\n",
    "LLM wrappers are critical for agents because they provide a consistent interface, enable tool calling, manage multi-step reasoning loops, and allow agents to work across different models without rewriting logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f164ce49-38e9-4b6c-b15c-a52e6d71543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97479634-5a11-47fa-a7ad-b65211e2c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI. It provides tools and components to easily create chatbots, virtual assistants, or other applications that can process and generate text. Think of it as a toolkit that makes it simpler to connect language models with different data sources and use them effectively in various projects.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Make sure OPENAI_API_KEY is set in your environment\n",
    "# export OPENAI_API_KEY=\"sk-...\"  (Mac/Linux)\n",
    "# setx OPENAI_API_KEY \"sk-...\"    (Windows)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # or \"gpt-4.1\", etc.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI tutor.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"What is LangChain in simple words?\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1729bf4a-e30f-41ca-81b3-b8c00879dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key present? True\n",
      "claude-opus-4-5-20251101\n",
      "claude-haiku-4-5-20251001\n",
      "claude-sonnet-4-5-20250929\n",
      "claude-opus-4-1-20250805\n",
      "claude-opus-4-20250514\n",
      "claude-sonnet-4-20250514\n",
      "claude-3-7-sonnet-20250219\n",
      "claude-3-5-haiku-20241022\n",
      "claude-3-haiku-20240307\n",
      "claude-3-opus-20240229\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"Key present?\", os.getenv(\"ANTHROPIC_API_KEY\") is not None)\n",
    "\n",
    "client = anthropic.Anthropic()  # uses ANTHROPIC_API_KEY\n",
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723b89aa-3489-4236-99d1-70c1b84d954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What's an LLM? ü§ñ\n",
      "\n",
      "Imagine you have a really smart robot friend who has read almost every book, website, and story on the internet. That's kind of what an LLM is!\n",
      "\n",
      "**LLM stands for \"Large Language Model\"** - which is a fancy way of saying \"a computer program that's really good with words.\"\n",
      "\n",
      "## How does it work?\n",
      "\n",
      "Think of it like this:\n",
      "- You know how you learn patterns? Like \"if it's raining, I should bring an umbrella\"\n",
      "- An LLM learned TONS of patterns about words - like which words usually go together, how to answer questions, and how to write stories\n",
      "\n",
      "## What can it do?\n",
      "\n",
      "- Answer your questions (like I'm doing right now!)\n",
      "- Help write stories or poems\n",
      "- Explain complicated things in simple ways\n",
      "- Translate languages\n",
      "- Help with homework\n",
      "\n",
      "## The coolest part?\n",
      "\n",
      "It doesn't actually \"know\" things like you do. It's more like it's really, really good at guessing what words should come next based on all the patterns it learned. Kind of like how you can finish your friend's sentence because you know them\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\", \n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Explain what an LLM is to a 10 year old.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda23a65-5b60-46ab-98da-e00feb1fa38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 compelling use-cases of Large Language Models (LLMs) in software testing:\n",
      "\n",
      "1.  **Automated Test Case Generation and Test Data Creation:**\n",
      "    *   **How LLMs Help:** LLMs can take various inputs like user stories, requirements documents, design specifications, or even existing code, and generate comprehensive test cases. They can infer scenarios, edge cases, positive/negative tests, and even produce detailed steps, expected results, and priority levels. Furthermore, they can generate realistic and varied test data (e.g., names, addresses, emails, product IDs) based on specified schemas, constraints, or inferred patterns, significantly reducing the manual effort of test data preparation.\n",
      "    *   **Benefit:** Dramatically speeds up the test design phase, ensures better test coverage by identifying overlooked scenarios, and provides more realistic test data for effective testing.\n",
      "\n",
      "2.  **Translating Natural Language to Test Automation Scripts:**\n",
      "    *   **How LLMs Help:** Testers can describe desired test actions in plain English (or other natural languages) like \"Login to the application with username 'testuser' and password 'password123',\" or \"Verify that the product price on the checkout page matches the price on the product detail page.\" An LLM can then translate these descriptions into executable code for various test automation frameworks (e.g., Selenium, Playwright, Cypress, Appium) or API testing tools (e.g., Postman collections, RestAssured scripts).\n",
      "    *   **Benefit:** Lowers the barrier to entry for test automation, allowing non-developers or less experienced automation engineers to create scripts faster. It also accelerates script development for complex scenarios and reduces boilerplate code writing.\n",
      "\n",
      "3.  **Intelligent Test Report Analysis and Bug Triaging:**\n",
      "    *   **How LLMs Help:** After test execution, LLMs can analyze vast amounts of test logs, failure reports, and crash dumps. They can summarize key findings, identify patterns in failures (e.g., common error messages, components failing together), suggest potential root causes, and even prioritize bugs based on severity, frequency, or impact mentioned in the logs. They can also cross-reference failures with recent code changes or deployment notes to provide context for developers.\n",
      "    *   **Benefit:** Saves significant time in post-execution analysis, helps QA teams more quickly identify critical issues, facilitates faster bug triaging, and provides developers with more actionable insights for debugging.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Give me 3 use-cases of LLMs in testing.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12b8649d-7bfa-4c82-bc74-32551be2569b",
   "metadata": {},
   "source": [
    "ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}} Selection deleted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a99f621-3f90-4cdf-a81a-9f7164b23867",
   "metadata": {},
   "source": [
    "ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3544afcd-c30e-413b-b60f-5ae0b2b24c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to explain LLMs in one line. Let me start by recalling what LLMs are. LLM stands for Large Language Model. They're a type of artificial intelligence that can generate human-like text based on patterns learned from large datasets. The key points here are their ability to process and generate text, their use of deep learning models, and their reliance on vast amounts of data for training.\n",
      "\n",
      "I need to make sure the explanation is concise but still covers the main aspects. Maybe start with the definition, mention their ability to generate text, their reliance on large datasets, and their application in various tasks like writing, answering questions, or creating content. Also, it's important to note that they use neural networks and deep learning techniques. Let me check if I'm missing anything. Oh, right, they can understand and respond to human language, which is why they're so versatile. I should also mention that they're trained on diverse data to improve their performance. Let me put that all together in a single, clear sentence.\n",
      "</think>\n",
      "\n",
      "Large Language Models (LLMs) are advanced AI systems that process and generate human-like text by analyzing patterns in vast datasets, enabling applications like writing, translation, and answering questions, while adapting to complex language nuances through deep learning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B:hf-inference\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain LLMs in one line\"}],\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77329f5d-b14d-4fd3-b436-fa446d0a35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to explain LLMs in one line. Let me start by recalling what LLMs are. They're large language models, right? Trained on massive datasets to generate human-like text. But I need to be concise. Let me break it down.\n",
      "\n",
      "First, define LLMs as models that can process and generate text based on patterns learned from data. They use techniques like transformers and attention mechanisms. The key points are their ability to understand and produce language, their training on vast amounts of text, and their applications in various fields like AI, content creation, and more.\n",
      "\n",
      "Wait, the user wants it in one line. So I need to condense all that into a single sentence. Let me try: \"Large language models (LLMs) are AI systems trained on vast datasets to generate human-like text, using transformer-based architectures and attention mechanisms, enabling applications in natural language processing, content creation, and more.\"\n",
      "\n",
      "Does that cover it? Let me check. It mentions the training data, the architecture, and the applications. Maybe I can make it even shorter. Remove \"AI systems\" since it's implied. Also, \"transformer-based architectures and attention mechanisms\" can be simplified to \"transformer-based models with attention.\" Hmm, but maybe the user wants the technical terms. Let me stick with the original. Alright, that should be a concise explanation.\n",
      "</think>\n",
      "\n",
      "Large language models (LLMs) are AI systems trained on vast datasets to generate human-like text, using transformer-based architectures and attention mechanisms, enabling applications in natural language processing, content creation, and more.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B:hf-inference\",\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"Explain LLMs in one line\")\n",
    "])\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca998b0-ddda-434d-99c0-6ca4bdcf9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edc3192-1429-4b90-b14d-87cf296a802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load tokenizer and model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73011420cc184cb6b9bfa861b6a9e01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ed9a02ac9e4bf8bea70298d8523c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd76ed38e905448899a1ec71411f58c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LotusBlue\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LotusBlue\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolLM3-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26ee7b32ab84a3cb3df4dd128bb6593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68b1f5853d34453b8c4ba699db0c87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/182 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put model on GPU if available\n",
      "Prepare prompt\n",
      "For chat-style prompts, wrap as chat messages\n",
      "Apply SmolLM3 chat template\n",
      "Tokenize and generate\n",
      "Decode & print\n",
      "<think>\n",
      "Okay, the user is asking for a simple explanation of LLM, which stands for Large Language Model. Let me start by breaking down the term. \"Large\" here refers to the model's size, which can be hundreds of billions of parameters. That's a lot compared to traditional models.\n",
      "\n",
      "Next, \"language model\" means the model is designed to understand and generate human language. So, it can predict what comes next in a sentence or create new text. I should mention that it's trained on a massive dataset, probably text from the internet or books, to learn patterns and relationships between words.\n",
      "\n",
      "I need to explain how it works in simple terms. Maybe use an analogy, like a really smart person who's read everything and can guess the next word in a sentence. But it's important to note that it's a machine, not a person, so it can't truly understand like a human does.\n",
      "\n",
      "Also, the user might be interested in practical applications. So, I should\n"
     ]
    }
   ],
   "source": [
    "# ===== Model Name =====\n",
    "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# ===== Load tokenizer and model =====\n",
    "print(\"Load tokenizer and model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Put model on GPU if available\n",
    "print(\"Put model on GPU if available\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# ===== Prepare prompt =====\n",
    "print(\"Prepare prompt\")\n",
    "prompt = \"Explain in simple terms what LLM is.\"\n",
    "\n",
    "# For chat-style prompts, wrap as chat messages\n",
    "print(\"For chat-style prompts, wrap as chat messages\")\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Apply SmolLM3 chat template\n",
    "print(\"Apply SmolLM3 chat template\")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    chat_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# ===== Tokenize and generate =====\n",
    "print(\"Tokenize and generate\")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,   # adjust length\n",
    "    temperature=0.7,      # creativity\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# ===== Decode & print =====\n",
    "print(\"Decode & print\")\n",
    "generated = output_ids[0][len(inputs.input_ids[0]):]\n",
    "print(tokenizer.decode(generated, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
