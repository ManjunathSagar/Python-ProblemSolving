{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2468bd2c-07f9-4444-bacf-7600342f33e6",
   "metadata": {},
   "source": [
    "LLM Wrappers in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f164ce49-38e9-4b6c-b15c-a52e6d71543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97479634-5a11-47fa-a7ad-b65211e2c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to help developers build applications that use language models, like ChatGPT or other AI text processing tools. It provides a set of tools and components that make it easier to connect language models with other resources, such as databases or APIs, handle complex workflows, and manage conversations effectively. Simply put, it streamlines the process of creating smart, interactive applications that can understand and generate human-like text.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Make sure OPENAI_API_KEY is set in your environment\n",
    "# export OPENAI_API_KEY=\"sk-...\"  (Mac/Linux)\n",
    "# setx OPENAI_API_KEY \"sk-...\"    (Windows)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # or \"gpt-4.1\", etc.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI tutor.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"What is LangChain in simple words?\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1729bf4a-e30f-41ca-81b3-b8c00879dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key present? True\n",
      "claude-opus-4-5-20251101\n",
      "claude-haiku-4-5-20251001\n",
      "claude-sonnet-4-5-20250929\n",
      "claude-opus-4-1-20250805\n",
      "claude-opus-4-20250514\n",
      "claude-sonnet-4-20250514\n",
      "claude-3-7-sonnet-20250219\n",
      "claude-3-5-haiku-20241022\n",
      "claude-3-haiku-20240307\n",
      "claude-3-opus-20240229\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"Key present?\", os.getenv(\"ANTHROPIC_API_KEY\") is not None)\n",
    "\n",
    "client = anthropic.Anthropic()  # uses ANTHROPIC_API_KEY\n",
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723b89aa-3489-4236-99d1-70c1b84d954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What's an LLM?\n",
      "\n",
      "Imagine you have a really smart robot friend that has read millions and millions of books, websites, and stories. An LLM (which stands for \"Large Language Model\") is kind of like that robot's brain!\n",
      "\n",
      "## How it works:\n",
      "\n",
      "**It's like a super pattern finder** ðŸ§©\n",
      "- It has seen SO many sentences that it learned how words usually fit together\n",
      "- Like how you know \"peanut butter and ___\" probably ends with \"jelly\"\n",
      "- But it can do this with much more complicated stuff!\n",
      "\n",
      "**It doesn't actually \"think\" like you do** ðŸ¤”\n",
      "- It doesn't have feelings or really understand things\n",
      "- It's more like a really, really good guesser\n",
      "- It predicts what words should come next, kind of like autocomplete on your mom's phone, but way more advanced\n",
      "\n",
      "## What can it do?\n",
      "\n",
      "- Answer questions\n",
      "- Write stories\n",
      "- Help with homework\n",
      "- Translate languages\n",
      "- Even write code!\n",
      "\n",
      "**Think of it like this:** If you typed millions of sentences into a super-smart calculator, it could learn to complete your sentences in ways that make\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\", \n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Explain what an LLM is to a 10 year old.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0da85bf-26d1-4253-b85f-52be591d4387",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or gemini-1.5-pro\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Give me 3 use-cases of LLMs in testing.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3544afcd-c30e-413b-b60f-5ae0b2b24c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatHuggingFace, HuggingFaceEndpoint\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Under the hood it calls a HF Inference Endpoint\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_huggingface'"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Under the hood it calls a HF Inference Endpoint\n",
    "hf_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",  \n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(\n",
    "    endpoint=hf_endpoint\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Explain LangChain in 2-3 bullet points.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77329f5d-b14d-4fd3-b436-fa446d0a35af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
