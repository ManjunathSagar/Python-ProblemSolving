{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2468bd2c-07f9-4444-bacf-7600342f33e6",
   "metadata": {},
   "source": [
    "#### LLM Wrappers in LangChain\n",
    "\n",
    "An LLM Wrapper is a Python class that lets LangChain talk to an AI model in a standard way.\n",
    "\n",
    "##### Understaning only - Story/Example \n",
    "+ Think of it like a universal remote \n",
    "+ Different TVs >>>>> one remote\n",
    "+ Different LLMs >>>>> one wrapper interface\n",
    " \n",
    "❌ Without wrappers → you must learn each API\n",
    "✅ With wrappers → one common interface\n",
    "\n",
    "##### Key Benefit\n",
    "\n",
    "“Write once, switch models anytime”\n",
    "\n",
    "##### You can switch:\n",
    "1. GPT → Claude\n",
    "2. Claude → Llama\n",
    "3. Llama → Mistral\n",
    "\n",
    "##### Without changing:\n",
    "1. prompts\n",
    "2. chains\n",
    "3. memory\n",
    "4. agents\n",
    "\n",
    "LLM Wrappers in LangChain provide a common interface to interact with different AI models without worrying about their individual APIs.\n",
    "\n",
    "LLM wrappers are critical for agents because they provide a consistent interface, enable tool calling, manage multi-step reasoning loops, and allow agents to work across different models without rewriting logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f164ce49-38e9-4b6c-b15c-a52e6d71543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97479634-5a11-47fa-a7ad-b65211e2c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to help developers build applications that utilize language models, like those from OpenAI or other AI providers. It makes it easier to connect these models with other tools and data sources, allowing for more complex and interactive AI applications. Essentially, it helps you create software that can understand and generate human language effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Make sure OPENAI_API_KEY is set in your environment\n",
    "# export OPENAI_API_KEY=\"sk-...\"  (Mac/Linux)\n",
    "# setx OPENAI_API_KEY \"sk-...\"    (Windows)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # or \"gpt-4.1\", etc.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI tutor.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"What is LangChain in simple words?\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1729bf4a-e30f-41ca-81b3-b8c00879dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key present? True\n",
      "claude-opus-4-5-20251101\n",
      "claude-haiku-4-5-20251001\n",
      "claude-sonnet-4-5-20250929\n",
      "claude-opus-4-1-20250805\n",
      "claude-opus-4-20250514\n",
      "claude-sonnet-4-20250514\n",
      "claude-3-7-sonnet-20250219\n",
      "claude-3-5-haiku-20241022\n",
      "claude-3-haiku-20240307\n",
      "claude-3-opus-20240229\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"Key present?\", os.getenv(\"ANTHROPIC_API_KEY\") is not None)\n",
    "\n",
    "client = anthropic.Anthropic()  # uses ANTHROPIC_API_KEY\n",
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723b89aa-3489-4236-99d1-70c1b84d954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What's an LLM?\n",
      "\n",
      "Imagine you have a really smart robot friend who has read almost every book, website, and story in the world. That's kind of what an LLM is!\n",
      "\n",
      "**LLM stands for \"Large Language Model\"** - which is a fancy way of saying \"a computer program that's really good with words.\"\n",
      "\n",
      "## How does it work?\n",
      "\n",
      "Think of it like this: \n",
      "\n",
      "- When you were learning to talk, you listened to people around you say lots of words and sentences\n",
      "- You learned patterns, like \"the sky is ___\" is usually followed by \"blue\"\n",
      "- An LLM does the same thing, but with BILLIONS of sentences!\n",
      "\n",
      "## What can it do?\n",
      "\n",
      "An LLM can:\n",
      "- Answer questions (like I'm doing now!)\n",
      "- Write stories\n",
      "- Help with homework\n",
      "- Translate languages\n",
      "- Even write poems or jokes\n",
      "\n",
      "## The important part:\n",
      "\n",
      "It's not actually \"thinking\" like you do. It's more like a super-powered pattern matcher - it predicts what words should come next based on all the examples it learned from. Kind of like how you can finish the sentence \"Twink\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\", \n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Explain what an LLM is to a 10 year old.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda23a65-5b60-46ab-98da-e00feb1fa38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs are rapidly finding applications across the software development lifecycle, and testing is no exception. Here are 3 use-cases of LLMs in testing:\n",
      "\n",
      "1.  **Test Case and Scenario Generation:**\n",
      "    *   **How it works:** An LLM can be fed various inputs such as user stories, functional requirements, design documents, API specifications, or even existing manual test cases. It can then process this natural language or structured data to generate detailed test cases (including steps, expected results, preconditions) and comprehensive test scenarios. It can identify edge cases, boundary conditions, and negative test scenarios that might be overlooked by humans.\n",
      "    *   **Example:** Providing an LLM with a user story like \"As a user, I want to be able to add items to my shopping cart and proceed to checkout,\" and it could generate test cases for: adding a single item, adding multiple items, adding items to an empty cart, adding items to a non-empty cart, attempting to add an out-of-stock item, removing an item, increasing/decreasing quantity, proceeding to checkout with an empty cart, proceeding with items, etc.\n",
      "    *   **Benefit:** Significantly accelerates the test design phase, ensures broader test coverage, and helps identify potential gaps in requirements.\n",
      "\n",
      "2.  **Automated Test Script Creation and Maintenance:**\n",
      "    *   **How it works:** LLMs can generate executable test scripts in various programming languages and testing frameworks (e.g., Python with Selenium/Playwright, Java with JUnit/TestNG, JavaScript with Cypress/Jest) from high-level descriptions, existing manual test cases, or even by analyzing application UI changes. They can also assist in debugging, refactoring, and updating existing test scripts to adapt to application changes, making maintenance more efficient.\n",
      "    *   **Example:** A tester could describe an interaction: \"Log in to the application with valid credentials, navigate to the user profile page, and verify the user's email address is displayed correctly.\" The LLM could then generate the corresponding Selenium/Playwright code to perform these actions and assertions. If the UI element locator changes, the LLM could suggest updates to the script.\n",
      "    *   **Benefit:** Speeds up the automation process, reduces the technical barrier for non-coding testers, and helps keep automation frameworks robust and up-to-date with less manual effort.\n",
      "\n",
      "3.  **Intelligent Test Data Generation:**\n",
      "    *   **How it works:** LLMs can generate diverse, realistic, and relevant test data based on schema definitions, data models, business rules, or examples of existing data (anonymized, of course). They can create data for specific test scenarios, including valid inputs, invalid inputs, boundary values, international variations, and data tailored for performance or security testing. This goes beyond simple random data generation by understanding context and relationships.\n",
      "    *   **Example:** For an e-commerce platform, an LLM could generate a variety of user profiles (different age groups, demographics, countries), product data (varying prices, stock levels, categories), or payment details (valid credit card numbers with correct checksums, invalid numbers, different card types) – all while adhering to specified data constraints and relationships.\n",
      "    *   **Benefit:** Improves the thoroughness of testing by using more realistic and varied data, helps uncover defects related to data handling, and reduces the manual effort and time spent on test data setup.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "resp = chain.invoke({\"input\": \"Give me 3 use-cases of LLMs in testing.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12b8649d-7bfa-4c82-bc74-32551be2569b",
   "metadata": {},
   "source": [
    "ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}} Selection deleted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a99f621-3f90-4cdf-a81a-9f7164b23867",
   "metadata": {},
   "source": [
    "ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3544afcd-c30e-413b-b60f-5ae0b2b24c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to explain LLMs in one line. Let me start by recalling what LLMs are. LLM stands for Large Language Model. They're a type of AI model that can generate human-like text based on patterns learned from vast amounts of data.\n",
      "\n",
      "I need to make this concise. Maybe mention their ability to process and generate text, their training on huge datasets, and their applications like chatbots, content creation, and more. Also, highlight their significance in AI and natural language processing.\n",
      "\n",
      "Wait, the user specified \"in one line,\" so I have to be precise. Let me structure it: Start with the term, describe their function, mention the data they're trained on, and their applications. Avoid technical jargon where possible. Make sure it's clear and covers the main points without being too verbose. Let me check if I missed anything important. Oh, maybe mention that they can understand and generate text, which is a key aspect. Alright, putting it all together now.\n",
      "</think>\n",
      "\n",
      "Large Language Models (LLMs) are AI systems trained on vast datasets to process, understand, and generate human-like text, enabling applications like chatbots, content creation, and language translation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B:hf-inference\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain LLMs in one line\"}],\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77329f5d-b14d-4fd3-b436-fa446d0a35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to explain LLMs in one line. Let me start by recalling what LLMs are. They're large language models, right? Trained on massive datasets to generate human-like text. But I need to be concise. Let me break it down.\n",
      "\n",
      "First, define LLMs as a type of AI. Then mention their training on vast amounts of text data. Next, their ability to process and generate human-like language. Also, their applications in various fields like chatbots, content creation, and more. I should avoid technical jargon but still be accurate. Let me check if I'm missing anything. Oh, maybe mention that they can understand context and generate coherent responses. That's important for their functionality. Let me put it all together in one line without being too wordy. Let me try: \"Large language models (LLMs) are AI systems trained on massive datasets to generate human-like text, enabling applications in natural language processing, content creation, and interactive systems by understanding context and producing coherent responses.\" Hmm, does that cover it? I think so. Let me check the key points again: training on data, generating text, applications, context understanding, coherence. Yes, that's all there. I think that's a solid one-liner.\n",
      "</think>\n",
      "\n",
      "Large language models (LLMs) are AI systems trained on vast datasets to generate human-like text, enabling applications in natural language processing, content creation, and interactive systems by understanding context and producing coherent responses.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B:hf-inference\",\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"Explain LLMs in one line\")\n",
    "])\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca998b0-ddda-434d-99c0-6ca4bdcf9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edc3192-1429-4b90-b14d-87cf296a802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load tokenizer and model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cce006da1840489e8a0d812f6c96d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put model on GPU if available\n",
      "Prepare prompt\n",
      "For chat-style prompts, wrap as chat messages\n",
      "Apply SmolLM3 chat template\n",
      "Tokenize and generate\n",
      "Decode & print\n",
      "<think>\n",
      "Okay, so I need to explain what an LLM is in simple terms. Let me start by breaking down the acronym. LLM stands for Large Language Model. But what does that really mean?\n",
      "\n",
      "First, I should define what a language model is. From what I remember, a language model is a computer program that predicts the next word in a sentence based on the words that came before it. Like how a chatbot might guess what you're going to say next. But how does that work?\n",
      "\n",
      "I think it uses some kind of data, maybe text. The model is trained on a huge amount of text data, like books, articles, websites, etc. So it learns patterns and structures in language. When you give it a prompt, like a question, it tries to generate a response based on what it learned from that training data.\n",
      "\n",
      "Wait, but how does it actually generate text? I've heard terms like deep learning and neural networks. Maybe an LLM is a type\n"
     ]
    }
   ],
   "source": [
    "# ===== Model Name =====\n",
    "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# ===== Load tokenizer and model =====\n",
    "print(\"Load tokenizer and model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Put model on GPU if available\n",
    "print(\"Put model on GPU if available\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# ===== Prepare prompt =====\n",
    "print(\"Prepare prompt\")\n",
    "prompt = \"Explain in simple terms what LLM is.\"\n",
    "\n",
    "# For chat-style prompts, wrap as chat messages\n",
    "print(\"For chat-style prompts, wrap as chat messages\")\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Apply SmolLM3 chat template\n",
    "print(\"Apply SmolLM3 chat template\")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    chat_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# ===== Tokenize and generate =====\n",
    "print(\"Tokenize and generate\")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,   # adjust length\n",
    "    temperature=0.7,      # creativity\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# ===== Decode & print =====\n",
    "print(\"Decode & print\")\n",
    "generated = output_ids[0][len(inputs.input_ids[0]):]\n",
    "print(tokenizer.decode(generated, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
