{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1e5eff-69a3-4d9e-b1c4-ed521899e4ae",
   "metadata": {},
   "source": [
    "##### What is prompt template?\n",
    "\n",
    "A Prompt Template is a predefined structure for prompts where only some parts change.\n",
    "\n",
    "##### Why do we use prompt templates?\n",
    "Instead of writing prompts again and again, we create a template and fill values dynamically\n",
    "\n",
    "##### Why Prompt Templates Matter (Before We Go Deeper)\n",
    "\n",
    "Without templates:\n",
    "+ prompts change every time\n",
    "+ outputs become unpredictable\n",
    "+ hallucinations increase\n",
    "+ agents break\n",
    "\n",
    "With templates:\n",
    "+ consistent instructions\n",
    "+ controlled outputs\n",
    "+ reusable prompts\n",
    "+ safer AI behavior\n",
    "\n",
    "##### Basic Prompt Template\n",
    "A simple text prompt with placeholders. Think of it like: \"Fill in the blanks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6a99b4-33b3-4f71-b424-08f4af20f566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain LLM in simple words'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Explain {topic} in simple words\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "prompt.format(topic=\"LLM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8476c-c191-4a84-a030-7e5180790057",
   "metadata": {},
   "source": [
    "Structured Output Prompt\n",
    "\n",
    "Problem Without Structure you face below issues: \n",
    "+ Sometimes paragraph\n",
    "+ Sometimes bullet points\n",
    "+ Sometimes random format\n",
    "\n",
    "This is hallucination in format, not facts.\n",
    "To resolve this problem: We tell the model exactly how to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fcec80-0014-4ab6-81b8-f1f05e33fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain {topic}.\n",
    "Return output strictly in JSON format:\n",
    "\n",
    "{{\n",
    "  \"definition\": \"\",\n",
    "  \"example\": \"\",\n",
    "  \"use_case\": \"\"\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0979869-676e-4a75-8b4d-df67d93e24b5",
   "metadata": {},
   "source": [
    "Why This Is Important\n",
    "\n",
    "‚úî Predictable output\n",
    "‚úî Easy to parse\n",
    "‚úî Required for APIs & agents\n",
    "\n",
    "üìå Agents REQUIRE structured outputs\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1133a-80f1-4ce6-b6de-5fd9dae1d5db",
   "metadata": {},
   "source": [
    "##### ChatPromptTemplate\n",
    "\n",
    "Its used for chat-based models such as GPT, Claude, etc.\n",
    "\n",
    "Supports roles:\n",
    "+ system\n",
    "+ human\n",
    "+ ai\n",
    "\n",
    "Why This Is Powerful\n",
    "+ System message controls behavior\n",
    "+ Human message controls task\n",
    "+ Separates instruction vs question\n",
    "\n",
    "Where to Use\n",
    "\n",
    "1. Chatbots\n",
    "2. Agents\n",
    "3. RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad08c28-a427-4851-9596-ca3aed61d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful teacher\"),\n",
    "    (\"human\", \"Explain {topic} to a beginner\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6530d-c70d-4585-a7c4-440b665361ad",
   "metadata": {},
   "source": [
    "##### Few-Shot Prompts: You show examples so the model learns the pattern.\n",
    "\n",
    "Why Few-Shot Works?\n",
    "\n",
    "LLMs are pattern learners. Examples reduce guessing.\n",
    "\n",
    "When should you use Few-Shot\n",
    "\n",
    "+ Classification\n",
    "+ Formatting tasks\n",
    "+ Data extraction\n",
    "+ Reducing hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68677d55-6f2d-4d65-8791-6cd545513151",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Q: What is AI?\n",
    "A: AI is the ability of machines to think.\n",
    "\n",
    "Q: What is ML?\n",
    "A: ML is a way machines learn from data.\n",
    "\n",
    "Q: What is LLM?\n",
    "A:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ef5ab-b013-4bd1-bc77-e12b9d896ec8",
   "metadata": {},
   "source": [
    "##### Why Prompt Templates Reduce Hallucinations\n",
    "\n",
    "Here are the reasons why prompt templates reduces hallucinations\n",
    "1. Clear Instructions: ‚ùå ‚ÄúExplain AI‚Äù ‚úÖ ‚ÄúExplain AI in 3 bullet points for beginners‚Äù\n",
    "2. Fixed Structure: Templates lock the response shape { \"answer\": \"\", \"example\": \"\" }\n",
    "3. Consistency Across Calls: Same prompt structure ‚Üí similar outputs. This is critical for agents\n",
    "4. Separation of Roles: System ‚â† Human ‚â† Output. Model understands who says what\n",
    "5. Reusability & Control: You don‚Äôt \"re-prompt\" every time: One correct template >>>>> reused everywhere\n",
    "   \n",
    "Prompt templates reduce hallucinations by enforcing clear instructions, fixed structure, consistent behavior, and example-based guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af598c40-67aa-47a5-a05e-97b85a090121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain langchain-openai pydantic python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d547817-d398-4932-8c0d-219745eae885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"Product name\")\n",
    "    price_inr: int = Field(..., ge=0, description=\"Price in INR (non-negative integer)\")\n",
    "    category: str = Field(..., description=\"Category like saree/kurti/coords\")\n",
    "    sizes: list[str] = Field(..., description=\"Available sizes, e.g., ['M','L','XL']\")\n",
    "    in_stock: bool = Field(..., description=\"Availability\")\n",
    "\n",
    "# All flights to London at 29999\n",
    "\n",
    "# class FlightInfo(BaseModel):\n",
    "#     destination: str = Field(..., description=\"Travel destination\")\n",
    "#     price_inr: int = Field(..., ge=0, description=\"Prices in INR (non-negative integer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a18bd90-8a1f-4c99-b087-9e09908ea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ProductInfo)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a strict data extraction assistant. Output must follow the schema.\"),\n",
    "    (\"human\",\n",
    "     \"Extract product details from this text:\\n\\n\"\n",
    "     \"{text}\\n\\n\"\n",
    "     \"{format_instructions}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde9632d-6d58-4e34-8584-d5d64942821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductInfo(name='Maslin kurti set with handwork', price_inr=1799, category='kurti', sizes=['M', 'L', 'XL', 'XXL'], in_stock=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "New Arrival\n",
    "Maslin kurti set with handwork, Dupatta: digital print.\n",
    "Price Rs 1799. Sizes M to XXL. Available now.\n",
    "\"\"\"\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"text\": text,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aefc776-b440-4c1d-9c2f-194305e1b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maslin kurti set with handwork\n",
      "1799\n",
      "['M', 'L', 'XL', 'XXL']\n",
      "{'name': 'Maslin kurti set with handwork', 'price_inr': 1799, 'category': 'kurti', 'sizes': ['M', 'L', 'XL', 'XXL'], 'in_stock': True}\n"
     ]
    }
   ],
   "source": [
    "print(result.name)\n",
    "print(result.price_inr)\n",
    "print(result.sizes)\n",
    "print(result.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4325ebf5-4467-4905-8361-0a3c718c3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed ‚úÖ\n",
      "<class 'NameError'>\n",
      "name 'chain_strict' is not defined\n"
     ]
    }
   ],
   "source": [
    "bad_text = \"Name: Saree, Price: 'five hundred' INR, Category: saree, Sizes: M, In stock: yes\"\n",
    "\n",
    "try:\n",
    "    chain_strict.invoke({\n",
    "        \"text\": bad_text,\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(\"Validation failed ‚úÖ\")\n",
    "    print(type(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06a8f230-cb5f-464d-9f96-860924942bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed ‚úÖ\n",
      "<class 'langchain_core.exceptions.OutputParserException'>\n",
      "Failed to parse ProductInfo from completion {\"name\": \"Saree\", \"price_inr\": -500, \"category\": \"saree\", \"sizes\": [\"M\"], \"in_stock\": true}. Got: 1 validation error for ProductInfo\n",
      "price_inr\n",
      "  Input should be greater than or equal to 0 [type=greater_than_equal, input_value=-500, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/greater_than_equal\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "bad_text = \"Price is -500 INR, size: M, product: Saree, in stock: yes\"\n",
    "\n",
    "strict_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a strict data extraction assistant.\\n\"\n",
    "     \"IMPORTANT: Do not correct, transform, or sanitize values.\\n\"\n",
    "     \"If the text says price is -500, output -500.\\n\"\n",
    "     \"Output must follow the schema.\"),\n",
    "    (\"human\",\n",
    "     \"Extract product details from this text:\\n\\n\"\n",
    "     \"{text}\\n\\n\"\n",
    "     \"{format_instructions}\")\n",
    "])\n",
    "\n",
    "chain_strict = strict_prompt | llm | parser\n",
    "\n",
    "try:\n",
    "    chain_strict.invoke({\n",
    "        \"text\": bad_text,\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(\"Validation failed ‚úÖ\")\n",
    "    print(type(e))\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6168f3a-58a5-48c4-92fb-2ad16e2d8775",
   "metadata": {},
   "source": [
    "#### Pydantic is the ‚Äúdata contract‚Äù, and structured output makes the LLM follow that contract so our code can safely use the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
