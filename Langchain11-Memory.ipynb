{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7d1883-1db4-4411-921a-ed9a4fba7d95",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "## LangChain Memory Types\n",
    "\n",
    "Memory in LangChain allows your applications to maintain context across multiple interactions, making conversations more coherent and natural. Let's explore each memory type in detail:\n",
    "\n",
    "### **ConversationBufferMemory**\n",
    "\n",
    "This is the simplest and most straightforward memory type. It stores the complete conversation history in a buffer, keeping every single message exchanged between the user and the AI.\n",
    "\n",
    "**How it works:** Every message (both user inputs and AI responses) is stored sequentially in memory. When the AI needs to respond, it has access to the entire conversation from the beginning.\n",
    "\n",
    "**Best use cases:** Short conversations where you need complete context, such as customer support chats or brief consultation sessions.\n",
    "\n",
    "**Limitations:** As conversations grow longer, this memory type consumes more tokens and can eventually hit context window limits. It's like keeping every page of notes from a meeting—eventually, you'll run out of space.\n",
    "\n",
    "**Example scenario:** A customer asking about their order status, then asking follow-up questions about shipping. The AI needs to remember the order number mentioned at the start.\n",
    "\n",
    "---\n",
    "\n",
    "### **ConversationBufferWindowMemory**\n",
    "\n",
    "This memory type maintains only the most recent K interactions (where K is a number you specify), creating a \"sliding window\" of conversation history.\n",
    "\n",
    "**How it works:** Instead of keeping everything, it only remembers the last N messages. For example, if you set the window to 5, it will keep the 5 most recent exchanges and forget anything older.\n",
    "\n",
    "**Best use cases:** Longer conversations where only recent context matters, like technical troubleshooting or step-by-step tutorials where earlier steps become irrelevant.\n",
    "\n",
    "**Advantages:** Prevents token count from growing indefinitely while maintaining relevant recent context. It's like taking notes but only keeping the last few pages.\n",
    "\n",
    "**Trade-off:** You lose access to earlier conversation details, which might be important in some scenarios.\n",
    "\n",
    "**Example scenario:** A coding assistant helping debug an issue across multiple files. You care about the current file being discussed, not the one from 20 messages ago.\n",
    "\n",
    "---\n",
    "\n",
    "### **ConversationTokenBufferMemory**\n",
    "\n",
    "Rather than tracking a fixed number of messages, this memory type limits conversation history based on token count—the actual \"size\" of the text.\n",
    "\n",
    "**How it works:** You set a maximum token limit (e.g., 2000 tokens). The memory keeps as many recent messages as possible without exceeding this limit. Since different messages have different lengths, this provides more flexible memory management.\n",
    "\n",
    "**Best use cases:** Applications where you need to precisely control API costs or stay within specific model context limits.\n",
    "\n",
    "**Advantages:** More efficient token usage compared to fixed message windows. A short \"yes\" uses fewer tokens than a paragraph-long explanation, so you can fit more short messages or fewer long ones.\n",
    "\n",
    "**Why it matters:** Different AI models have different context windows and pricing based on tokens. This memory type gives you fine-grained control over resource usage.\n",
    "\n",
    "**Example scenario:** A chatbot that needs to work within a strict budget constraint or support models with smaller context windows.\n",
    "\n",
    "---\n",
    "\n",
    "### **VectorStore Memory**\n",
    "\n",
    "This is the most sophisticated memory type, using semantic similarity rather than recency to retrieve relevant past conversations.\n",
    "\n",
    "**How it works:** Every conversation is converted into embeddings (numerical representations of meaning) and stored in a vector database. When a new query comes in, the system searches for semantically similar past conversations and retrieves the most relevant ones, regardless of when they occurred.\n",
    "\n",
    "**Best use cases:** Long-term memory scenarios where you might need to recall information from days or weeks ago, such as personal assistants, medical history tracking, or customer relationship management.\n",
    "\n",
    "**Advantages:** Can handle virtually unlimited conversation history because it doesn't load everything into context—only what's relevant. It's like having a searchable archive rather than trying to remember everything at once.\n",
    "\n",
    "**Technical note:** Requires a vector database (like Pinecone, Weaviate, or Chroma) and embedding models to function.\n",
    "\n",
    "**Example scenario:** A personal AI assistant remembering that you mentioned your daughter's birthday three weeks ago when you now ask for gift recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Custom Memory**\n",
    "\n",
    "LangChain allows you to build your own memory implementations tailored to specific needs that aren't covered by the built-in types.\n",
    "\n",
    "**When to use:** When you have unique requirements like:\n",
    "- Combining multiple memory types (e.g., recent buffer + long-term vector store)\n",
    "- Implementing business logic for what should be remembered or forgotten\n",
    "- Integrating with existing databases or storage systems\n",
    "- Creating memory that respects data privacy rules or retention policies\n",
    "\n",
    "**Implementation approach:** You inherit from LangChain's base memory classes and override methods to define how data is saved, loaded, and retrieved.\n",
    "\n",
    "**Example scenarios:** \n",
    "- A healthcare chatbot that must forget certain sensitive information after 24 hours for compliance\n",
    "- An e-commerce assistant that prioritizes remembering purchase history over casual conversation\n",
    "- A multi-user system where each user has isolated memory\n",
    "\n",
    "---\n",
    "\n",
    "## ** Where Memory is Required**\n",
    "\n",
    "### **Chatbots**\n",
    "\n",
    "Chatbots absolutely need memory to create natural, human-like conversations. Without memory, every message would be treated as the first message, creating a frustrating and disjointed experience.\n",
    "\n",
    "**Why it's critical:** Users expect chatbots to remember context within a conversation. If a user says \"My name is Sarah\" and then asks a question, they expect the bot to still know their name without being told again.\n",
    "\n",
    "**Real-world impact:** Memory transforms a chatbot from a simple Q&A system into a conversational partner. It enables the bot to:\n",
    "- Maintain topic continuity\n",
    "- Reference earlier points in the discussion\n",
    "- Build upon previous answers\n",
    "- Understand pronouns and references (\"it,\" \"that one,\" \"like I mentioned\")\n",
    "\n",
    "**Example:** A banking chatbot where the user asks about their checking account balance, then asks \"What about savings?\"—the bot needs to remember we're discussing the user's accounts.\n",
    "\n",
    "---\n",
    "\n",
    "### **Assistants**\n",
    "\n",
    "Personal and professional assistants (like virtual secretaries, scheduling assistants, or research helpers) require memory to provide personalized, context-aware assistance over time.\n",
    "\n",
    "**Why it's essential:** Assistants need to learn user preferences, remember past requests, and maintain awareness of ongoing projects or tasks.\n",
    "\n",
    "**Memory requirements:**\n",
    "- **Short-term:** Remember the current task or conversation goal\n",
    "- **Long-term:** Recall user preferences, past decisions, and historical interactions\n",
    "\n",
    "**Value proposition:** Memory enables assistants to become more helpful over time, reducing the need for users to repeatedly provide the same information.\n",
    "\n",
    "**Example:** A research assistant that remembers you're writing a paper on climate change, so when you ask \"Find more sources,\" it knows what topic you mean and can even reference sources it already found for you.\n",
    "\n",
    "---\n",
    "\n",
    "### **Agents with Long Workflows**\n",
    "\n",
    "AI agents that perform multi-step tasks or workflows require memory to track progress, maintain state, and coordinate complex operations.\n",
    "\n",
    "**Why it's necessary:** Long workflows often involve multiple steps where the output of one step becomes the input for the next. Without memory, the agent would lose track of what it's doing.\n",
    "\n",
    "**Challenges addressed:**\n",
    "- **State management:** Keeping track of which steps are completed\n",
    "- **Data passing:** Carrying information from one step to another\n",
    "- **Error recovery:** Remembering context if something fails and needs to be retried\n",
    "- **Decision making:** Using results from earlier steps to inform later choices\n",
    "\n",
    "**Example scenarios:**\n",
    "- A data analysis agent that: (1) loads data, (2) cleans it, (3) performs analysis, (4) generates visualizations, (5) writes a report\n",
    "- A customer onboarding agent that guides users through multiple forms and verification steps over several sessions\n",
    "- A code generation agent that needs to remember the project structure, dependencies, and previous code it generated\n",
    "\n",
    "**Technical consideration:** For very long workflows, you might combine memory types—using buffer memory for immediate task context and vector memory for accessing relevant information from earlier in the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaway**\n",
    "\n",
    "Memory isn't just a nice-to-have feature—it's fundamental to creating AI applications that feel intelligent and contextually aware. The choice of memory type depends on your specific use case: conversation length, token budget, need for long-term recall, and the importance of semantic versus chronological relevance. Understanding these trade-offs helps you build more efficient and effective LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85f3c6-1c15-4e03-8700-1e978a7b3fd2",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23331a0e-38ac-43c5-b43f-6d46a3e3e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found in env!\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful tutor. Answer clearly.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# In-memory store of sessions\n",
    "_store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in _store:\n",
    "        _store[session_id] = ChatMessageHistory()\n",
    "    return _store[session_id]\n",
    "\n",
    "chat = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Helper to ask in one line\n",
    "def ask(text, session_id=\"s1\"):\n",
    "    return chat.invoke({\"input\": text}, config={\"configurable\": {\"session_id\": session_id}}).content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9a76a-9aec-4fc5-b101-f566d5332ec0",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory (Full history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e94b7f-aeb1-41cf-89e8-fbc8f496ddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Keerthi! How can I assist you today?\n",
      "Your name is Keerthi. How can I help you further?\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"My name is Keerthi.\"))\n",
    "print(ask(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e00f3-6773-4602-a435-67a4545fb661",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory (Last K messages)\n",
    "\n",
    "Meaning: keep only the last K turns (sliding window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71faf896-1b0f-4c0e-9d49-7e13b42bdfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got it! Cotton kurtis are a great choice for comfort and style. They come in various designs, colors, and patterns, making them versatile for different occasions. Do you have any specific styles or colors in mind that you like?\n",
      "Got it! You're in Bengaluru. If you're looking for cotton kurtis, there are plenty of great local shops and markets in the city, as well as online options. Would you like recommendations for places to shop or tips on styling your kurtis?\n",
      "Got it! Your favorite color is maroon. Maroon cotton kurtis can look stunning and are perfect for various occasions. If you need suggestions for styles, patterns, or where to find maroon kurtis in Bengaluru, just let me know!\n",
      "I remember that you are in Bengaluru and that your favorite color is maroon. If there's anything else you'd like me to remember or if you have any specific questions or topics you'd like to discuss, feel free to let me know!\n",
      "\n",
      "Stored messages count: 4\n",
      "HumanMessage : Remember: My favorite color is maroon.\n",
      "AIMessage : Got it! Your favorite color is maroon. Maroon cotton kurtis can look stunning and are perfect for various occasions. If you need suggestions for styles, patterns, or where to find maroon kurtis in Bengaluru, just let me know!\n",
      "HumanMessage : What do you remember about me?\n",
      "AIMessage : I remember that you are in Bengaluru and that your favorite color is maroon. If there's anything else you'd like me to remember or if you have any specific questions or topics you'd like to discuss, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 1) LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 2) Prompt that expects \"history\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# 3) Store histories per session\n",
    "_store_window = {}\n",
    "\n",
    "# 4) Helper: trim last k turns (k human+ai pairs = 2k messages)\n",
    "def trim_history(history: InMemoryChatMessageHistory, k_turns: int):\n",
    "    max_msgs = 2 * k_turns\n",
    "    if len(history.messages) > max_msgs:\n",
    "        history.messages = history.messages[-max_msgs:]\n",
    "\n",
    "def get_window_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in _store_window:\n",
    "        _store_window[session_id] = InMemoryChatMessageHistory()\n",
    "    return _store_window[session_id]\n",
    "\n",
    "# 5) Wrap chain with message history\n",
    "chat_window = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_window_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 6) Ask function with window trimming\n",
    "def ask_window(text, session_id=\"w1\", k_turns=2):\n",
    "    result = chat_window.invoke(\n",
    "        {\"input\": text},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    # trim AFTER adding new messages\n",
    "    trim_history(get_window_history(session_id), k_turns=k_turns)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "# Demo\n",
    "print(ask_window(\"Remember: I like cotton kurtis.\", \"w1\", k_turns=2))\n",
    "print(ask_window(\"Remember: My city is Bengaluru.\", \"w1\", k_turns=2))\n",
    "print(ask_window(\"Remember: My favorite color is maroon.\", \"w1\", k_turns=2))\n",
    "\n",
    "# Because k_turns=2, only the last 2 turns remain strongly in memory\n",
    "print(ask_window(\"What do you remember about me?\", \"w1\", k_turns=2))\n",
    "\n",
    "# Inspect what's stored\n",
    "print(\"\\nStored messages count:\", len(get_window_history(\"w1\").messages))\n",
    "for m in get_window_history(\"w1\").messages:\n",
    "    print(type(m).__name__, \":\", m.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1a473-bfc8-449d-b253-f35744c22294",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory (Keep history until token budget)\n",
    "\n",
    "Meaning: keep messages until a token limit is hit; then drop older ones.\n",
    "\n",
    "Here’s a simple token-based truncation using tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b82a28-2f1e-4615-9943-06b8a368c82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! \"Myntra\" is a catchy name for a boutique, especially for selling kurtis. Here are a few ideas to help you promote your boutique and attract customers:\n",
      "\n",
      "1. **Brand Identity**: Create a unique logo and color scheme that reflects the essence of your boutique. Consider using traditional motifs or modern designs that resonate with your target audience.\n",
      "\n",
      "2. **Social Media Presence**: Utilize platforms like Instagram and Facebook to showcase your kurtis. Post high-quality images, styling tips, and customer testimonials. Engage with your audience through stories and live sessions.\n",
      "\n",
      "3. **Website**: If you don’t have one already, consider creating a website where customers can browse your collection, learn about your brand, and make purchases online.\n",
      "\n",
      "4. **Local Events**: Participate in local fashion shows, fairs, or markets to showcase your kurtis. This can help you reach a wider audience and build a community around your brand.\n",
      "\n",
      "5. **Collaborations**: Partner with local influencers or fashion bloggers to promote your kurtis. They can help you reach a larger audience and provide credibility to your brand.\n",
      "\n",
      "6. **Customer Loyalty Program**: Consider implementing a loyalty program where customers can earn points or discounts for repeat purchases. This encourages customer retention.\n",
      "\n",
      "7. **Seasonal Collections**: Launch seasonal or festive collections to keep your offerings fresh and relevant. Promote these collections through themed marketing campaigns.\n",
      "\n",
      "8. **Styling Tips**: Share styling tips on how to wear kurtis for different occasions. This can help customers envision how they can incorporate your products into their wardrobes.\n",
      "\n",
      "9. **Feedback and Reviews**: Encourage customers to leave reviews and feedback. Positive testimonials can greatly influence potential buyers.\n",
      "\n",
      "10. **Email Marketing**: Build an email list and send out newsletters with updates on new arrivals, special promotions, and styling tips.\n",
      "\n",
      "If you need more specific ideas or assistance with any of these suggestions, feel free to ask!\n",
      "That sounds great! Selling cotton, muslin, and party wear designs offers a diverse range of products that can appeal to various customer preferences. Here are some ideas to help you promote and enhance your offerings:\n",
      "\n",
      "### Marketing Strategies\n",
      "1. **Social Media Presence**: Utilize platforms like Instagram, Facebook, and Pinterest to showcase your designs. High-quality images and engaging content can attract potential customers.\n",
      "\n",
      "2. **Influencer Collaborations**: Partner with fashion influencers who align with your brand to reach a wider audience. They can showcase your products in their posts and stories.\n",
      "\n",
      "3. **Seasonal Promotions**: Create special promotions around holidays or events (e.g., weddings, festivals) to encourage purchases of party wear.\n",
      "\n",
      "4. **Content Marketing**: Start a blog or video series about fashion tips, styling ideas, and the benefits of cotton and muslin fabrics. This can help establish your brand as an authority in the fashion space.\n",
      "\n",
      "5. **Email Marketing**: Build an email list and send newsletters featuring new arrivals, styling tips, and exclusive discounts.\n",
      "\n",
      "### Product Development\n",
      "1. **Custom Designs**: Offer customization options for party wear, allowing customers to choose colors, patterns, or styles.\n",
      "\n",
      "2. **Sustainable Practices**: Highlight any sustainable practices in sourcing or production, as many consumers are increasingly interested in eco-friendly fashion.\n",
      "\n",
      "3. **Mix and Match**: Create collections that allow customers to mix and match cotton and muslin pieces with party wear for versatile styling options.\n",
      "\n",
      "### Customer Engagement\n",
      "1. **Loyalty Programs**: Implement a loyalty program to reward repeat customers with discounts or exclusive access to new collections.\n",
      "\n",
      "2. **Customer Feedback**: Encourage customers to leave reviews and share photos of themselves wearing your designs. This can build community and trust.\n",
      "\n",
      "3. **Pop-Up Shops**: Consider hosting pop-up shops or participating in local markets to reach customers in person and showcase your products.\n",
      "\n",
      "### Visual Merchandising\n",
      "1. **Lookbooks**: Create seasonal lookbooks that highlight your designs in styled outfits, providing inspiration for customers.\n",
      "\n",
      "2. **Themed Displays**: If you have a physical store, create themed displays that highlight your cotton, muslin, and party wear collections together.\n",
      "\n",
      "By combining these strategies, you can effectively promote your cotton, muslin, and party wear designs, attract new customers, and build a loyal customer base. If you have any specific questions or need further assistance, feel free to ask!\n",
      "It seems like you're looking for information about products or services within a price range of $1000 to $2000, possibly with weekend discounts. Could you please provide more details about what specific items or services you are interested in? For example, are you looking for electronics, furniture, travel packages, or something else? This will help me assist you better!\n",
      "It seems I don't have any prior information about your boutique. However, if you provide me with some details about it—such as the types of products you sell, your target audience, your location, and any unique features or services you offer—I can help you summarize that information effectively!\n",
      "\n",
      "Stored messages: 2\n",
      "Token count: 68\n",
      "HumanMessage : Summarize what you know about my boutique.\n",
      "AIMessage : It seems I don't have any prior information about your boutique. However, if you provide me with some details about it—such as the types of products you sell, your target audience, your location, and any unique features or services you offer—I can help you summarize that information effectively!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# 1) LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 2) Prompt expects \"history\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# 3) Store per session\n",
    "_store_token = {}\n",
    "\n",
    "def get_token_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in _store_token:\n",
    "        _store_token[session_id] = InMemoryChatMessageHistory()\n",
    "    return _store_token[session_id]\n",
    "\n",
    "chat_token = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_token_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 4) Token counting + trimming\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # good default for OpenAI-style tokenization\n",
    "\n",
    "def count_tokens_in_history(history: InMemoryChatMessageHistory) -> int:\n",
    "    # Very simple token estimate: sum tokens of message content\n",
    "    return sum(len(enc.encode(m.content)) for m in history.messages)\n",
    "\n",
    "def trim_to_token_budget(history: InMemoryChatMessageHistory, max_tokens: int):\n",
    "    # Keep removing oldest messages until within budget\n",
    "    while history.messages and count_tokens_in_history(history) > max_tokens:\n",
    "        history.messages.pop(0)\n",
    "\n",
    "# 5) Ask function\n",
    "def ask_token(text, session_id=\"t1\", max_tokens=120):\n",
    "    result = chat_token.invoke(\n",
    "        {\"input\": text},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    # trim AFTER messages were added\n",
    "    hist = get_token_history(session_id)\n",
    "    trim_to_token_budget(hist, max_tokens=max_tokens)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "# Demo (small budget so students can see truncation happening)\n",
    "print(ask_token(\"My boutique name is Myntra and I sell kurtis.\", \"t1\", max_tokens=120))\n",
    "print(ask_token(\"We sell cotton, muslin, and party wear designs.\", \"t1\", max_tokens=120))\n",
    "print(ask_token(\"Price range is 1000 to 2000 and weekend discounts.\", \"t1\", max_tokens=120))\n",
    "\n",
    "print(ask_token(\"Summarize what you know about my boutique.\", \"t1\", max_tokens=120))\n",
    "\n",
    "# Inspect stored history\n",
    "hist = get_token_history(\"t1\")\n",
    "print(\"\\nStored messages:\", len(hist.messages))\n",
    "print(\"Token count:\", count_tokens_in_history(hist))\n",
    "for m in hist.messages:\n",
    "    print(type(m).__name__, \":\", m.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1aa7a7-98a8-4013-89a7-34eff23f5f38",
   "metadata": {},
   "source": [
    "## VectorStore Memory (Semantic long-term memory)\n",
    "\n",
    "Instead of sending full chat history, it stores messages as embeddings and retrieves only the most relevant past info. This is commonly done via VectorStoreRetrieverMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cb1c26a-6850-47d4-ba01-36e8f8688de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your boutique, Myntra, is located in Nagarabhavi, Bengaluru. It specializes in cotton kurtis, sarees, and muslin dress sets. If you have any specific customer preferences or additional details you'd like to share, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found in env!\"\n",
    "\n",
    "# 1) LLM + Embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 2) Create a Vector DB (FAISS) to store \"memories\"\n",
    "vectorstore = FAISS.from_documents(\n",
    "    [Document(page_content=\"(init)\")],\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3) Functions: save memory + recall memory\n",
    "def save_memory(text: str):\n",
    "    vectorstore.add_documents([Document(page_content=text)])\n",
    "\n",
    "def recall_memory(query: str) -> str:\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# 4) Prompt that uses recalled memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the MEMORY section to answer.\"),\n",
    "    (\"human\", \"MEMORY:\\n{memory}\\n\\nQUESTION:\\n{question}\")\n",
    "])\n",
    "\n",
    "# 5) Store some \"memories\"\n",
    "save_memory(\"Boutique name is Myntra.\")\n",
    "save_memory(\"Boutique is in Nagarabhavi, Bengaluru.\")\n",
    "save_memory(\"Main products: cotton kurtis, sarees, and muslin dress sets.\")\n",
    "save_memory(\"Customer budget range is 1000 to 2000 INR.\")\n",
    "\n",
    "# 6) Ask question: retrieve relevant memory, then answer with LLM\n",
    "question = \"What do you remember about my boutique and customer preferences?\"\n",
    "memory_text = recall_memory(question)\n",
    "\n",
    "response = (prompt | llm).invoke({\"memory\": memory_text, \"question\": question})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2883fc3-4ac5-4cbb-ab93-60dd0780b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy as np\n",
    "print(sys.version)\n",
    "print(np.__version__)\n",
    "\n",
    "# pip uninstall -y faiss faiss-cpu\n",
    "# conda install -c conda-forge faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea30219-6cc6-4f11-8021-edbc24d29a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2fdf22f-f0c3-4120-880c-72d2b4350def",
   "metadata": {},
   "source": [
    "## Custom Memory (you decide what to store + how to inject it)\n",
    "\n",
    "Example: store only user profile fields (name, city, preference) and inject them into the prompt every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdaf3197-cf19-4eb2-8638-5cf1fbdcac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Keerthi! How can I assist you today?\n",
      "Bengaluru is a vibrant city with a lot to offer! Do you have any specific interests or topics you'd like to discuss related to Bengaluru?\n",
      "Cotton kurtis are a great choice, especially for the warm climate in Bengaluru! Do you have a favorite style or color when it comes to cotton kurtis? Or are you looking for recommendations on where to shop for them?\n",
      "Based on your preference for cotton kurtis, here are three suggestions you might like:\n",
      "\n",
      "1. **Floral Printed Cotton Kurti**: A knee-length kurti with vibrant floral prints can be perfect for casual outings or even office wear. Look for one with a comfortable fit and breathable fabric.\n",
      "\n",
      "2. **Anarkali Style Cotton Kurti**: An Anarkali kurti with a flared bottom can be a stylish choice for festive occasions. Opt for one with intricate embroidery or mirror work for a more traditional look.\n",
      "\n",
      "3. **Straight Cut Cotton Kurti with Side Slits**: A straight-cut kurti with side slits can be both chic and comfortable. Pair it with palazzos or leggings for a complete look. Choose a solid color or subtle patterns for versatility.\n",
      "\n",
      "Would you like more details on where to find these styles in Bengaluru?\n",
      "\n",
      "PROFILE STORE: {'name': 'Keerthi', 'city': 'Bengaluru', 'preference': 'cotton kurtis'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---- Custom memory store (structured) ----\n",
    "profile_store = {}\n",
    "\n",
    "def update_profile(session_id: str, user_text: str):\n",
    "    p = profile_store.get(session_id, {\"name\": None, \"city\": None, \"preference\": None})\n",
    "\n",
    "    t = user_text.lower()\n",
    "    if \"my name is\" in t:\n",
    "        p[\"name\"] = user_text.split(\"is\", 1)[-1].strip().strip(\".\")\n",
    "    if \"i live in\" in t:\n",
    "        p[\"city\"] = user_text.split(\"in\", 1)[-1].strip().strip(\".\")\n",
    "    if \"i like\" in t:\n",
    "        p[\"preference\"] = user_text.split(\"like\", 1)[-1].strip().strip(\".\")\n",
    "\n",
    "    profile_store[session_id] = p\n",
    "\n",
    "# Chat prompt that injects profile + chat history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use PROFILE + chat HISTORY.\"),\n",
    "    (\"system\", \"PROFILE: {profile}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "base_chain = prompt | llm\n",
    "\n",
    "# store chat history (buffer memory)\n",
    "history_store = {}\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in history_store:\n",
    "        history_store[session_id] = ChatMessageHistory()\n",
    "    return history_store[session_id]\n",
    "\n",
    "chat = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "def ask(text: str, session_id=\"s1\"):\n",
    "    update_profile(session_id, text)  # <-- custom memory update\n",
    "    profile = profile_store.get(session_id, {})\n",
    "    resp = chat.invoke(\n",
    "        {\"input\": text, \"profile\": profile},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return resp.content\n",
    "\n",
    "print(ask(\"My name is Keerthi.\", \"s1\"))\n",
    "print(ask(\"I live in Bengaluru.\", \"s1\"))\n",
    "print(ask(\"I like cotton kurtis.\", \"s1\"))\n",
    "print(ask(\"Suggest 3 items for me based on my profile.\", \"s1\"))\n",
    "\n",
    "print(\"\\nPROFILE STORE:\", profile_store[\"s1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee20106-460e-4433-928c-17867a9494aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
