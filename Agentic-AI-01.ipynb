{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6085e8bd-12c8-46e8-8fc7-c48909709ab2",
   "metadata": {},
   "source": [
    "\n",
    "## 1. What is an AI Agent?\n",
    "\n",
    "### Core Definition\n",
    "An **AI agent** is an autonomous system that uses a language model (LM) to iteratively reason about problems, make decisions, and take actions in pursuit of a goal. Unlike simple chatbots that respond to single queries, agents can:\n",
    "\n",
    "- Break down complex tasks into subtasks\n",
    "- Make multi-step plans\n",
    "- Use external tools and APIs\n",
    "- Gather information dynamically\n",
    "- Adapt based on feedback\n",
    "- Operate with varying degrees of autonomy\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "**Autonomy**: Agents can operate independently without constant human intervention, making decisions based on their observations and goals.\n",
    "\n",
    "**Reactivity**: They perceive their environment (through APIs, databases, search results, etc.) and respond to changes in it.\n",
    "\n",
    "**Proactivity**: Agents take initiative to achieve goals, not just responding to immediate stimuli.\n",
    "\n",
    "**Goal-oriented behavior**: All actions are directed toward achieving specific objectives, whether explicit (user-defined) or implicit (system-designed).\n",
    "\n",
    "### Agent vs. Traditional LLM Applications\n",
    "\n",
    "| Traditional LLM | AI Agent |\n",
    "|----------------|----------|\n",
    "| Single-turn responses | Multi-turn interactions with memory |\n",
    "| Static context | Dynamic environment interaction |\n",
    "| No external actions | Tool usage and API calls |\n",
    "| Passive information retrieval | Active problem-solving |\n",
    "| Deterministic flow | Adaptive decision-making |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Reasoning vs. Action\n",
    "\n",
    "This is a fundamental terminolgy in agent design that addresses **when to think** versus **when to act**.\n",
    "\n",
    "### Reasoning (Thought)\n",
    "\n",
    "**Definition**: Internal cognitive processes where the agent analyzes information, plans, and deliberates without taking external actions.\n",
    "\n",
    "**Characteristics**:\n",
    "- Happens \"in the agent's head\" (within the LM's context)\n",
    "- No side effects on the environment\n",
    "- Can involve chain-of-thought (CoT), planning, reflection\n",
    "- Consumes tokens but doesn't execute actions\n",
    "\n",
    "**Examples of reasoning**:\n",
    "```\n",
    "\"Let me think about this step by step...\"\n",
    "\"To solve this, I need to first understand X, then Y...\"\n",
    "\"The user wants A, but to get A, I need to gather B and C first...\"\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Reduces expensive/irreversible actions\n",
    "- Enables planning and foresight\n",
    "- Allows error detection before execution\n",
    "- Supports complex problem decomposition\n",
    "\n",
    "**Disadvantages**:\n",
    "- Can lead to \"analysis paralysis\"\n",
    "- May hallucinate about environment state without verification\n",
    "- Consumes context window without gathering real information\n",
    "\n",
    "### Action (Execution)\n",
    "\n",
    "**Definition**: External operations that interact with and modify the environment or gather real-world information.\n",
    "\n",
    "**Characteristics**:\n",
    "- Observable effects outside the agent\n",
    "- Tool calls, API requests, database queries\n",
    "- Can have side effects (writing files, sending emails)\n",
    "- Provides ground truth information\n",
    "\n",
    "**Examples of actions**:\n",
    "```python\n",
    "# Tool calls\n",
    "search_web(query=\"current weather in Tokyo\")\n",
    "execute_code(code=\"import pandas as pd...\")\n",
    "send_email(to=\"user@example.com\", subject=\"Report\")\n",
    "write_file(path=\"/data/output.json\", content=\"{...}\")\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Grounds reasoning in reality\n",
    "- Enables verification of hypotheses\n",
    "- Necessary for task completion\n",
    "- Provides real-time information\n",
    "\n",
    "**Disadvantages**:\n",
    "- Can be expensive (API costs, latency)\n",
    "- May have irreversible consequences\n",
    "- Requires careful error handling\n",
    "- Potential for unintended side effects\n",
    "\n",
    "### The Balance: Interleaving Reasoning and Action\n",
    "\n",
    "Modern agents don't choose one or the other—they **interleave** reasoning and action in a loop:\n",
    "\n",
    "1. **Observe** the current state\n",
    "2. **Reason** about what to do next\n",
    "3. **Act** (if necessary) to gather information or make progress\n",
    "4. **Observe** the result of the action\n",
    "5. Repeat until goal is achieved\n",
    "\n",
    "This creates a **perception-cognition-action cycle** similar to human problem-solving.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ReAct: Reasoning + Acting Paradigm\n",
    "\n",
    "### What is ReAct?\n",
    "\n",
    "**ReAct** (Reasoning and Acting) is a seminal framework introduced by Yao et al. (2023) that synergizes reasoning traces and task-specific actions in language models. It's one of the most influential agent architectures.\n",
    "\n",
    "**Paper**: \"ReAct: Synergizing Reasoning and Acting in Language Models\" (ICLR 2023)\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "ReAct prompts the LM to generate both:\n",
    "- **Reasoning traces** (Thought): Internal dialogue explaining the agent's thinking\n",
    "- **Actions**: Specific tool calls or operations to execute\n",
    "\n",
    "These are interleaved in a structured format, creating a reasoning-action-observation loop.\n",
    "\n",
    "### ReAct Structure\n",
    "\n",
    "```\n",
    "Question: [User's question]\n",
    "\n",
    "Thought 1: [Agent's reasoning about what to do first]\n",
    "Action 1: [Specific action to take]\n",
    "Observation 1: [Result of Action 1]\n",
    "\n",
    "Thought 2: [Agent's reasoning given the observation]\n",
    "Action 2: [Next action based on new information]\n",
    "Observation 2: [Result of Action 2]\n",
    "\n",
    "...\n",
    "\n",
    "Thought N: [Final reasoning]\n",
    "Answer: [Final answer to the question]\n",
    "```\n",
    "\n",
    "### Example: ReAct in Action\n",
    "\n",
    "**Question**: What is the elevation of the highest peak in the state where the University of Washington is located?\n",
    "\n",
    "```\n",
    "Thought 1: I need to find out what state the University of Washington is in.\n",
    "Action 1: Search[University of Washington location]\n",
    "Observation 1: The University of Washington is located in Seattle, Washington.\n",
    "\n",
    "Thought 2: Now I know it's in Washington state. I need to find the highest peak in Washington.\n",
    "Action 2: Search[highest peak in Washington state]\n",
    "Observation 2: The highest peak in Washington state is Mount Rainier.\n",
    "\n",
    "Thought 3: Now I need to find the elevation of Mount Rainier.\n",
    "Action 3: Search[Mount Rainier elevation]\n",
    "Observation 3: Mount Rainier has an elevation of 14,411 feet (4,392 meters).\n",
    "\n",
    "Thought 4: I have all the information needed to answer the question.\n",
    "Answer: The elevation of the highest peak in Washington state (Mount Rainier) is 14,411 feet or 4,392 meters.\n",
    "```\n",
    "\n",
    "### Why ReAct Works\n",
    "\n",
    "**Explicit reasoning improves action selection**: By forcing the model to articulate its thinking, it makes better decisions about which tools to use and when.\n",
    "\n",
    "**Actions ground reasoning in reality**: Instead of hallucinating facts, the agent retrieves real information, reducing errors.\n",
    "\n",
    "**Interpretability**: The thought traces make the agent's decision-making process transparent and debuggable.\n",
    "\n",
    "**Error recovery**: When an action fails or returns unexpected results, the reasoning step helps the agent adapt and try alternative approaches.\n",
    "\n",
    "**Human-like problem solving**: Mirrors how humans think-aloud while solving problems, alternating between planning and execution.\n",
    "\n",
    "### ReAct Variants and Extensions\n",
    "\n",
    "**ReAct + Self-Reflection**: Agent reviews its own reasoning and actions to identify and correct mistakes.\n",
    "\n",
    "**ReAct + Memory**: Stores past reasoning-action trajectories to learn from experience.\n",
    "\n",
    "**Reflexion**: Extends ReAct with explicit self-reflection and learning from failures.\n",
    "\n",
    "**Tree of Thoughts (ToT)**: Explores multiple reasoning paths simultaneously, using search algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tool-Calling Behavior\n",
    "\n",
    "### What is Tool Calling?\n",
    "\n",
    "**Tool calling** (also called function calling) is the mechanism by which an LLM invokes external functions, APIs, or capabilities beyond text generation. It's the primary way agents interact with the outside world.\n",
    "\n",
    "### How Tool Calling Works\n",
    "\n",
    "#### 1. Tool Definition\n",
    "\n",
    "Tools are defined with schemas that specify:\n",
    "- **Name**: Identifier for the tool\n",
    "- **Description**: What the tool does (critical for selection)\n",
    "- **Parameters**: Expected inputs with types and descriptions\n",
    "- **Return type**: What the tool outputs\n",
    "\n",
    "**Example Tool Schema**:\n",
    "```json\n",
    "{\n",
    "  \"name\": \"web_search\",\n",
    "  \"description\": \"Search the web for current information on a topic\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"query\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The search query to execute\"\n",
    "      },\n",
    "      \"num_results\": {\n",
    "        \"type\": \"integer\",\n",
    "        \"description\": \"Number of results to return (default: 5)\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"query\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. Tool Selection\n",
    "\n",
    "The LM analyzes the user's request and available tools to determine:\n",
    "- **Which tool(s)** to use\n",
    "- **What parameters** to pass\n",
    "- **In what order** to call them (if multiple tools needed)\n",
    "\n",
    "This happens through few-shot prompting or fine-tuning.\n",
    "\n",
    "#### 3. Tool Execution\n",
    "\n",
    "The agent outputs a structured tool call (often JSON), which the orchestration layer:\n",
    "- Parses and validates\n",
    "- Executes against the actual tool/API\n",
    "- Captures the result\n",
    "\n",
    "#### 4. Result Integration\n",
    "\n",
    "The tool's output is fed back to the LM as an **observation**, which then:\n",
    "- Reasons about the result\n",
    "- Decides next actions\n",
    "- Continues until task completion\n",
    "\n",
    "### Tool Calling Formats\n",
    "\n",
    "Different platforms use different formats:\n",
    "\n",
    "**OpenAI Function Calling**:\n",
    "```json\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": null,\n",
    "  \"function_call\": {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"arguments\": \"{\\\"location\\\": \\\"Tokyo\\\", \\\"unit\\\": \\\"celsius\\\"}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Anthropic Tool Use**:\n",
    "```json\n",
    "{\n",
    "  \"type\": \"tool_use\",\n",
    "  \"id\": \"tool_abc123\",\n",
    "  \"name\": \"get_weather\",\n",
    "  \"input\": {\n",
    "    \"location\": \"Tokyo\",\n",
    "    \"unit\": \"celsius\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Generic Text-Based**:\n",
    "```\n",
    "Action: web_search\n",
    "Action Input: {\"query\": \"Tokyo weather today\"}\n",
    "```\n",
    "\n",
    "### Types of Tools\n",
    "\n",
    "**Information Retrieval**:\n",
    "- Web search (Google, Bing)\n",
    "- Database queries (SQL, vector databases)\n",
    "- Document retrieval (RAG systems)\n",
    "- API data fetching\n",
    "\n",
    "**Computation**:\n",
    "- Code execution (Python, JavaScript)\n",
    "- Mathematical calculations\n",
    "- Data analysis and visualization\n",
    "- Model inference (calling other ML models)\n",
    "\n",
    "**External Actions**:\n",
    "- File system operations (read/write)\n",
    "- Email sending\n",
    "- Calendar management\n",
    "- API integrations (Slack, GitHub, etc.)\n",
    "\n",
    "**Specialized Domain Tools**:\n",
    "- Medical databases\n",
    "- Financial market data\n",
    "- Scientific computation libraries\n",
    "- Geographic information systems\n",
    "\n",
    "### Tool Calling Challenges\n",
    "\n",
    "**Tool Selection Errors**: The LM might choose the wrong tool for a task, especially when multiple similar tools exist.\n",
    "\n",
    "**Parameter Hallucination**: The model might generate plausible-looking but incorrect parameters (e.g., non-existent API endpoints).\n",
    "\n",
    "**Error Handling**: Tools can fail due to network issues, rate limits, invalid inputs, or missing permissions. Agents must gracefully handle these.\n",
    "\n",
    "**Context Window Limitations**: Each tool call and result consumes tokens. Long agent runs can exceed context limits.\n",
    "\n",
    "**Execution Safety**: Some tools (like code execution or file deletion) can have dangerous side effects if misused.\n",
    "\n",
    "### Best Practices for Tool Calling\n",
    "\n",
    "**Clear, Specific Descriptions**: Tool descriptions should be detailed enough for the LM to understand exactly when and how to use them.\n",
    "\n",
    "**Robust Parameter Validation**: Always validate tool inputs before execution to prevent errors or security issues.\n",
    "\n",
    "**Meaningful Error Messages**: When tools fail, return informative error messages that help the agent recover.\n",
    "\n",
    "**Rate Limiting and Quotas**: Implement safeguards to prevent excessive tool usage.\n",
    "\n",
    "**Observability**: Log all tool calls and results for debugging and monitoring.\n",
    "\n",
    "**Graceful Degradation**: Design agents to continue functioning even when some tools are unavailable.\n",
    "\n",
    "**Security Sandboxing**: Isolate tool execution environments to prevent unauthorized access or damage.\n",
    "\n",
    "### Advanced Tool Calling Patterns\n",
    "\n",
    "**Parallel Tool Calling**: Execute multiple independent tool calls simultaneously to reduce latency.\n",
    "\n",
    "**Conditional Tool Chains**: Tool B is called only if Tool A returns specific results.\n",
    "\n",
    "**Tool Composition**: Combining outputs from multiple tools to achieve complex goals.\n",
    "\n",
    "**Agentic Tool Creation**: Meta-agents that can define and register new tools dynamically.\n",
    "\n",
    "**Human-in-the-Loop**: Critical tools that require human approval before execution.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Putting It All Together: Agent Architectures\n",
    "\n",
    "### Simple Reflex Agent\n",
    "- No reasoning, direct stimulus-response\n",
    "- Fast but inflexible\n",
    "- Example: Rule-based chatbot\n",
    "\n",
    "### Model-Based Reflex Agent\n",
    "- Maintains internal state/memory\n",
    "- Still reactive, but context-aware\n",
    "- Example: Stateful conversation bot\n",
    "\n",
    "### Goal-Based Agent (ReAct Pattern)\n",
    "- Explicit goals and planning\n",
    "- Reasoning + action loops\n",
    "- Example: Research assistant, task automation\n",
    "\n",
    "### Utility-Based Agent\n",
    "- Optimizes for specific metrics\n",
    "- Evaluates action quality\n",
    "- Example: Recommendation systems, trading bots\n",
    "\n",
    "### Learning Agent\n",
    "- Improves over time from experience\n",
    "- Self-reflection and adaptation\n",
    "- Example: Reflexion, reinforcement learning agents\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Takeaways for Interviews\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **Define agents clearly**: Autonomous, goal-oriented, use reasoning + actions\n",
    "- **Explain the reasoning-action dichotomy**: Why both are necessary\n",
    "- **Describe ReAct's innovation**: Interleaving thoughts and actions improves performance\n",
    "- **Understand tool calling mechanics**: Schema → Selection → Execution → Integration\n",
    "\n",
    "### Practical Skills\n",
    "- **Implement a basic ReAct loop** in code\n",
    "- **Design tool schemas** that are clear and unambiguous\n",
    "- **Handle tool errors** gracefully with retry logic\n",
    "- **Optimize for context efficiency** (minimize unnecessary reasoning/actions)\n",
    "\n",
    "### Advanced Topics\n",
    "- **Multi-agent systems**: Agents collaborating or competing\n",
    "- **Agent evaluation**: How to measure success (task completion, efficiency, correctness)\n",
    "- **Safety and alignment**: Preventing harmful actions, ensuring goal alignment\n",
    "- **Scalability challenges**: Managing long conversations, expensive tools\n",
    "\n",
    "### Common Interview Questions\n",
    "\n",
    "1. **\"How would you build an agent to book a restaurant reservation?\"**\n",
    "   - Define goal, identify needed tools (search, availability check, booking API)\n",
    "   - Design ReAct loop with error handling\n",
    "   - Discuss edge cases (no availability, ambiguous requests)\n",
    "\n",
    "2. **\"What's the difference between RAG and an agent?\"**\n",
    "   - RAG: Retrieval augmentation for single-turn QA\n",
    "   - Agent: Multi-turn, autonomous, can use RAG as one tool among many\n",
    "\n",
    "3. **\"How do you prevent an agent from getting stuck in loops?\"**\n",
    "   - Max iteration limits\n",
    "   - Detect repeated actions/states\n",
    "   - Explicit termination conditions\n",
    "\n",
    "4. **\"What's the biggest challenge in agent systems?\"**\n",
    "   - Reliability (hallucinations, tool errors)\n",
    "   - Cost (many LLM calls + tool usage)\n",
    "   - Evaluation (measuring true task success)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908238fc-aaf2-4a8b-a203-7b4c1745316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43ebe9e-55e7-4986-b968-cce0155aacf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing action: RETRY\n"
     ]
    }
   ],
   "source": [
    "# Reasoning vs Action using LangChain\n",
    "# pip install -U langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---- Reasoning step ----\n",
    "reason_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an agent.\n",
    "Observation: {observation}\n",
    "\n",
    "Decide the next action ONLY from:\n",
    "- RETRY\n",
    "- ESCALATE\n",
    "- IGNORE\n",
    "\"\"\")\n",
    "\n",
    "reason_chain = reason_prompt | llm\n",
    "\n",
    "# ---- Action step ----\n",
    "def take_action(action: str):\n",
    "    return f\"Executing action: {action}\"\n",
    "\n",
    "action_chain = RunnableLambda(take_action)\n",
    "\n",
    "# ---- Agent loop ----\n",
    "def agent(observation: str):\n",
    "    decision = reason_chain.invoke({\"observation\": observation}).content.strip()\n",
    "    return action_chain.invoke(decision)\n",
    "\n",
    "print(agent(\"Build failed due to timeout\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70127955-c969-4807-98ee-63f8a89d1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing action: IGNORE\n"
     ]
    }
   ],
   "source": [
    "print(agent(\"Build completed successfully\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffacf3e-a281-4aac-bb9f-1444d57f187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The error \"element click intercepted\" typically indicates that the element intended for interaction is obscured by another element or is not in a clickable state. I need to investigate the UI to determine what might be blocking the click and ensure the element is interactable.\n",
      "\n",
      "Action: Check the visibility and position of the element in the UI, and if necessary, wait for any overlays or animations to complete before attempting the click again. If an overlay is present, I will close it or wait for it to disappear.\n"
     ]
    }
   ],
   "source": [
    "# ReAct style demo\n",
    "# pip install -U langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an agent using ReAct.\n",
    "\n",
    "Observation: {observation}\n",
    "\n",
    "Think about what to do.\n",
    "Then respond in this format:\n",
    "\n",
    "Thought: <reasoning>\n",
    "Action: <action>\n",
    "\"\"\")\n",
    "\n",
    "response = llm.invoke(\n",
    "    prompt.format_prompt(observation=\"UI test failed with element click intercepted\")\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef93ae4f-38c0-4f53-9f2a-a58cfaa059f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: content='When a test fails due to a timeout, you can take the following actions:\\n\\n1. **Retry the Test**: Sometimes, the failure may be temporary, and retrying the test can lead to a successful outcome.\\n\\n2. **Escalate the Issue**: If the test continues to fail after multiple retries, it may indicate a deeper issue that needs to be addressed by a human or a more experienced team member.\\n\\nWould you like me to retry the test or escalate the issue?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 62, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CzpB4IVGS1GOjc6ca2iV2c5vS7c0h', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bd7b8-bd99-7840-894f-259a22c6b4a9-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 62, 'output_tokens': 99, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Tool calls: []\n"
     ]
    }
   ],
   "source": [
    "# Tool Calling Example\n",
    "# pip install -U langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# ---- Define tools ----\n",
    "@tool\n",
    "def retry_test():\n",
    "    \"\"\"Retry the failed test\"\"\"\n",
    "    return \"Test retried successfully\"\n",
    "\n",
    "@tool\n",
    "def escalate_issue():\n",
    "    \"\"\"Escalate issue to human\"\"\"\n",
    "    return \"Issue escalated to DevOps\"\n",
    "\n",
    "# ---- LLM with tools ----\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([retry_test, escalate_issue])\n",
    "\n",
    "# ---- Invoke agent ----\n",
    "response = llm_with_tools.invoke(\n",
    "    \"Test failed due to timeout. What should you do?\"\n",
    ")\n",
    "\n",
    "print(\"LLM response:\", response)\n",
    "print(\"Tool calls:\", response.tool_calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09da27-5835-40f2-82ec-937d91cf001d",
   "metadata": {},
   "source": [
    "LLM response: content='When a test fails due to a timeout, you can take the following actions:\\n\\n1. \n",
    "\n",
    "**Retry the Test**: Sometimes, the failure may be temporary, and retrying the test can lead to a successful outcome.\\n\\n2. \n",
    "\n",
    "**Escalate the Issue**: If the test continues to fail after multiple retries, it may indicate a deeper issue that needs to be addressed by a human or a more experienced team member.\\n\\n\n",
    "\n",
    "Would you like me to retry the test or escalate the issue?' \n",
    "\n",
    "additional_kwargs={'refusal': None} \n",
    "\n",
    "response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 62, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CzpB4IVGS1GOjc6ca2iV2c5vS7c0h', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bd7b8-bd99-7840-894f-259a22c6b4a9-0' \n",
    "\n",
    "tool_calls=[] \n",
    "\n",
    "invalid_tool_calls=[] \n",
    "\n",
    "usage_metadata={'input_tokens': 62, 'output_tokens': 99, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "Tool calls: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9c201-8215-4153-9d01-9b0254eae448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
