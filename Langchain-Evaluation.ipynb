{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1e2af7-9328-4068-b48b-b19cfacfeb9c",
   "metadata": {},
   "source": [
    "## Evaluation of RAG Systems\n",
    "# Introduction\n",
    "Evaluating Retrieval-Augmented Generation (RAG) systems is crucial to ensure they're providing accurate, relevant, and reliable responses. Unlike traditional applications, RAG systems combine retrieval and generation, making evaluation more complex. We need to assess both the quality of retrieved information and the generated responses.\n",
    "### Faithfulness\n",
    "#### What is Faithfulness?\n",
    "Faithfulness measures whether the generated answer is grounded in and supported by the retrieved context. In other words, is the model making things up (hallucinating) or is it staying true to the information it retrieved?\n",
    "#### Why it matters:\n",
    "If your RAG system generates answers that contradict or aren't supported by the retrieved documents, users will receive incorrect information even though you have the right documents in your knowledge base. This defeats the entire purpose of RAG.\n",
    "#### How to evaluate faithfulness:\n",
    "You can assess faithfulness by checking if every claim in the generated answer can be traced back to the retrieved context. Think of it like fact-checking with citations. There are several approaches:\n",
    "\n",
    "Manual review: Have humans read the context and answer, then verify if the answer is supported\n",
    "LLM-as-judge: Use another LLM to compare the generated answer against the retrieved context and score faithfulness\n",
    "Statement verification: Break the answer into individual claims and verify each one against the context\n",
    "\n",
    "##### Example of high faithfulness:\n",
    "Retrieved context: \"The Eiffel Tower was completed in 1889 and stands 330 meters tall.\"\n",
    "Generated answer: \"The Eiffel Tower, completed in 1889, has a height of 330 meters.\"\n",
    "\n",
    "##### Example of low faithfulness:\n",
    "Retrieved context: \"The Eiffel Tower was completed in 1889 and stands 330 meters tall.\"\n",
    "Generated answer: \"The Eiffel Tower was built in 1887 and is the tallest structure in Paris.\"\n",
    "### Context Relevance\n",
    "##### What is Context Relevance?\n",
    "Context relevance evaluates whether the retrieved documents actually contain information that's useful for answering the user's question. It's possible to retrieve documents that are topically related but don't actually help answer the specific query.\n",
    "##### Why it matters:\n",
    "Poor context relevance leads to two problems. First, irrelevant context can confuse the LLM and lead to poor or off-topic answers. Second, you're wasting tokens and processing time on information that doesn't help. In production systems with cost constraints, this inefficiency adds up quickly.\n",
    "##### How to evaluate context relevance:\n",
    "The key question is: \"Does this retrieved chunk contain information needed to answer the query?\" You can measure this through:\n",
    "\n",
    "Relevance scoring: Rate each retrieved chunk on a scale (like 1-5) for how relevant it is to the query\n",
    "Binary classification: Simple yes/no on whether the chunk is relevant\n",
    "LLM evaluation: Have an LLM assess whether the context could help answer the question\n",
    "Precision metrics: Calculate what percentage of retrieved chunks are actually relevant\n",
    "\n",
    "##### Improving context relevance:\n",
    "If you find low context relevance scores, you might need to improve your retrieval strategy by refining your embedding model, adjusting chunk sizes, using hybrid search (combining semantic and keyword search), implementing query expansion or rewriting, or adjusting the number of retrieved chunks.\n",
    "\n",
    "##### Example scenario:\n",
    "Query: \"What are the side effects of ibuprofen?\"\n",
    "\n",
    "High relevance context: \"Common side effects of ibuprofen include nausea, stomach pain, and dizziness.\"\n",
    "\n",
    "Low relevance context: \"Ibuprofen is available over-the-counter at most pharmacies and was first developed in the 1960s.\"\n",
    "\n",
    "### Using LangSmith to Evaluate\n",
    "#### What is LangSmith?\n",
    "LangSmith is a platform by LangChain for developing, monitoring, and evaluating LLM applications. It provides tools specifically designed for testing and improving RAG systems.\n",
    "##### Key features for RAG evaluation:\n",
    "1. Tracing and debugging: LangSmith automatically captures the full execution trace of your RAG pipeline, showing you the query, retrieved documents, prompts sent to the LLM, and final responses. This visibility is invaluable for understanding what's happening inside your system.\n",
    "2. Dataset creation: You can create test datasets with example queries and expected answers (or just queries if you want to evaluate without ground truth). These datasets can be version-controlled and shared across your team.\n",
    "3. Evaluation runs: LangSmith lets you run your RAG system against your test dataset and apply evaluators automatically. You can compare different versions of your system side-by-side to see which performs better.\n",
    "4. Built-in evaluators: LangSmith provides pre-built evaluators for common metrics including faithfulness, relevance, answer correctness, and more. You can also create custom evaluators using LLMs or code.\n",
    "##### How to use LangSmith for RAG evaluation:\n",
    "First, instrument your code by wrapping your RAG application with LangSmith tracing. This usually involves adding a simple decorator or context manager to your code.\n",
    "Next, create evaluation datasets containing representative user queries. Include edge cases and challenging questions, not just easy examples.\n",
    "Then define your evaluators by choosing from built-in evaluators or creating custom ones. For RAG, you typically want to evaluate both retrieval quality and generation quality.\n",
    "Run evaluations by executing your RAG system against your dataset while LangSmith collects metrics. Analyze the results in the LangSmith dashboard to identify patterns in failures and compare different system versions.\n",
    "Finally, iterate by making improvements to your system based on insights, then re-run evaluations to measure progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cef72a1-6285-4b24-a472-f19fbd121c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f89b50d7-b372-43a2-b433-a93402722c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for RAG evaluation\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"GDPR penalties can be up to €20 million or 4% of global annual turnover.\",\n",
    "        metadata={\"topic\": \"gdpr\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"GDPR defines rights such as access and erasure for individuals.\",\n",
    "        metadata={\"topic\": \"gdpr\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"SOC 2 is a compliance framework focused on security controls.\",\n",
    "        metadata={\"topic\": \"soc2\"}\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db2cf78-8ab0-4124-89b8-56953112d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# Create embeddings + vector DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vs = Chroma.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "# THIS is the missing object\n",
    "base_retriever = vs.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9afa6c5-ff41-4b1b-9752-47fc96972b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>token_f1</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>used_context_ids</th>\n",
       "      <th>judge_reason_relevance</th>\n",
       "      <th>judge_reason_faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are GDPR penalties?</td>\n",
       "      <td>GDPR penalties can be up to €20 million or 4% ...</td>\n",
       "      <td>GDPR penalties can be up to €20 million or 4% ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>The answer directly addresses the question by ...</td>\n",
       "      <td>The answer is fully supported by the retrieved...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What rights does GDPR provide to individuals?</td>\n",
       "      <td>GDPR provides rights such as access and erasur...</td>\n",
       "      <td>GDPR provides rights such as access and erasur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>The answer directly addresses the question by ...</td>\n",
       "      <td>All claims in the answer are supported by the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question  \\\n",
       "0                       What are GDPR penalties?   \n",
       "1  What rights does GDPR provide to individuals?   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  GDPR penalties can be up to €20 million or 4% ...   \n",
       "1  GDPR provides rights such as access and erasur...   \n",
       "\n",
       "                                              answer  exact_match  token_f1  \\\n",
       "0  GDPR penalties can be up to €20 million or 4% ...            0  0.903226   \n",
       "1  GDPR provides rights such as access and erasur...            0  0.800000   \n",
       "\n",
       "   answer_relevance  faithfulness  context_recall  context_precision  \\\n",
       "0                 5             5               5                  4   \n",
       "1                 5             5               5                  3   \n",
       "\n",
       "  used_context_ids                             judge_reason_relevance  \\\n",
       "0              [0]  The answer directly addresses the question by ...   \n",
       "1              [0]  The answer directly addresses the question by ...   \n",
       "\n",
       "                           judge_reason_faithfulness  \n",
       "0  The answer is fully supported by the retrieved...  \n",
       "1  All claims in the answer are supported by the ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "judge_llm  = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "df = evaluate_rag_dataset(\n",
    "    eval_set=eval_set,\n",
    "    retriever=base_retriever,\n",
    "    answer_llm=answer_llm,\n",
    "    judge_llm=judge_llm,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160bf84-c521-491c-9882-e3a0eea65f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
