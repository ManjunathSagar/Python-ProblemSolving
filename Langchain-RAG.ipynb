{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab53845-a329-4c32-8867-5143005e7ee8",
   "metadata": {},
   "source": [
    "### Vector databases \n",
    "\n",
    "FAISS → “Fast local vector search, no server”\n",
    "\n",
    "Chroma → “Beginner-friendly persistent vector DB”\n",
    "\n",
    "Pinecone → “Managed cloud vector DB for production”\n",
    "\n",
    "Weaviate → “Hybrid semantic + keyword search”\n",
    "\n",
    "Milvus → “Distributed vector DB for massive scale”\n",
    "\n",
    "Qdrant → “High-performance, self-hosted option”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fc877a-55da-4321-a9c2-0cfbc7c9d690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text splitters are used in RAG to break large documents into chunks, which helps in retrieving relevant documents before answering.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "# Check API key\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY\"\n",
    "\n",
    "# -------------------------\n",
    "# 1. Documents (Loader step)\n",
    "# -------------------------\n",
    "docs = [\n",
    "    Document(page_content=\"RAG retrieves relevant documents before answering.\"),\n",
    "    Document(page_content=\"Text splitters break large documents into chunks.\"),\n",
    "    Document(page_content=\"FAISS is a local vector database used for similarity search.\"),\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# 2. Splitter\n",
    "# -------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=120,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Embeddings + Vector DB\n",
    "# -------------------------\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Retriever\n",
    "# -------------------------\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# -------------------------\n",
    "# 5. Prompt + LLM\n",
    "# -------------------------\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question using ONLY the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# -------------------------\n",
    "# 6. RAG Pipeline (LCEL)\n",
    "# -------------------------\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": lambda x: x\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Ask a question\n",
    "# -------------------------\n",
    "response = rag_chain.invoke(\"Why do we use text splitters in RAG?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7f9b47-1636-4c11-b14e-b9409ac28391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LotusBlue\\AppData\\Local\\Temp\\ipykernel_12156\\1271726819.py:63: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma is a vector database that can run locally and persist embeddings, which can be used in RAG to store and retrieve relevant documents.\n"
     ]
    }
   ],
   "source": [
    "# RAG with chromaDB and metadata insertion to vector store\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# -------------------------\n",
    "# 0) API Key\n",
    "# -------------------------\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Loader step (simulated) with METADATA\n",
    "# -------------------------\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"RAG retrieves relevant documents before answering using an LLM.\",\n",
    "        metadata={\"source\": \"notes\", \"topic\": \"rag\", \"level\": \"beginner\", \"doc_id\": \"D1\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Chroma is a vector database that can run locally and persist embeddings.\",\n",
    "        metadata={\"source\": \"notes\", \"topic\": \"vector_db\", \"product\": \"chroma\", \"doc_id\": \"D2\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Chunk overlap helps preserve meaning across chunk boundaries in long documents.\",\n",
    "        metadata={\"source\": \"lecture\", \"topic\": \"chunking\", \"level\": \"beginner\", \"doc_id\": \"D3\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# 2) Splitter (keeps metadata on each chunk)\n",
    "# -------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=140, chunk_overlap=30)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# OPTIONAL: add chunk index metadata (useful for tracing)\n",
    "for i, d in enumerate(chunks):\n",
    "    d.metadata[\"chunk_id\"] = f\"C{i+1}\"\n",
    "\n",
    "# -------------------------\n",
    "# 3) Embeddings\n",
    "# -------------------------\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# -------------------------\n",
    "# 4) Vector DB: Chroma (with persistence)\n",
    "# -------------------------\n",
    "persist_dir = \"./chroma_store\"  # folder created locally\n",
    "collection_name = \"rag_demo\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Persist to disk (so you can restart kernel and reuse)\n",
    "vectorstore.persist()\n",
    "\n",
    "# -------------------------\n",
    "# 5) Retriever (basic)\n",
    "# -------------------------\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# -------------------------\n",
    "# 6) Prompt + LLM\n",
    "# -------------------------\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer using ONLY the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# -------------------------\n",
    "# 7) RAG chain (LCEL)\n",
    "# -------------------------\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"What is Chroma used for in RAG?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea6ebb2-4e93-4a41-a122-e2f30addcdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG retrieves relevant documents before answering using an LLM.\n"
     ]
    }
   ],
   "source": [
    "# Example: Retrieve only documents where topic=\"rag\"\n",
    "\n",
    "filtered_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 2,\n",
    "        \"filter\": {\"topic\": \"rag\"}   # metadata filter\n",
    "    }\n",
    ")\n",
    "\n",
    "filtered_rag_chain = (\n",
    "    {\"context\": filtered_retriever, \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(filtered_rag_chain.invoke(\"Explain RAG in one line.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8abdd9dc-71c6-4e8d-a240-0a111f41a901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use overlap to help preserve meaning across chunk boundaries in long documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Retrieve only docs from source=\"lecture\"\n",
    "\n",
    "lecture_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 2, \"filter\": {\"source\": \"lecture\"}}\n",
    ")\n",
    "\n",
    "lecture_chain = (\n",
    "    {\"context\": lecture_retriever, \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(lecture_chain.invoke(\"Why do we use overlap?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1645f4-3a71-4186-9c35-cf29650387fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
